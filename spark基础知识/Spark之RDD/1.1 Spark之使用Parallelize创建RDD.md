## **使用sparkContext.parallelize创建RDD** 

> If you are using scala, get [SparkContext](https://sparkbyexamples.com/spark/spark-sparkcontext/) object from [SparkSession](https://sparkbyexamples.com/spark/sparksession-explained-with-examples/) and use `sparkContext.parallelize()` to create rdd, this function also has another signature which additionally takes integer argument to specifies the number of partitions. Partitions are basic units of parallelism in Apache Spark. RDDs in Apache Spark are a collection of partitions.

如果您使用的是 scala，请从 [SparkSession](https://sparkbyexamples.com/spark/sparksession-explained-with-examples/) 获取 [SparkContext](https://sparkbyexamples.com/spark/spark-sparkcontext/) 对象 ) 并使用 `sparkContext.parallelize()` 创建 rdd，此函数还有另一个参数，该参数另外采用整数参数来指定分区数。 分区是 Apache Spark 中的基本并行单元。 Apache Spark 中的 RDD 是分区的集合。

## 1 使用Parallelize创建RDD

```scala
object ParralizeDemo {
  def main(args: Array[String]): Unit = {
    val spark: SparkSession = SparkSession.builder().master("local").appName("parralizeDemo").getOrCreate()

    //使用parallelize创建RDD
    //案例1
    val dataSeq = Seq(("java",20000),("python",20000),("Scala",20000))
    val rdd: RDD[(String, Int)] = spark.sparkContext.parallelize(dataSeq)
    val rddcollect: Array[(String, Int)] = rdd.collect()
    println("该RDD的分区数为：" + rdd.getNumPartitions)
    println("该RDD的第一个元素是：" + rdd.first())
    //案例2
    val sc: SparkContext = spark.sparkContext
    val rdd2: RDD[Int] = sc.parallelize(Array(1,2,3,4,5,6,7,8,9))
    println("遍历第二个RDD")
    rdd2.foreach(println)

    //案例3 创建一个空的RDD
    val emptyRDD: RDD[String] = sc.parallelize(Seq.empty[String])

    spark.stop()
  }

}

```

结果：

```
该RDD的分区数为：1
该RDD的第一个元素是：(java,20000)
遍历第二个RDD
1
2
3
4
5
6
7
8
9
```

