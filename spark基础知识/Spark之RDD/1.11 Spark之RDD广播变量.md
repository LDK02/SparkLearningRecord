# 广播变量Spark Broadcast Variables

在 Spark RDD 和 DataFrame 中，广播变量是<font color=red>只读的共享变量</font>，在集群中的所有节点上缓存并可用，以便任务访问或使用。 <font color=red>spark 不是将这些数据与每个任务一起发送，而是使用有效的广播算法将广播变量分配给机器，以降低通信成本。</font>

> In Spark RDD and DataFrame, Broadcast variables are read-only shared variables that are cached and available on all nodes in a cluster in-order to access or use by the tasks. Instead of sending this data along with every task, spark distributes broadcast variables to the machine using efficient broadcast algorithms to reduce communication costs.

## 何时使用

让我用一个例子来解释一下，假设您在一个文件中获得了一个由两个字母组成的国家/地区代码，并且您希望通过查找将其转换为完整的州名（例如 CA 到加利福尼亚，NY 到纽约等） 参考映射。 在某些情况下，这些数据可能很大，您可能有很多这样的查找（如邮政编码）。

> Let me explain with an example, assume you are getting a two-letter country state code in a file and you wanted to transform it to full state name, (for example CA to California, NY to New York e.t.c) by doing a lookup to reference mapping. In some instances, this data could be large and you may have many such lookups (like zip code).

我们可以使用广播变量在每台机器上缓存此查找信息，<font color=red>并且任务在执行转换时使用此缓存信息，而不是通过网络将此信息与每个任务一起分发（开销和耗时）</font>。

> Instead of distributing this information along with each task over the network (overhead and time consuming), we can use the broadcast variable to cache this lookup info on each machine and tasks use this cached info while executing the transformations.



## 广播变量如何工作?

广播变量的使用方式与 RDD、DataFrame 和 Dataset 相同。

**当您运行 Spark RDD、定义并使用了广播变量的 DataFrame 作业时，Spark 会执行以下操作。**

> Broadcast variables are used in the same way for RDD, DataFrame, and Dataset.

> When you run a Spark RDD, DataFrame jobs that has the Broadcast variables defined and used, Spark does the following.



- Spark breaks the job into stages that have distributed shuffling and actions are executed with in the stage.（Spark 将作业分解为具有分布式shuffle的阶段，并且在该阶段中执行操作。）

- Later Stages are also broken into tasks（后期将stage阶段也分为任务)

- Spark broadcasts the common data (reusable) needed by tasks within each stage.(Spark 在每个阶段广播任务所需的公共数据（可重用）。

- The broadcasted data is cache in serialized format and deserialized before executing each task.(广播的数据以序列化格式缓存，并在执行每个任务之前进行反序列化。)

  您应该为跨多个阶段和任务共享的数据创建和使用广播变量。

- > You should be creating and using broadcast variables for data that shared across multiple stages and tasks.

**请注意**:广播变量不会通过 `sc.broadcast(variable) `调用发送给执行程序，而是在第一次使用时发送给执行程序。

> Note that broadcast variables are not sent to executors with sc.broadcast(variable) call instead, they will be sent to executors when they are first used.

## 如何创建广播变量

Spark 广播变量可以使用 SparkContext 类的 `broadcast(v)` 方法创建。 此方法采用您要广播的参数 v。

> The Spark Broadcast is created using the `broadcast(v)` method of the SparkContext class. This method takes the argument v that you want to broadcast.



```scala
object BroadcastDemo {
  def main(args: Array[String]): Unit = {
    val spark: SparkSession = SparkSession.builder()
      .appName("broadcastDemo")
      .master("local")
      .getOrCreate()
    val sc: SparkContext = spark.sparkContext
    val broadcastVar: Broadcast[Array[Int]] = sc.broadcast(Array(0,1,2,3))
    println("打印广播变量的值："+broadcastVar.value.mkString(","))
  }

}
```



## Spark RDD 广播变量案例

下面是一个非常简单的示例，说明如何在 RDD 上使用广播变量。 此示例在 Map 变量中定义常用数据（国家和州），并使用 SparkContext.broadcast() 分配变量，然后在 RDD map() 转换中使用这些变量。

> Below is a very simple example of how to use broadcast variables on RDD. This example defines commonly used data (country and states) in a Map variable and distributes the variable using `SparkContext.broadcast()` and then use these variables on RDD map() transformation.



```scala

import org.apache.spark.SparkContext
import org.apache.spark.broadcast.Broadcast
import org.apache.spark.rdd.RDD
import org.apache.spark.sql.SparkSession

object BroadcastDemo {
  def main(args: Array[String]): Unit = {
    val spark: SparkSession = SparkSession.builder()
      .appName("broadcastDemo")
      .master("local")
      .getOrCreate()
    val sc: SparkContext = spark.sparkContext

    val states = Map(("NY","New York"),("CA","California"),("FL","Florida"))
    val countries = Map(("USA","United States of America"),("IN","India"))
    //定义广播变量,并定义广播变量的数据类型
    val broadcastStates: Broadcast[Map[String, String]] = sc.broadcast(states)
    val broadcastCountries: Broadcast[Map[String, String]] = sc.broadcast(countries)

    val data = Seq(("James","Smith","USA","CA"),
      ("Michael","Rose","USA","NY"),
      ("Robert","Williams","USA","CA"),
      ("Maria","Jones","USA","FL")
    )
    val rdd: RDD[(String, String, String, String)] = spark.sparkContext.parallelize(data)
    val rdd2: RDD[(String, String, String, String)] = rdd.map(f => {
      val country: String = f._3
      val state: String = f._4
        
      //调用广播变量,
      val fullCountry: String = broadcastCountries.value.get(country).get
      val fullstate: String = broadcastStates.value.get(state).get
      (f._1, f._2, fullCountry, fullstate)
    })
    println(rdd2.collect().mkString("\n"))

  }

}
```

结果：

```scala
(James,Smith,United States of America,California)
(Michael,Rose,United States of America,New York)
(Robert,Williams,United States of America,California)
(Maria,Jones,United States of America,Florida)
```



## Spark DataFrame 广播变量案例

下面是如何在 DataFrame 上使用广播变量的示例。 与上面的 RDD 示例类似，这在 Map 变量中定义常用数据（国家和州），并使用 SparkContext.broadcast() 分配变量，然后在 DataFrame map() 转换中使用这些变量。

> Below is an example of how to use broadcast variables on DataFrame. similar to above RDD example, This defines commonly used data (country and states) in a Map variable and distributes the variable using `SparkContext.broadcast()` and then use these variables on DataFrame map() transformation.

```scala

object BroadcastDemo {
  def main(args: Array[String]): Unit = {
    val spark: SparkSession = SparkSession.builder()
      .appName("broadcastDemo")
      .master("local")
      .getOrCreate()
    val sc: SparkContext = spark.sparkContext

    val states = Map(("NY","New York"),("CA","California"),("FL","Florida"))
    val countries = Map(("USA","United States of America"),("IN","India"))
    //定义广播变量
    val broadcastStates: Broadcast[Map[String, String]] = sc.broadcast(states)
    val broadcastCountries: Broadcast[Map[String, String]] = sc.broadcast(countries)

    val data = Seq(("James","Smith","USA","CA"),
      ("Michael","Rose","USA","NY"),
      ("Robert","Williams","USA","CA"),
      ("Maria","Jones","USA","FL")
    )
    val columns = Seq("firstname","lastname","country","state")
    import spark.sqlContext.implicits._
    val df = data.toDF(columns:_*)

  /*  val rdd: RDD[(String, String, String, String)] = spark.sparkContext.parallelize(data)
    val rdd2: RDD[(String, String, String, String)] = rdd.map(f => {
      val country: String = f._3
      val state: String = f._4
      //调用广播变量
      val fullCountry: String = broadcastCountries.value.get(country).get
      val fullstate: String = broadcastStates.value.get(state).get
      (f._1, f._2, fullCountry, fullstate)
    })
    println(rdd2.collect().mkString("\n"))
*/
    val df2: DataFrame = df.map(row => {
      val country: String = row.getString(2)
      val state: String = row.getString(3)
      val fullcountry: String = broadcastCountries.value.get(country).get
      val fullstate: String = broadcastStates.value.get(state).get
      (row.getString(0), row.getString(1), fullcountry, fullstate)
    }).toDF(columns: _*)
    df2.show(false)

  }

}
```

结果：

```scala
+---------+--------+------------------------+----------+
|firstname|lastname|country                 |state     |
+---------+--------+------------------------+----------+
|James    |Smith   |United States of America|California|
|Michael  |Rose    |United States of America|New York  |
|Robert   |Williams|United States of America|California|
|Maria    |Jones   |United States of America|Florida   |
+---------+--------+------------------------+----------+

```

## 完整代码：

```scala

import org.apache.spark.SparkContext
import org.apache.spark.broadcast.Broadcast
import org.apache.spark.rdd.RDD
import org.apache.spark.sql.{DataFrame, SparkSession}

object BroadcastDemo {
  def main(args: Array[String]): Unit = {
    val spark: SparkSession = SparkSession.builder()
      .appName("broadcastDemo")
      .master("local")
      .getOrCreate()
    val sc: SparkContext = spark.sparkContext

    val states = Map(("NY","New York"),("CA","California"),("FL","Florida"))
    val countries = Map(("USA","United States of America"),("IN","India"))
    //定义广播变量
    val broadcastStates: Broadcast[Map[String, String]] = sc.broadcast(states)
    val broadcastCountries: Broadcast[Map[String, String]] = sc.broadcast(countries)

    val data = Seq(("James","Smith","USA","CA"),
      ("Michael","Rose","USA","NY"),
      ("Robert","Williams","USA","CA"),
      ("Maria","Jones","USA","FL")
    )
    val columns = Seq("firstname","lastname","country","state")
    import spark.sqlContext.implicits._
    val df = data.toDF(columns:_*)

    val rdd: RDD[(String, String, String, String)] = spark.sparkContext.parallelize(data)
    val rdd2: RDD[(String, String, String, String)] = rdd.map(f => {
      val country: String = f._3
      val state: String = f._4
      //调用广播变量
      val fullCountry: String = broadcastCountries.value.get(country).get
      val fullstate: String = broadcastStates.value.get(state).get
      (f._1, f._2, fullCountry, fullstate)
    })
    println(rdd2.collect().mkString("\n"))

    val df2: DataFrame = df.map(row => {
      val country: String = row.getString(2)
      val state: String = row.getString(3)
      val fullcountry: String = broadcastCountries.value.get(country).get
      val fullstate: String = broadcastStates.value.get(state).get
      (row.getString(0), row.getString(1), fullcountry, fullstate)
    }).toDF(columns: _*)
    df2.show(false)

  }

}
```



# 总结Conclusion

在这篇 Spark 广播变量文章中，您已经了解了什么是广播变量，它的优势以及如何在 RDD 和 Dataframe 中使用 scala 示例。

> In this Spark Broadcast variable article you have learned what is Broadcast variable, it’s advantage and how to use in RDD and Dataframe with scala example.



