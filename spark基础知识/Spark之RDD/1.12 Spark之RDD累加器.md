# Spark之累加器Spark Accumulators

Spark 累加器是共享变量，仅通过关联和交换操作“添加”，<font color=red>用于执行计数器（类似于 Map-reduce 计数器）或求和操作。</font>

> Spark Accumulators are shared variables which are only “added” through an associative and commutative operation and are used to perform counters (Similar to Map-reduce counters) or sum operations.

Spark 默认支持创建任何数字类型的累加器，并提供添加自定义累加器类型的功能。

> Spark by default supports to create an accumulators of any numeric type and provide a capability to add custom accumulator types.



程序员可以创建以下累加器

> Programmers can create following accumulators

> - 命名累加器 (named accumulators)
>- 未命名的累加器(unnamed accumulators)

创建命名累加器时，可以在 <font color=red>Spark Web UI 的“累加器”选项卡下看到它们</font>。 在此选项卡上，您将看到两个表格； 第一个表`accumulable`——由所有命名的累加器变量及其值组成。 在第二个表`Tasks`上——任务修改的每个累加器的值。

> When you create a named accumulator, you can see them on Spark web UI under the “Accumulator” tab. On this tab, you will see two tables; the first table “accumulable” – consists of all named accumulator variables and their values. And on the second table “Tasks” – value for each accumulator modified by a task.

未命名的累加器不会显示在 Spark Web UI 上，所以在实际使用过程中，建议使用命名的累加器。

>  And, unnamed accumulators are not shows on Spark web UI, For all practical purposes it is suggestable to use named accumulators.

## 创建累加器

Spark 默认为 long、double 和 collection 类型提供累加器方法。 所有这些方法都存在于 [SparkContext](https://sparkbyexamples.com/spark/spark-sparkcontext/) 类中并返回 。

> Spark by default provides accumulator methods for long, double and collection types. All these methods are present in [SparkContext](https://sparkbyexamples.com/spark/spark-sparkcontext/) class and return respectively.

- [Long Accumulator](#Long类型累加器（Long Accumulator）)
- [Double Accumulator](https://sparkbyexamples.com/spark/spark-accumulators/#DoubleAccumulator)
- [Collection Accumulator](https://sparkbyexamples.com/spark/spark-accumulators/#CollectionAccumulator)

例如，您可以使用 spark-shell 创建长累加器:

> For example, you can create long accumulator on spark-shell using

```scala
val sc: SparkContext = spark.sparkContext
//定义一个long类型的累加器
val accum: LongAccumulator = sc.longAccumulator("SumAccumulator")
println(accum)

结果：
LongAccumulator(id: 0, name: Some(SumAccumulator), value: 0)
```

上面的语句创建了一个命名的累加器“SumAccumulator”。 现在，让我们看看如何将数组中的元素加到这个累加器中。

>The above statement creates a named accumulator “SumAccumulator”. Now, Let’s see how to add up the elements from an array to this accumulator.

```scala
val sc: SparkContext = spark.sparkContext
//定义一个long类型的累加器
val accum: LongAccumulator = sc.longAccumulator("SumAccumulator")
println(accum)
//
sc.parallelize(Array(1,2,3)).foreach(x => accum.add(x))
println("经过累加后的accum的值为：" + accum.value)

结果：
经过累加后的accum的值为：6
```

这些累加器类中的每一个都有几个方法，其中，<font color=red>从集群上运行的任务调用 add() 方法。 任务不能从累加器中读取值，只有驱动程序可以使用 value() 方法读取累加器的值</font>。

> Each of these accumulator classes has several methods, among these, `add()` method call from tasks running on the cluster. Tasks can’t read the values from the accumulator and only the driver program can read accumulators value using the `value()` method.

## Long类型累加器（Long Accumulator）

`SparkContext `中的 `longAccumulator()` 方法返回 `LongAccumulator`

> longAccumulator() methods from SparkContext returns LongAccumulator

```scala
//Long Accumulator
def longAccumulator : org.apache.spark.util.LongAccumulator
def longAccumulator(name : scala.Predef.String) : 
//使用时需要导入的包
org.apache.spark.util.LongAccumulator

```

可以使用 `SparkContext.longAccumulator(v) `为`Long`类型创建命名累加器，并为不带参数的未命名使用签名创建。

> You can create named accumulators for long type using `SparkContext.longAccumulator(v)` and for unnamed use signature that doesn’t take an argument.

```scala

val spark = SparkSession.builder()
.appName("SparkByExample")
.master("local")
.getOrCreate()
//创建long类型的累加器
val longAcc = spark.sparkContext.longAccumulator("SumAccumulator")

val rdd = spark.sparkContext.parallelize(Array(1, 2, 3))

rdd.foreach(x => longAcc.add(x))
println(longAcc.value)
```

longAccumulator类提供的方法：

- isZero：判断累加器是否为空，即是否为0.0。
- copy：复制累加器。
- reset：初始化累加器，使累加器的值为0.0。
- add：添加元素。
- count：统计元素个数。
- sum：求和。
- avg：求平均值
- merge：合并累加器
- value：返回累加器的值

## Double类型的累加器（Double Accumulator）

可以使用 `SparkContext.DoubleAccumulator(v) `为double类型创建命名累加器，并为不带参数的未命名使用签名创建。

> For named double type using `SparkContext.doubleAccumulator(v)` and for unnamed use signature that doesn’t take an argument.

```scala
//Double Accumulator
def doubleAccumulator : org.apache.spark.util.DoubleAccumulator
def doubleAccumulator(name : scala.Predef.String) : 
//使用时需要导入的包
org.apache.spark.util.DoubleAccumulator
```

DoubleAccumulator 提供的方法和LongAccumulator类似。

> DoubleAccumulator class also provides methods similar to LongAccumulator

```scala
//定义一个Double类型的累加器
val doubsum: DoubleAccumulator = sc.doubleAccumulator("DoubleSumAccmulatror")
sc.parallelize(Array(3.5,4.7,5.7)).foreach(x => doubsum.add(x))
println("经过累加后的doubsum的值为：" +  doubsum.value)

结果：
经过累加后的doubsum的值为：13.899999999999999
```

## 集合类型累加器（Collection Accumulator）

对于使用 `SparkContext.collectionAccumulator(v)` 的命名集合类型和不带参数的未命名使用签名。

> For named collection type using `SparkContext.collectionAccumulator(v)` and for unnamed use signature that doesn’t take an argument.

```scala
源码：
//Collection Accumulator
//创建并注册一个“CollectionAccumulator”，它以空列表开始，并通过将输入添加到列表中来累积输入。
  def collectionAccumulator[T](name: String): CollectionAccumulator[T] = {
    val acc = new CollectionAccumulator[T]
    register(acc, name)
    acc
  }

```
案例

```scala
//使用定义函数传入一个累加器名称，创建一个保存类型double的集合
val accmulatorcol: CollectionAccumulator[Double] = sc.collectionAccumulator[Double]("accmulatorMap")

sc.parallelize(Array(4.1,5.1,6.1,8.1)).foreach( x => accmulatorcol.add(x))
println("经过累加后的集合的值为：" + accmulatorcol.value)

结果：
经过累加后的集合的值为：[4.1, 5.1, 6.1, 8.1]
```

CollectionAccumulator 类提供以下方法

- isZero
- copyAndReset
- copy
- reset
- add
- merge
- value

**注意：**这些累加器类中的每一个都有几个方法，<font color=red>其中包括从集群上运行的任务调用的 `add()` 方法。 任务无法从累加器中读取值，只有驱动程序可以使用 `value()` 方法读取累加器值</font>。

> Note: Each of these accumulator classes has several methods, among these, `add()` method call from tasks running on the cluster. Tasks can’t read the values from the accumulator and only the driver program can read accumulators value using the `value()` method.



## 完整代码：

```scala

import org.apache.spark.SparkContext
import org.apache.spark.sql.SparkSession
import org.apache.spark.util.{CollectionAccumulator, DoubleAccumulator, LongAccumulator}

object AccumulatorsDemo {
  def main(args: Array[String]): Unit = {
    val spark: SparkSession = SparkSession
      .builder()
      .appName("Accumulators")
      .master("local")
      .getOrCreate()

    val sc: SparkContext = spark.sparkContext
    //定义一个long类型的累加器
    val accum: LongAccumulator = sc.longAccumulator("SumAccumulator")
    println(accum)
    //使用累加器对元素进行求和
    sc.parallelize(Array(1,2,3)).foreach(x => accum.add(x))
    println("经过累加后的accum的值为：" + accum.value)
    //定义一个Double类型的累加器
    val doubsum: DoubleAccumulator = sc.doubleAccumulator("DoubleSumAccmulatror")
    sc.parallelize(Array(3.5,4.7,5.7)).foreach(x => doubsum.add(x))
    println("经过累加后的doubsum的值为：" + doubsum.value)
	//定义一个集合类型的累加器
    val accmulatorcol: CollectionAccumulator[Double] = sc.collectionAccumulator[Double]("accmulatorMap")
    sc.parallelize(Array(4.1,5.1,6.1,8.1)).foreach( x => accmulatorcol.add(x))
    println("经过累加后的集合的值为：" + accmulatorcol.value)
  }

}
```



## Conclusion

在这篇 Spark 累加器共享变量文章中，您了解了累加器仅通过关联、交换和操作“添加”，用于执行计数器（类似于 Map-reduce 计数器）或求和操作，还学习了不同的累加器类以及 他们的方法。

> In this Spark accumulators shared variable article, you have learned the Accumulators are only “added” through an associative and commutative and operation and are used to perform counters (Similar to Map-reduce counters) or sum operations and also learned different Accumulator classes along with their methods.

## 官方教程

- https://spark.apache.org/docs/2.2.0/rdd-programming-guide.html#accumulators



