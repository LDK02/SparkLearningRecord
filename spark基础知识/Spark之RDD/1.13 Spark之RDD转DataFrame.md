# RDD转为DataFrame

在Apache Spark中使用 Scala  时，我们经常需要**将 Spark RDD 转换为 DataFrame** 和 Dataset，因为它们提供了比 RDD 更多的优势。 例如，DataFrame 是一种分布式数据集合，组织成类似于数据库表的命名列，并提供[优化和性能改进](https://sparkbyexamples.com/spark/spark-performance-tuning/)。

> While working in Apache Spark with Scala, we often need to **Convert Spark RDD to DataFrame** and Dataset as these provide more advantages over RDD. For instance, DataFrame is a distributed collection of data organized into named columns similar to Database tables and provides [optimization and performance improvement](https://sparkbyexamples.com/spark/spark-performance-tuning/).



在本文中，我将通过几个示例来解释如何**将 Spark RDD 转换为 Dataframe** 和 Dataset。

> In this article, I will explain how to **Convert Spark RDD to Dataframe** and Dataset using several examples.

## 目录

- [创建RDD](https://sparkbyexamples.com/apache-spark-rdd/convert-spark-rdd-to-dataframe-dataset/#spark-rdd)
- RDD转变DataFrame
  - [using toDF()方法](https://sparkbyexamples.com/apache-spark-rdd/convert-spark-rdd-to-dataframe-dataset/#rdd-todf)
  - [using createDataFrame()方法](https://sparkbyexamples.com/apache-spark-rdd/convert-spark-rdd-to-dataframe-dataset/#rdd-createDataFrame)
  - [using RDD row type & schema方法](https://sparkbyexamples.com/apache-spark-rdd/convert-spark-rdd-to-dataframe-dataset/#rdd-row-schema)
- [RDD 转为 Dataset](https://sparkbyexamples.com/apache-spark-rdd/convert-spark-rdd-to-dataframe-dataset/#spark-rdd-dataset)

##  Create Spark RDD

首先，让我们通过将 `Seq `对象传递给 `sparkContext.parallelize()` 函数来创建一个 RDD。 对于下面的所有示例，我们都需要这个“rdd”对象。

>  First, let’s create an RDD by passing Seq object to `sparkContext.parallelize()` function. We would need this “rdd” object for all our examples below.

```scala
val spark: SparkSession = SparkSession.
builder()
.master("local")
.appName("rdd2Dataframe")
.getOrCreate()

import spark.implicits._
val columns = Seq("language","users_count")
val data = Seq(("java","20000"),("python","1000000"),("Scala","3000"))
val rdd: RDD[(String, String)] = spark.sparkContext.parallelize(data)
```

可以使用 toDF()、createDataFrame() 并将 `rdd[Row]` 转换为DataFrame来将 Spark RDD 转换为 DataFrame。

> Converting Spark RDD to DataFrame can be done using toDF(), createDataFrame() and transforming `rdd[Row]` to the data frame.

## Spark RDD 转为DataFrame – Using toDF()

Spark 提供了一个隐式函数 `toDF()`，<font color=red>用于将 RDD、Seq[T]、List[T] 转换为 DataFrame</font>。 为了使用 toDF() 函数，我们应该首先使用 `import spark.implicits._` 导入隐式。

> Spark provides an implicit function `toDF()` which would be used to convert RDD, Seq[T], List[T] to DataFrame. In order to use toDF() function, we should import implicits first using `import spark.implicits._`.

```scala
import spark.implicits._
```

默认情况下，toDF() 函数将列名创建为“_1”和“_2”，就像元组一样。 模式下的输出。

> By default, toDF() function creates column names as “_1” and “_2” like Tuples. Outputs below schema.

```scala
/** 使用toDF方法将RDD转为DataFrame
*
* */
//第一种方式
val dfFromRDD1: DataFrame = rdd.toDF()//不传入列名
println("第一种方式的结构信息：")
dfFromRDD1.printSchema()//打印dataframe的结构信息

第一种方式的结构信息：
root
 |-- _1: string (nullable = true)
 |-- _2: string (nullable = true)
```

`toDF()` 有一个参数，它接受参数来定义列名，如下所示。

> `toDF() `has another signature that takes arguments to define column names as shown below.

```scala
//第二种方式
val dfFromRDD2: DataFrame = rdd.toDF("language","users_count")
println("第二种方式的结构信息：")
dfFromRDD2.printSchema()

第二种方式的结构信息：
root
 |-- language: string (nullable = true)
 |-- users_count: string (nullable = true)
```

默认情况下，这些列的数据类型推断数据的类型，并设置为 nullable 为 true。 我们可以通过使用 [StructType](https://sparkbyexamples.com/spark/spark-sql) 提供 [schema](https://sparkbyexamples.com/spark/spark-schema-explained-with-examples/) 来改变这种行为 -struct type-on-dataframe/)——我们可以在其中为每个字段/列指定列名、数据类型和是否为空。

> By default, the datatype of these columns infers to the type of data and set’s nullable to true. We can change this behavior by supplying [schema](https://sparkbyexamples.com/spark/spark-schema-explained-with-examples/) using [StructType](https://sparkbyexamples.com/spark/spark-sql-structtype-on-dataframe/) – where we can specify a column name, data type and nullable for each field/column.



## 4Spark RDD 转为DataFrame – Using createDataFrame()

`SparkSession`类提供[`createDataFrame()`方法来创建DataFrame](https://sparkbyexamples.com/spark/different-ways-to-create-a-spark-dataframe/)，它以rdd对象为参数 . 并将其与 toDF() 链接以指定列的名称。

> `SparkSession `class provides[ `createDataFrame()` method to create DataFrame](https://sparkbyexamples.com/spark/different-ways-to-create-a-spark-dataframe/) and it takes rdd object as an argument. and chain it with toDF() to specify names to the columns.

在这里，我们使用 scala 运算符 `<strong>:_*</strong>` 将<font color=red>列数组分解为逗号分隔值</font>。

```scala
/** 使用createDataFrame 将RDD转为Dataframe*/
val dfFromRDD3: DataFrame = spark.createDataFrame(rdd).toDF(columns:_*)
println("使用createDataFrame：")
dfFromRDD3.show()//展示dataframe

使用createDataFrame：
+--------+-----------+
|language|users_count|
+--------+-----------+
|    java|      20000|
|  python|    1000000|
|   Scala|       3000|
+--------+-----------+
```

在这里，我们使用 scala 运算符 <strong>:_*</strong> 将列数组分解为逗号分隔值。

> Here, we are using scala operator `:_*` to explode columns array to comma-separated values.

## 使用RDD Row 数据类型 RDD[Row] 转为DataFrame

Spark `createDataFrame()` 有一个参数，它将 RDD[Row] 类型和列名的模式作为参数。 要首先使用它，我们需要将“rdd”对象从 RDD[T] 转换为 RDD[Row]。 为了定义一个模式，我们使用 StructType，它接受一个 StructField 数组。 StructField 将列名、数据类型和可空/不可作为参数。

> Spark `createDataFrame()` has another signature which takes the RDD[Row] type and schema for column names as arguments. To use this first, we need to convert our “rdd” object from RDD[T] to RDD[Row]. To define a schema, we use StructType that takes an array of StructField. And StructField takes column name, data type and nullable/not as arguments.



```scala
/** 使用RDD row type 将RDD转为DataFrame*/
val schema: StructType = StructType(
columns.map(fielName => StructField(fielName,StringType,true))
)
//RDD[T] -> RDD[Row]
val rowRDD: RDD[Row] = rdd.map( attibutes => Row(attibutes._1,attibutes._2))
val dfFromRDD4: DataFrame = spark.createDataFrame(rowRDD,schema)
println("使用Row type 方式：")
dfFromRDD4.show()

使用Row type 方式：
+--------+-----------+
|language|users_count|
+--------+-----------+
|    java|      20000|
|  python|    1000000|
|   Scala|       3000|
+--------+-----------+
```

This creates a data frame from RDD and assigns column names using schema.

这会从 RDD 创建一个DataFrame，并使用模式分配列名。

## Spark RDD 转为Dataset

DataFrame API 与 RDD API 完全不同，因为它是一个用于构建关系查询计划的 API，然后 Spark 的 Catalyst 优化器可以执行该计划。

> The DataFrame API is radically different from the RDD API because it is an API for building a relational query plan that Spark’s Catalyst optimizer can then execute.

[Dataset API](https://databricks.com/spark/getting-started-with-apache-spark/datasets) 旨在提供两全其美：熟悉的面向对象编程风格和编译时类型- RDD API 的安全性，但具有 Catalyst 查询优化器的性能优势。数据集也使用与 DataFrame API 相同的高效堆外存储机制。

> The [Dataset API](https://databricks.com/spark/getting-started-with-apache-spark/datasets) aims to provide the best of both worlds: the familiar object-oriented programming style and compile-time type-safety of the RDD API but with the performance benefits of the Catalyst query optimizer. Datasets also use the same efficient off-heap storage mechanism as the DataFrame API.

DataFrame 是 *Dataset[Row]* 的别名。正如我们之前提到的，Datasets 针对类型化的工程任务进行了优化，您需要类型检查和面向对象的编程接口，而 DataFrames 对于交互式分析来说更快并且接近 SQL 风格。

> DataFrame is an alias to *Dataset[Row]*. As we mentioned before, Datasets are optimized for typed engineering tasks, for which you want types checking and object-oriented programming interface, while DataFrames are faster for interactive analytics and close to SQL style.

关于数据序列化。 Dataset API 具有 [encoders](https://databricks.com/blog/2016/01/04/introducing-apache-spark-datasets.html) 的概念，它在 JVM 表示（对象）和 Spark 的内部二进制格式之间进行转换. Spark 具有非常先进的内置编码器，<font color=red>它们生成字节码以与堆外数据交互并提供对单个属性的按需访问，而无需反序列化整个对象</font>。

> About data serializing. The Dataset API has the concept of [encoders](https://databricks.com/blog/2016/01/04/introducing-apache-spark-datasets.html) which translate between JVM representations (objects) and Spark’s internal binary format. Spark has built-in encoders that are very advanced in that they generate byte code to interact with off-heap data and provide on-demand access to individual attributes without having to de-serialize an entire object.

```scala
/* 将RDD转为dataset*/
val ds: Dataset[(String, String)] = spark.createDataset(rdd)
println("RDD转为dataset：")
ds.show()

RDD转为dataset：
+------+-------+
|    _1|     _2|
+------+-------+
|  java|  20000|
|python|1000000|
| Scala|   3000|
+------+-------+
```



## 总结

在本文中，您学习了如何将 Spark RDD 转换为 DataFrame 和 Dataset，在 Spark 中工作时我们会经常需要这些，因为它们提供了优于 RDD 的优化和性能。

> In this article, you have learned how to convert Spark RDD to DataFrame and Dataset, we would need these frequently while working in Spark as these provides optimization and performance over RDD.

Happy Learning !!

快乐学习！！

## 完整代码

```scala

import org.apache.spark.rdd.RDD
import org.apache.spark.sql.types.{StringType, StructField, StructType}
import org.apache.spark.sql.{DataFrame, Dataset, Row, SparkSession}

object RDD2DataFrame {
  def main(args: Array[String]): Unit = {
    val spark: SparkSession = SparkSession.
      builder()
      .master("local")
      .appName("rdd2Dataframe")
      .getOrCreate()

    import spark.implicits._
    val columns = Seq("language","users_count")
    val data = Seq(("java","20000"),("python","1000000"),("Scala","3000"))
    val rdd: RDD[(String, String)] = spark.sparkContext.parallelize(data)
    /** 使用toDF方法将RDD转为DataFrame
      *
      * */
      //第一种方式
    val dfFromRDD1: DataFrame = rdd.toDF()//不传入列名
    println("第一种方式的结构信息：")
    dfFromRDD1.printSchema()//打印dataframe的结构信息
    //第二种方式
    val dfFromRDD2: DataFrame = rdd.toDF("language","users_count")
    println("第二种方式的结构信息：")
    dfFromRDD2.printSchema()

    /** 使用createDataFrame 将RDD转为Dataframe*/
    val dfFromRDD3: DataFrame = spark.createDataFrame(rdd).toDF(columns:_*)
    println("使用createDataFrame：")
    dfFromRDD3.show()//展示dataframe

    /** 使用RDD row type 将RDD转为DataFrame*/
    val schema: StructType = StructType(
      columns.map(fielName => StructField(fielName,StringType,true))
    )
    //RDD[T] -> RDD[Row]
    val rowRDD: RDD[Row] = rdd.map( attibutes => Row(attibutes._1,attibutes._2))
    val dfFromRDD4: DataFrame = spark.createDataFrame(rowRDD,schema)
    println("使用Row type 方式：")
    dfFromRDD4.show()

    /* 将RDD转为dataset*/
    val ds: Dataset[(String, String)] = spark.createDataset(rdd)
    println("RDD转为dataset：")
    ds.show()



  }

}

```

