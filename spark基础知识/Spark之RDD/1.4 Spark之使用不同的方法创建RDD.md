# 创建 Spark RDD的不同方式

Spark RDD 可以使用 Scala 和 Pyspark 语言以多种方式创建，例如，<font color=red>可以使用 sparkContext.parallelize() 从文本文件、另一个 RDD、DataFrame 和 Dataset 创建</font>。 尽管我们在这里介绍了 Scala 中的大部分示例，但同样的概念可以用于在 PySpark (Python Spark) 中创建 RDD

> Spark RDD can be created in several ways using Scala & Pyspark languages, for example, It can be created by using sparkContext.parallelize(), from text file, from another RDD, DataFrame, and Dataset. Though we have covered most of the examples in Scala here, the same concept can be used to create RDD in PySpark (Python Spark)

## 目录

[1Spark 从 Seq 或 List 创建 RDD](#1Spark 从 Seq 或 List 创建 RDD)

[2 从文本文件创建 RDD](#2 从文本文件创建 RDD)

[3 从另一个 RDD 创建RDD](#3 从另一个 RDD 创建RDD)

[4 从 DataFrames 和 DataSet创建RDD](4 从 DataFrames 和 DataSet创建RDD)

[5 完整代码](#5 完整代码)

## 1Spark 从 Seq 或 List 创建 RDD

RDD 通常由并行集合创建，即通过从驱动程序（scala、python 等）获取现有集合并将其传递给 SparkContext 的 `parallelize()` 方法。 此方法仅用于测试，而不是实时使用，因为整个数据将驻留在一个节点上，这对于生产来说并不理想。

> RDD’s are generally created by parallelized collection i.e. by taking an existing collection from driver program (scala, python e.t.c) and passing it to SparkContext’s `parallelize()` method. This method is used only for testing but not in realtime as the entire data will reside on one node which is not ideal for production.

```scala
/** 使用Seq and list创建ＲＤＤ*/

    val RDD: RDD[(String, Int)] = spark.sparkContext.parallelize(Seq(("Java", 20000),
      ("Python", 100000), ("Scala", 3000)))

    RDD.foreach(println)
```



## 2 从文本文件创建 RDD

大多数情况下，对于生产系统，我们从文件创建 RDD。 这里将看到如何通过从文件中读取数据来创建 RDD。

> Mostly for production systems, we create RDD’s from files. here will see how to create an RDD by reading data from a file.

如果要将文件的全部内容作为单个记录读取，请使用 sparkContext 上的 WholeTextFiles() 方法。该方法不仅会返回文件内容，也会返回文件名。

> If you want to read the entire content of a file as a single record use wholeTextFiles() method on sparkContext.



```scala
/** 使用txt文件创建RDD*/
    val txtRDD: RDD[String] = spark.sparkContext.textFile("InData/SparkScalaExampleData/files/invalid.txt")
    println("---txt文件内容--")
    txtRDD.foreach(println)

    /** 使用wholeTextFiles 读取数据，该方法会返回文件名*/
    val wholeTxtRDD: RDD[(String, String)] = spark.sparkContext.wholeTextFiles("InData/SparkScalaExampleData/files/invalid.txt")
    wholeTxtRDD.foreach(record=>println("FileName : "+record._1+", FileContents :"+record._2))
```



## 3 从另一个 RDD 创建RDD

您可以使用 map、flatmap、filter 等转换从现有的 RDD 创建新的 RDD。

> You can use transformations like map, flatmap, filter to create a new RDD from an existing one.

```scala
 /** 使用其他ＲＤＤ生成*/
    val newRDD: RDD[(String, Int)] = RDD.map{row => (row._1,row._2 + 100)}//row._1表示第一个元素
```

## 4 从 DataFrames 和 DataSet创建RDD

要将 DataSet 或 DataFrame 转换为 RDD，只需对任何这些数据类型使用 `rdd()` 方法。

> To convert DataSet or DataFrame to RDD just use `rdd()` method on any of these data types.

```scala
/** 从dataframe and dataset 转换为ＲＤＤ*/

    val frame: DataFrame = spark.range(20).toDF()
    val frameRdd: RDD[Row] = frame.rdd

    frameRdd.foreach(println)
```

## 5 完整代码

```scala

object CreateRDDWays {
  def main(args: Array[String]): Unit = {
    val spark: SparkSession = SparkSession.builder()
      .appName("createRDD")
      .master("local")
      .getOrCreate()

    /** 使用Seq and list创建ＲＤＤ*/

    val RDD: RDD[(String, Int)] = spark.sparkContext.parallelize(Seq(("Java", 20000),
      ("Python", 100000), ("Scala", 3000)))

    RDD.foreach(println)

    /** 使用txt文件创建RDD*/
    val txtRDD: RDD[String] = spark.sparkContext.textFile("InData/SparkScalaExampleData/files/invalid.txt")
    println("---txt文件内容--")
    txtRDD.foreach(println)

    /** 使用wholeTextFiles 读取数据，该方法会返回文件名*/
    val wholeTxtRDD: RDD[(String, String)] = spark.sparkContext.wholeTextFiles("InData/SparkScalaExampleData/files/invalid.txt")
    wholeTxtRDD.foreach(record=>println("FileName : "+record._1+", FileContents :"+record._2))

    /** 使用其他ＲＤＤ生成*/
    val newRDD: RDD[(String, Int)] = RDD.map{row => (row._1,row._2 + 100)}//row._1表示第一个元素

    /** 从dataframe and dataset 转换为ＲＤＤ*/

    val frame: DataFrame = spark.range(20).toDF()
    val frameRdd: RDD[Row] = frame.rdd
    frameRdd.foreach(println)

  }

}

```

结果：

```scala
(Java,20000)
(Python,100000)
(Scala,3000)
---txt文件内容--
Invalid,I
FileName : file:/D:/workSpace/Ideaworkspace/SparkHadoop/InData/SparkScalaExampleData/files/invalid.txt, FileContents :Invalid,I
[0]
[1]
[2]
[3]
[4]
[5]
[6]
[7]
[8]
[9]
[10]
[11]
[12]
[13]
[14]
[15]
[16]
[17]
[18]
[19]
```

## 总结：

在本文中，学习了<font color=red>从列表或序列、文本文件、另一个 RDD、DataFrame 和 Dataset 创建 Spark RDD</font>。

> In this article, you have learned creating Spark RDD from list or seq, text file, from another RDD, DataFrame, and Dataset.



