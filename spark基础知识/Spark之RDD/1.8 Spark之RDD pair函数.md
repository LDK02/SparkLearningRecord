# Spark 之键值对 Functions

Spark 定义了 **PairRDDFunctions** 类，其中包含几个函数来处理<font color=red> 双RDD 或 RDD 键值对</font>，在本教程中，我们将通过 Scala 示例学习这些函数。 当您需要应用哈希分区、集合操作、连接等转换时，配对 RDD 会派上用场。

> Spark defines **PairRDDFunctions** class with several functions to work with Pair RDD or RDD key-value pair, In this tutorial, we will learn these functions with Scala examples. Pair RDD’s are come in handy when you need to apply transformations like hash partition, set operations, joins e.t.c.

所有这些功能都分为类似于常规 RDD 的转换和操作

> All these functions are grouped into Transformations and Actions similar to regular RDD’s

## 1 Spark之键值对行动算子

| 方法                      | 描述                                                         |
| :------------------------ | :----------------------------------------------------------- |
| collectAsMap              | Returns the pair RDD as a Map to the Spark Master.（将 pair RDD 作为 Map 返回给 Spark Master。） |
| countByKey                | Returns the count of each key elements. This returns the final result to local Map which is your driver.（返回每个key元素的计数。 这会将最终结果返回给本地 Map。） |
| countByKeyApprox          | Same as countByKey but returns the partial result. This takes a timeout as parameter to specify how long this function to run before returning.(与 countByKey 相同，但返回部分结果。 这需要一个超时作为参数来指定这个函数在返回之前运行多长时间。) |
| lookup                    | Returns a list of values from RDD for a given input key.(从给定输入键的 RDD 返回值列表) |
| reduceByKeyLocally        | Returns a merged RDD by merging the values of each key and final result will be sent to the master.(通过合并每个 key 的值返回一个合并的 RDD，最终结果将发送到 master。) |
| saveAsHadoopDataset       | Saves RDD to any hadoop supported file system (HDFS, S3, ElasticSearch, e.t.c), It uses Hadoop JobConf object to save.(将 RDD 保存到任何支持 hadoop 的文件系统（HDFS、S3、ElasticSearch 等），它使用 Hadoop JobConf 对象进行保存。) |
| saveAsHadoopFile          | Saves RDD to any hadoop supported file system (HDFS, S3, ElasticSearch, e.t.c), It uses Hadoop OutputFormat class to save.(将 RDD 保存到任何支持 hadoop 的文件系统（HDFS、S3、ElasticSearch 等），它使用 Hadoop OutputFormat 类进行保存。) |
| saveAsNewAPIHadoopDataset | Saves RDD to any hadoop supported file system (HDFS, S3, ElasticSearch, e.t.c) with new Hadoop API, It uses Hadoop Configuration object to save.(使用新的 Hadoop API 将 RDD 保存到任何支持 hadoop 的文件系统（HDFS、S3、ElasticSearch 等），它使用 Hadoop 配置对象进行保存。) |
| saveAsNewAPIHadoopFile    | Saves RDD to any hadoop supported fule system (HDFS, S3, ElasticSearch, e.t.c), It uses new Hadoop API OutputFormat class to save.(将 RDD 保存到任何支持 hadoop 的完整系统（HDFS、S3、ElasticSearch 等），它使用新的 Hadoop API OutputFormat 类进行保存。) |

## 2 Spark 之键值对转换算子

| 方法                     | 方法描述                                                     |
| :----------------------- | :----------------------------------------------------------- |
| aggregateByKey           | Aggregate the values of each key in a data set. This function can return a different result type then the values in input RDD.（将相同key的values相加） |
| combineByKey             | Combines the elements for each key.（按键进行组合）          |
| combineByKeyWithClassTag | Combines the elements for each key.                          |
| flatMapValues            | It's flatten the values of each key with out changing key values and keeps the original RDD partition.（它在不更改键值的情况下展平每个键的值并保留原始 RDD 分区。） |
| foldByKey                | Merges the values of each key.（合并每个键的值。）           |
| groupByKey               | Returns the grouped RDD by grouping the values of each key.（通过对每个键的值进行分组来返回分组的 RDD。） |
| mapValues                | It applied a map function for each value in a pair RDD with out changing keys. |
| reduceByKey              | Returns a merged RDD by merging the values of each key.（通过合并每个键的值返回一个合并的 RDD） |
| reduceByKeyLocally       | Returns a merged RDD by merging the values of each key and final result will be sent to the master.（通过合并每个 key 的值返回一个合并的 RDD，最终结果将发送到 master。） |
| sampleByKey              | Returns the subset of the RDD.（返回 RDD 的子集）            |
| subtractByKey            | Return an RDD with the pairs from this whose keys are not in other.（返回一个 RDD，其中包含来自 this 的对，其键不在 other 中） |
| keys                     | Returns all keys of this RDD as a RDD[T].(返回map中所有的keys) |
| values                   | Returns an RDD with just values.（返回map中所有的values）    |
| partitionBy              | Returns a new RDD after applying specified partitioner.（应用指定的分区器后返回一个新的 RDD。） |
| fullOuterJoin            | Return RDD after applying fullOuterJoin on current and parameter RDD（对当前和参数 RDD 应用 fullOuterJoin 后返回 RDD） |
| join                     | Return RDD after applying join on current and parameter RDD（对当前和参数 RDD 应用连接后返回 RDD） |
| leftOuterJoin            | Return RDD after applying leftOuterJoin on current and parameter RDD（在当前和参数 RDD 上应用 leftOuterJoin 后返回 RDD） |
| rightOuterJoin           | Return RDD after applying rightOuterJoin on current and parameter RDD（对当前和参数 RDD 应用 rightOuterJoin 后返回 RDD） |

## 3 准备数据

在本节中，我将使用 scala 示例解释 Spark 对 RDD 函数，在开始之前让我们创建一个对 RDD。

> In this section, I will explain Spark pair RDD functions with scala examples, before we get started let’s create a pair RDD.

```scala
val rdd = spark.sparkContext.parallelize(
    List("Germany India USA","USA India Russia","India Brazil Canada China")
)
//依据空格分割数据
val wordsRdd: RDD[String] = rdd.flatMap(_.split(" "))
println("----wordsRdd结果----------")
wordsRdd.foreach(println)
val pairRDD: RDD[(String, Int)] = wordsRdd.map(f => (f, 1))
//改变每个元素的形式，f -> (f,1)
println("-------pairRDD结果-----------")
pairRDD.foreach(println)
```

结果：

```scala
----wordsRdd结果----------
Germany
India
USA
USA
India
Russia
India
Brazil
Canada
China
-------pairRDD结果-----------
(Germany,1)
(India,1)
(USA,1)
(USA,1)
(India,1)
(Russia,1)
(India,1)
(Brazil,1)
(Canada,1)
(China,1)
```

此代码片段通过在 RDD 中的每个元素上按空格拆分来创建一对 RDD，将其展平以在 RDD 中的每个元素上形成单个单词字符串，最后为每个单词分配一个整数“1”。

> This snippet creates a pair RDD by splitting by space on every element in an RDD, flatten it to form a single word string on each element in RDD and finally assigns an integer “1” to every word.

## 4 键值对相关函数案例

### 4.1 collectAsMap 

 **功能：**This is an action function and returns Map to the master for retrieving all date from a dataset.（这是一个动作函数，以Map方式将所有数据 返回给 master。）

### 4.2 count 和collectAsMap之间的区别？

返回形式不同，count是以数组的形式返回数据集，collectAsMap则是以MaP的形式返回。

#### 案例：

```scala
/**
* callectAsMap
* */
println("collect结果 ==> ")
val tuples: Array[(String, Int)] = pairRDD.collect()
tuples.foreach(println)
println("collectAsMap结果 ==> ")
val stringToInt: collection.Map[String, Int] = pairRDD.collectAsMap()
stringToInt.foreach(println)
```

### 4.3 countByKey()

功能：统计每个key类别的个数。

**注意事项：**

该方法比较适用小量数据，处理大量数据时可以使用如下方法代替：

```scala
rdd.mapValues(_ => 1L).reduceByKey(_ + _)
```

**countByKey()源码:**

```

```

```scala
/**
 * Count the number of elements for each key, collecting the results to a local Map.
 *
 * @note This method should only be used if the resulting map is expected to be small, as
 * the whole thing is loaded into the driver's memory.
 * To handle very large results, consider using rdd.mapValues(_ => 1L).reduceByKey(_ + _), which
 * returns an RDD[T, Long] instead of a map.
 */
def countByKey(): Map[K, Long] = self.withScope {
  self.mapValues(_ => 1L).reduceByKey(_ + _).collect().toMap
}
```

案例

```scala
/** 2
  * countbykey
  * */
val stringToLong: collection.Map[String, Long] = pairRDD.countByKey()
println("--------案例1 countbykey结果------------")
stringToLong.foreach(println)
//创建类似(元素1，,2)，（元素2,2）的rdd,测试map中value值是否对countbykey结果产生影响
val pairRDD2: RDD[(String, Int)] = wordsRdd.map(f => (f,2))
val stringToLong2: collection.Map[String, Long] = pairRDD2.countByKey()
println("--------案例2 countbykey结果------------")
stringToLong2.foreach(println)
```

结果：

```scala
--------案例1 countbykey结果------------
(USA,2)
(Russia,1)
(China,1)
(Canada,1)
(India,3)
(Brazil,1)
(Germany,1)
--------案例2 countbykey结果------------
(USA,2)
(Russia,1)
(China,1)
(Canada,1)
(India,3)
(Brazil,1)
(Germany,1)
```

结论：countByKey()在计算每个key的个数时，其value值不会对其结果产生影响。

### 4.4 countByKeyApprox

功能：countByKey 的近似版本，如果未在超时内完成，则可以返回部分结果。

案例：

```scala
/**
  * 3 countByKeyApprox
  * *@param timeout 等待作业的最长时间，以毫秒为单位
  * *@param confidence 结果中所需的统计置信度
  * @return 一个可能不完整的结果，带有错误界限
  * */
val value: PartialResult[collection.Map[String, BoundedDouble]] = pairRDD.countByKeyApprox(60,0.9)
println("------countByKeyApprox结果:---------")
val stringToDouble: collection.Map[String, BoundedDouble] = value.getFinalValue()
println(stringToDouble)
```

结果：

```scala
------countByKeyApprox结果:---------
Map(USA -> [2.000, 2.000], Russia -> [1.000, 1.000], China -> [1.000, 1.000], Canada -> [1.000, 1.000], India -> [3.000, 3.000], Brazil -> [1.000, 1.000], Germany -> [1.000, 1.000])
```

### 4.4 distinct 

**功能：**Returns distinct keys.（对key进行去重）

#### 案例

```scala
val rdd = spark.sparkContext.parallelize(
List("Germany India USA","USA India Russia","India Brazil Canada China")
)
//依据空格分割数据
val wordsRdd: RDD[String] = rdd.flatMap(_.split(" "))
val pairRDD: RDD[(String, Int)] = wordsRdd.map(f => (f, 1))
pairRDD//改变每个元素的形式，f -> (f,1)
println("------------------")
pairRDD.foreach(println)
//去重打印
println("------去重打印--------------")
pairRDD.distinct().foreach(println)
```

结果：

```scala
------去重打印--------------
(Germany,1)
(India,1)
(Brazil,1)
(China,1)
(USA,1)
(Canada,1)
(Russia,1)
```

### 4.5 reduceByKey 

**功能：**将相同key的values相加

Result RDD contains unique keys.（结果 RDD 包含唯一键）

**源码：**

```scala
/**
   * Merge the values for each key using an associative and commutative reduce function. This will
   * also perform the merging locally on each mapper before sending results to a reducer, similarly
   * to a "combiner" in MapReduce. Output will be hash-partitioned with the existing partitioner/
   * parallelism level.
   
   */
def reduceByKey(func: (V, V) => V): RDD[(K, V)] = self.withScope {
    reduceByKey(defaultPartitioner(self), func)
}
```

#### 案例

```scala
/**
* reduceByKey
* 将相同key的values相加
* */
val wordCount: RDD[(String, Int)] = pairRDD.reduceByKey((a,b) => a+b)

println("Reduce by key ==>")
wordCount.foreach(println)
```

结果：

```scala
Reduce by key 的结果
(Brazil,1)
(Canada,1)
(China,1)
(USA,2)
(Germany,1)
(Russia,1)
(India,3)
```

### 4.6 aggregateByKey 

 **功能：**和reduceBykey功能一样,将相同key的values相加

> In our example, this is similar to reduceByKey but uses a different approach.

在我们的示例中，这类似于 reduceByKey，但使用了不同的方法。

```scala
/**
* aggregateByKey
* 将相同key的values相加
* */

def param1 = (accu:Int,v:Int) => accu +v//分区内的计算规则
def param2 = (accu1:Int,accu2:Int) => accu1 + accu2//分区间的计算规则
//0 为初始值
val wordCount2: RDD[(String, Int)] = pairRDD.aggregateByKey(0)(param1,param2)
println("Aggreate by key ==> wordCount2")
wordCount2.foreach(println)
```

结果：

```scala
Aggreate by key 的结果==> 
(Brazil,1)
(Canada,1)
(China,1)
(USA,2)
(Germany,1)
(Russia,1)
(India,3)
```



### 4.7 keys 

**功能：**Return RDD[K] with all keys in an dataset（返回数据集中的所有keys）

```scala
/**
  * 7 keys
  * 获取每个RDD的key值
  **/
println("keys ====> ")
wordCount2.keys.foreach(println)
```

### 4.8 values 

**功能：**return RDD[V] with all values in an dataset(返回数据集中的所有values)

```scala
/**
  * 8 values
  * 获取每个RDD的value值
  **/
println("keys ====> ")
wordCount2.values.foreach(println)
```

### 4.9 sortByKey 

**功能：**返回一个 按键排序后的RDD

**源码：**

```scala
/**
* 对RDD进行key排序，使得每个分区都包含一个排序范围内的元素。 在生成的 RDD 上调用 `collect` 或 `save` 将返回或输出一个有序的记录列表
（在 `save` 的情况下，它们将按照键的顺序写入文件系统中的多个 `part-X` 文件）。
*/
def sortByKey(ascending: Boolean = true, numPartitions: Int = self.partitions.length)
: RDD[(K, V)] = self.withScope
{
val part = new RangePartitioner(numPartitions, self, ascending)
new ShuffledRDD[K, V, V](self, part)
.setKeyOrdering(if (ascending) ordering else ordering.reverse)
}
```

案例:

```scala
/**
* sortBykey
* 按照key值进行排序
* */
val sortRDD: RDD[(String, Int)] = pairRDD.sortByKey()
println("Sort by key ==>")
sortRDD.foreach(println)
```

结果：

```scala
Sort by key ==>
(Brazil,1)
(Canada,1)
(China,1)
(Germany,1)
(India,1)
(India,1)
(India,1)
(Russia,1)
(USA,1)
(USA,1)
```

### 4.10 combineByKey

源码：

```scala
def combineByKey[C](
    createCombiner: V => C,//对每个类别的第一个key进行操作
    mergeValue: (C, V) => C,//分区内的操作
    //分区间的操作
    mergeCombiners: (C, C) => C): RDD[(K, C)] = self.withScope {
  combineByKeyWithClassTag(createCombiner, mergeValue, mergeCombiners)(null)
}
```

案例：

```scala
/**
  * 10 combineByKey
  * */
val rdd2: RDD[String] = spark.sparkContext.parallelize(List("Java Scala C++", "Scala python C++", "C++ Java C#"),2)
val comRDD: RDD[(String, Int)] = rdd2.flatMap(_.split(" ")).map(f => (f,1))
println("pairRDD的分区数为："+comRDD.getNumPartitions)
comRDD.saveAsTextFile("data/output/comRDD")
//定义分区内的计算,因只对values进行计算，所以为ＲＤＤ的value的类型，
def paramCom1 = (accu: Int, v: Int) => accu + v//可以理解为accu和v分别为第一个RDD 和第二个RDD的value值
//定义分区间的计算
def paramCom2 = (accu1: Int, accu2: Int) => accu1 + accu2//
//这里的item => (item+1)操作仅仅是针对每个partition中的每个key的第一个数据进行操作，具体为对value进行操作
val res: RDD[(String, Int)] = comRDD.combineByKey(item => (item+1),paramCom1,paramCom2)
println("------combineByKey计算结果-------")
res.foreach(println)
```

计算结果为：

```scala
pairRDD的分区数为：2
------combineByKey计算结果-------
(C#,2)
(Java,4)
(python,2)
(Scala,4)
(C++,5)
```

### 图解combineBykey：

![image-20220602181449099](1.8%20Spark%E4%B9%8BRDD%20pair%E5%87%BD%E6%95%B0.assets/image-20220602181449099.png)

## 完整代码

```scala

import org.apache.spark.rdd.RDD
import org.apache.spark.sql.SparkSession

object PairRDDfunctionsDemo {
  def main(args: Array[String]): Unit = {
    val spark: SparkSession = SparkSession
      .builder()
      .appName("Pair——RDD——function")
      .master("local")
      .getOrCreate()

    val rdd = spark.sparkContext.parallelize(
      List("Germany India USA","USA India Russia","India Brazil Canada China")
    )
   //依据空格分割数据
    val wordsRdd: RDD[String] = rdd.flatMap(_.split(" "))
    val pairRDD: RDD[(String, Int)] = wordsRdd.map(f => (f, 1))
    pairRDD//改变每个元素的形式，f -> (f,1)
    println("------------------")
    pairRDD.foreach(println)
    //去重打印
    println("------去重打印--------------")
    pairRDD.distinct().foreach(println)

    /**
      * sortBykey
      * 按照key值进行排序
      * */
    val sortRDD: RDD[(String, Int)] = pairRDD.sortByKey()
    println("Sort by key ==>")
    sortRDD.foreach(println)

    /**
      * reduceByKey
      * 将相同key的values相加
      * */
    val wordCount: RDD[(String, Int)] = pairRDD.reduceByKey((a,b) => a+b)

    println("Reduce by key ==>")
    wordCount.foreach(println)

    /**
    * aggregateByKey
      * 将相同key的values相加
    * */

    def param1 = (accu:Int,v:Int) => accu +v//分区内的计算规则
    def param2 = (accu1:Int,accu2:Int) => accu1 + accu2//分区间的计算规则
    val wordCount2: RDD[(String, Int)] = pairRDD.aggregateByKey(0)(param1,param2)
    println("Aggreate by key ==> wordCount2")
    wordCount2.foreach(println)

    /**
      * keys
      * */
    println("keys ====> ")
    wordCount2.keys.foreach(println)
    /**
      * values
      * */
    println("keys ====> ")
    wordCount2.values.foreach(println)

    /**
      * callectAsMap
      * */
    println("collect结果 ==> ")
    val tuples: Array[(String, Int)] = pairRDD.collect()
    tuples.foreach(println)
    println("collectAsMap结果 ==> ")
    val stringToInt: collection.Map[String, Int] = pairRDD.collectAsMap()
    stringToInt.foreach(println)


  }

}

```

