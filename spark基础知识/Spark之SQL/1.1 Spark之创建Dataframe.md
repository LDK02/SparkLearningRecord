# Spark 创建 DataFrame示例

In Spark, create DataFrame() and toDF() methods are used to create a DataFrame manually, using these methods you can create a Spark DataFrame from already existing RDD, DataFrame, Dataset, List, Seq data objects, here I will examplain these with Scala examples.

> 在 Spark 中，create DataFrame() 和 toDF() 方法用于手动创建 DataFrame，<font color=darorange>使用这些方法可以从现有的 RDD、DataFrame、Dataset、List、Seq 数据对象创建 Spark DataFrame，这里我将用Scala示例</font>。

You can also create a DataFrame from different sources like [Text](https://sparkbyexamples.com/spark/spark-read-text-file-rdd-dataframe/), [CSV](https://sparkbyexamples.com/spark/spark-read-csv-file-into-dataframe/), [JSON](https://sparkbyexamples.com/spark/spark-read-and-write-json-file/), [XML](https://sparkbyexamples.com/spark/spark-read-write-xml/), [Parquet](https://sparkbyexamples.com/pyspark/pyspark-read-and-write-parquet-file/), [Avro](https://sparkbyexamples.com/spark/spark-read-write-avro-files-from-amazon-s3/), [ORC](https://sparkbyexamples.com/spark/spark-read-orc-file-into-dataframe/), [Binary](https://sparkbyexamples.com/spark/spark-read-binary-file-into-dataframe/) files, RDBMS Tables, Hive, [HBase](https://sparkbyexamples.com/spark/spark-read-write-using-hbase-spark-connector/), and many more.

你可以从不同来源创建dataframe，如 [Text](https://sparkbyexamples.com/spark/spark-read-text-file-rdd-dataframe/), [CSV](https://sparkbyexamples.com/spark/spark-read-csv-file-into-dataframe/), [JSON](https://sparkbyexamples.com/spark/spark-read-and-write-json-file/), [XML](https://sparkbyexamples.com/spark/spark-read-write-xml/), [Parquet](https://sparkbyexamples.com/pyspark/pyspark-read-and-write-parquet-file/), [Avro](https://sparkbyexamples.com/spark/spark-read-write-avro-files-from-amazon-s3/), [ORC](https://sparkbyexamples.com/spark/spark-read-orc-file-into-dataframe/), [Binary](https://sparkbyexamples.com/spark/spark-read-binary-file-into-dataframe/) files, RDBMS Tables, Hive, [HBase](https://sparkbyexamples.com/spark/spark-read-write-using-hbase-spark-connector/),等。

> DataFrame is a distributed collection of data organized into named columns. It is conceptually equivalent to a table in a relational database or a data frame in R/Python, but with richer optimizations under the hood. DataFrames can be constructed from a wide array of sources such as: structured data files, tables in Hive, external databases, or existing RDDs.
>
> DataFrame 是组织成命名列的分布式数据集合。 它在概念上等同于关系数据库中的表或 R/Python 中的数据框，但在底层进行了更丰富的优化。 DataFrames 可以从多种来源构建，例如：结构化数据文件、Hive 中的表、外部数据库或现有 RDD。

## 目录

- [1 使用RDD创建Dataframe](#1. Spark Create DataFrame from RDD)
- [2 从 List 和 Seq 集合创建 Spark DataFrame](#2. 从 List 和 Seq 集合创建 Spark DataFrame(Create Spark DataFrame from List and Seq Collection))
- [3 从 CSV 创建 Spark DataFrame](#3.从 CSV 创建 Spark DataFrame（Create Spark DataFrame from CSV）)
- [4 从文本 (TXT) 文件创建Dataframe](#4. 从文本 (TXT) 文件创建（Creating from text (TXT) file）)
- [5 从json文件创建Dataframe](#5. 从json文件创建Creating from JSON file)
- [6 从XML文件创建Dataframe](6. 从XML文件创建Creating from an XML file)
- [7 从hive读取数据创建Dataframe](#7. 从hive读取数据创建Dataframe(Creating from Hive))
- [8 Spark 从 RDBMS 数据库创建 DataFrame](#8. Spark 从 RDBMS 数据库创建 DataFrame(Spark Create DataFrame from RDBMS Database))
- [9 从 HBase 表创建 DataFrame](#9. 从 HBase 表创建 DataFrame(Create DataFrame from HBase table))
- [10 Other sources (Avro, Parquet e.t.c)](#10.  其他来源（Avro、Parquet、Kafka）)

First, let’s import spark implicits as it needed for our examples ( for example when we want to use `.toDF() function`) and create the sample data.

>  首先，让我们根据示例的需要导入 spark 转换需要的包（例如，当我们想要使用 `.toDF() 函数`时）并创建示例数据。

## 使用RDD创建DataFrame(Spark Create DataFrame from RDD)

One easy way to create Spark DataFrame manually is from an existing RDD. first, let’s [create an RDD](https://sparkbyexamples.com/spark/different-ways-to-create-spark-rdd/) from a collection Seq by calling [parallelize()](https://sparkbyexamples.com/tag/sparkcontext-parallelize/).

手动创建 Spark DataFrame 的一种简单方法是从现有的 RDD。 首先，让我们通过调用 parallelize() 从集合 Seq 创建一个 RDD。

I will be using this **rdd** object for all our examples below.

我将在下面的所有示例中使用这个 **rdd** 对象。

```scala
/** 准备数据*/
import spark.implicits._
val columns = Seq("language","users_count")
val data = Seq(("java", "20000"), ("Python", "100000"), ("Scala", "3000"))
```



### 使用toDF()函数（Using toDF() function）

Once we have an RDD, let’s use `toDF()` to create DataFrame in Spark. By default, it creates column names as “_1” and “_2” as we have two columns for each row.

一旦我们有了一个 RDD，让我们使用 `toDF()` 在 Spark 中创建 DataFrame。 默认情况下，它将列名创建为“_1”和“_2”，因为我们每行有两列。

```scala
/** 使用RDD创建DataFrame*/
val rdd: RDD[(String, String)] = spark.sparkContext.parallelize(data)
//默认情况下，toDF()方法会给创建列名：_1,_2,...给每一列
val dfFromRDD1: DataFrame = rdd.toDF()
println("toDF创建的默认列名：" + dfFromRDD1.schema)
```

Since RDD is schema-less without column names and data type, converting from RDD to DataFrame gives you default column names as `_1`, `_2` and so on and data type as String.

由于 RDD 是无模式的，没有列名和数据类型，从 RDD 转换为 DataFrame 会为您提供默认的列名，如`_1`、`_2` 等，数据类型为字符串。

`toDF()` has another signature to assign a column name, this takes a variable number of arguments for column names as shown below.

`toDF()` 有另一个参数来指定列名，这需要可变数量的列名参数，如下所示。

```scala
 //可以给toDF()，传递列名，自定义创建后的列名
val dfFromRDD2: DataFrame = rdd.toDF("lanuage","users_count")
println("使用toDF()自定义列名：" + dfFromRDD2.schema)
```

By default, the datatype of these columns assigns to String. We can change this behavior by supplying schema – where [we can specify a column name, data type and nullable for each field/column](https://sparkbyexamples.com/spark/spark-sql-structtype-on-dataframe/).

默认情况下，这些列的数据类型分配给 String。 我们可以通过提供模式来改变这种行为——其中[我们可以为每个字段/列指定列名、数据类型和可为空](https://sparkbyexamples.com/spark/spark-sql-structtype-on-dataframe/) .

###  使用createDataFrame函数（using Spark createDataFrame() from SparkSession)

Using `createDataFrame()` from SparkSession is another way to create and it takes rdd object as an argument. and chain with toDF() to specify names to the columns.

使用 SparkSession 中的 `createDataFrame()` 是另一种创建方式，它将 rdd 对象作为参数。 并与 toDF() 链接以指定列的名称。

```scala
 /** 1.2使用SparkSession中的createDataFrame方法创建DataFrame*/
 val dfFromRDD3: DataFrame = spark.createDataFrame(rdd).toDF(columns:_*)
 println("createDataFrame创建的dataframe:")
 dfFromRDD3.show()
```



###  使用带有 Row 类型的 createDataFrame()(Using createDataFrame() with the Row type)

`createDataFrame()` has another signature which takes the RDD[Row] type and schema for column names as arguments. To use this first we need to convert our “rdd” object from `RDD[T]` to `RDD[Row]` and define a [schema using StructType & StructField](https://sparkbyexamples.com/spark/spark-sql-structtype-on-dataframe/).

`createDataFrame()` 有另一个参数，它将 RDD[Row] 类型和列名的模式作为参数。 要首先使用它，我们需要将“rdd”对象从 `RDD[T]` 转换为 `RDD[Row]` 并定义一个 [schema using StructType & StructField](https://sparkbyexamples.com/spark/spark- sql-structtype-on-dataframe/)。

```scala
/** 1.3 使用createDataFrame 创建Dataframe，并指定数据类型*/
val schema = StructType(Array(
    StructField("language", StringType, true),
    StructField("user_count", StringType, true)
))
val rowRDD: RDD[Row] = rdd.map(attributes => Row(attributes._1,attributes._2))
val dfFromRDD4: DataFrame = spark.createDataFrame(rowRDD,schema)
```



## 从 List 和 Seq 集合创建 Spark DataFrame(Create Spark DataFrame from List and Seq Collection)

> In this section, we will see several approaches to create Spark DataFrame from collection `Seq[T]` or `List[T]`. These examples would be similar to what we have seen in the above section with RDD, but we use “data” object instead of “rdd” object.

在本节中，我们将看到几种从集合 `Seq[T]` 或 `List[T]` 创建 Spark DataFrame 的方法。 这些示例类似于我们在上面的 RDD 部分中看到的示例，**但我们使用“data”对象而不是“rdd”对象。**

### 在 List 或 Seq 集合上使用 toDF()（Using toDF() on List or Seq collection）

`toDF()` on collection (Seq, List) object creates a DataFrame. make sure importing `import spark.implicits._` to use toDF()

集合（Seq，List）对象上的`toDF（）`创建一个DataFrame。 确保导入 `import spark.implicits._` 以使用 toDF()

```scala
/** 2.1 使用list 或 Seq数据结构创建Dataframe*/
val dfFromdata1: DataFrame = data.toDF()
```

###  使用 SparkSession 中的 createDataFrame()（Using createDataFrame() from SparkSession）

Calling `createDataFrame()` from `SparkSession` is another way to create and it takes collection object (Seq or List) as an argument. and chain with toDF() to specify names to the columns.

从 `SparkSession` 调用 `createDataFrame()` 是另一种创建方式，它将集合对象（Seq 或 List）作为参数。 并与 toDF() 链接以指定列的名称。

```scala
//指定列名
val dfFromData2: DataFrame = spark.createDataFrame(data).toDF(columns:_*)
```

### 使用带有 Row 类型的 createDataFrame()（Using createDataFrame() with the Row type）

createDataFrame()` has another signature in Spark which takes the `util.List` of Row type and schema for column names as arguments. To use this first we need to `**import scala.collection.JavaConversions._**

createDataFrame()` 在 Spark 中有另一个参数，它将 Row 类型的 `util.List` 和列名的模式作为参数。 要首先使用它，我们需要`import scala.collection.JavaConversions._

```scala
//指定数据类型
import scala.collection.JavaConversions._
val rowdata = Seq(Row("java", "20000"),
Row("Python", "100000"),
Row("Scala", "3000")
)
val dfFromData3: DataFrame = spark.createDataFrame(rowdata,schema)
```

## 从 CSV 创建 Spark DataFrame（Create Spark DataFrame from CSV）

In all the above examples, you have learned Spark to create DataFrame from RDD and data collection objects. In real-time these are less used, In this and following sections, you will learn how to create DataFrame from data sources like CSV, text, JSON, Avro e.t.c

Spark by default provides an API to read a delimiter files like comma, pipe, tab separated files and it also provides several options on handling with header, with out header, double quotes, data types e.t.c.

For detailed example, refer to [create DataFrame from a CSV file](https://sparkbyexamples.com/spark/spark-read-csv-file-into-dataframe/).

```scala
/** 3 从csv创建Dataframe*/
val csvDataframe: DataFrame = spark.read.csv("InData/SparkScalaExampleData/CSVData/text01.csv")
println("读取csv文件创建的dataframe:")
csvDataframe.show()
```

## 从文本 (TXT) 文件创建（Creating from text (TXT) file）

Here, will see how to create from a TXT file.

```scala
/** 4 从text文件创建Dataframe*/
val textFrame: DataFrame = spark.read.text("InData/SparkScalaExampleData/test.txt")
println("读取text文件创建的dataframe:")
textFrame.show()
```

## 从json文件创建Creating from JSON file

Here, will see how to [create from a JSON file](https://sparkbyexamples.com/spark/spark-read-and-write-json-file/).

```scala
/** 从jso文件创建dataframe*/
val jsonDataFrame: DataFrame = spark.read.json("InData/SparkScalaExampleData/resources/schema.json")
println("读取json文件创建的dataframe:")
jsonDataFrame.show()
```

## 完整代码：

```scala
package sparkScalaExamples.SQL

import org.apache.spark.rdd.RDD
import org.apache.spark.sql.types.{StringType, StructField, StructType}
import org.apache.spark.sql.{DataFrame, Row, SparkSession}

object RDDdataframe {
  def main(args: Array[String]): Unit = {
    val spark: SparkSession = SparkSession
      .builder()
      .master("local")
      .appName("rdddataframe")
      .getOrCreate()

    /** 准备数据*/
    import spark.implicits._

    val columns = Seq("language","users_count")
    val data = Seq(("java", "20000"), ("Python", "100000"), ("Scala", "3000"))
    /** 1*/
    /** 1.1 使用RDD创建DataFrame*/
    val rdd: RDD[(String, String)] = spark.sparkContext.parallelize(data)
    //默认情况下，toDF()方法会给创建列名：_1,_2,...给每一列
    val dfFromRDD1: DataFrame = rdd.toDF()
    println("toDF创建的默认列名：" + dfFromRDD1.schema)
    //可以给toDF()，传递列名，自定义创建后的列名
    val dfFromRDD2: DataFrame = rdd.toDF("lanuage","users_count")
    println("使用toDF()自定义列名：" + dfFromRDD2.schema)


    /** 1.2使用SparkSession中的createDataFrame方法创建DataFrame*/
    val dfFromRDD3: DataFrame = spark.createDataFrame(rdd).toDF(columns:_*)
    println("createDataFrame创建的dataframe:")
    dfFromRDD3.show()
    /** 1.3 使用createDataFrame 创建Dataframe，并指定数据类型*/
    val schema = StructType(Array(
      StructField("language", StringType, true),
      StructField("user_count", StringType, true)
    ))
    val rowRDD: RDD[Row] = rdd.map(attributes => Row(attributes._1,attributes._2))
    val dfFromRDD4: DataFrame = spark.createDataFrame(rowRDD,schema)
    /**2 */
    /** 2.1 使用list 或 Seq数据结构创建Dataframe*/
    val dfFromdata1: DataFrame = data.toDF()

    //指定列名
    val dfFromData2: DataFrame = spark.createDataFrame(data).toDF(columns:_*)
    //指定数据类型
    import scala.collection.JavaConversions._
    val rowdata = Seq(Row("java", "20000"),
      Row("Python", "100000"),
      Row("Scala", "3000")
    )
    val dfFromData3: DataFrame = spark.createDataFrame(rowdata,schema)

    /** 3 从csv创建Dataframe*/
    val csvDataframe: DataFrame = spark.read.csv("InData/SparkScalaExampleData/CSVData/text01.csv")
    println("读取csv文件创建的dataframe:")
    csvDataframe.show()

    /** 4 从text文件创建Dataframe*/
    val textFrame: DataFrame = spark.read.text("InData/SparkScalaExampleData/test.txt")
    println("读取text文件创建的dataframe:")
    textFrame.show()

    /** 从jso文件创建dataframe*/
    val jsonDataFrame: DataFrame = spark.read.json("InData/SparkScalaExampleData/resources/zipcodes.json")
    println("读取json文件创建的dataframe:")
    jsonDataFrame.show()



  }

}

```



## 从XML文件创建Creating from an XML file

To[ create DataFrame by parse XML](https://sparkbyexamples.com/spark/processing-xml-files-in-spark/), we should use DataSource `"com.databricks.spark.xml"` spark-xml api from Databricks.

要[通过解析 XML 创建 DataFrame](https://sparkbyexamples.com/spark/processing-xml-files-in-spark/)，我们应该使用 DataSource `"com.databricks.spark.xml"` spark-xml api 来自 Databricks。

添加依赖：

```xml
<dependency>
     <groupId>com.databricks</groupId>
     <artifactId>spark-xml_2.11</artifactId>
     <version>0.6.0</version>
 </dependency>
```

代码：

```scala

val df = spark.read
      .format("com.databricks.spark.xml")
      .option("rowTag", "person")
      .xml("src/main/resources/persons.xml")

```

## 从hive读取数据创建Dataframe(Creating from Hive)

```scala
val hiveContext = new org.apache.spark.sql.hive.HiveContext(spark.sparkContext)
val hiveDF = hiveContext.sql(“select * from emp”)
```

## Spark 从 RDBMS 数据库创建 DataFrame(Spark Create DataFrame from RDBMS Database)

### From **Mysql** table

Make sure you have MySQL library as a dependency in your pom.xml file or MySQL jars in your classpath.

确保你的 pom.xml 文件中有 MySQL 库作为依赖项，或者你的类路径中有 MySQL jars。

```scala
val df_mysql = spark.read.format(“jdbc”)
   .option(“url”, “jdbc:mysql://localhost:port/db”)
   .option(“driver”, “com.mysql.jdbc.Driver”)
   .option(“dbtable”, “tablename”) 
   .option(“user”, “user”) 
   .option(“password”, “password”) 
   .load()

```

###  From **DB2** table

Make sure you have DB2 library as a dependency in your pom.xml file or DB2 jars in your classpath.

确保在 pom.xml 文件中有 DB2 库作为依赖项，或者在类路径中有 DB2 jar。

```scala

val df_db2 = spark.read.format(“jdbc”)
   .option(“url”, “jdbc:db2://localhost:50000/dbname”)
   .option(“driver”, “com.ibm.db2.jcc.DB2Driver”)
   .option(“dbtable”, “tablename”) 
   .option(“user”, “user”) 
   .option(“password”, “password”) 
   .load()
```

## 从 HBase 表创建 DataFrame(Create DataFrame from HBase table)

To create[ Spark DataFrame from the HBase table](https://sparkbyexamples.com/spark/spark-read-write-using-hbase-spark-connector/), we should use DataSource defined in [Spark HBase connectors](https://sparkbyexamples.com/hbase/spark-hbase-connectors-which-one-to-use/). for example use DataSource “`org.apache.spark.sql.execution.datasources.hbase`” from Hortonworks or use “`org.apache.hadoop.hbase.spark`” from spark HBase connector.

要从 HBase 表创建 [Spark DataFrame](https://sparkbyexamples.com/spark/spark-read-write-using-hbase-spark-connector/)，我们应该使用 [Spark HBase 连接器](https://sparkbyexamples.com/hbase/spark-hbase-connectors-which-one-to-use/)。 例如，使用来自 Hortonworks 的数据源“`org.apache.spark.sql.execution.datasources.hbase`”或使用来自 spark HBase 连接器的“`org.apache.hadoop.hbase.spark`”。

```scala
val hbaseDF = sparkSession.read
.options(Map(HBaseTableCatalog.tableCatalog -> catalog))
.format("org.apache.spark.sql.execution.datasources.hbase")
.load()
```

## 其他来源（Avro、Parquet、Kafka）

We can also create DataFrame from Avro, Parquet, HBase and reading data from Kafka which I’ve explained in the below articles, I would recommend reading these when you have time.

我们还可以从 Avro、Parquet、HBase 中创建 DataFrame 并从 Kafka 中读取数据，我在下面的文章中进行了解释，我建议您有时间时阅读这些。

- [Creating DataFrame from Parquet source](https://sparkbyexamples.com/spark/spark-read-write-dataframe-parquet-example/)
- [Creating DataFrame from Avro source](https://sparkbyexamples.com/spark/using-avro-data-files-from-spark-sql-2-4/)
- [Creating DataFrame by Streaming data from Kafka](https://sparkbyexamples.com/spark/spark-streaming-kafka-consumer-example-in-json-format/)

The complete code can be downloaded from [GitHub](https://github.com/spark-examples/spark-scala-examples/blob/master/src/main/scala/com/sparkbyexamples/spark/dataframe/CreateDataFrame.scala)

#  创建空的Dataframe

```scala
package com.dkl.leanring.spark.df

import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.types._
import org.apache.spark.sql.Row
import org.apache.spark.rdd.EmptyRDD

/**
 * Spark创建空DataFrame示例
 */
object EmptyDataFrame {

  def main(args: Array[String]): Unit = {

    val spark = SparkSession.builder().appName("EmptyDataFrame").master("local").getOrCreate()

    /**
     * 创建一个空的DataFrame，代表用户
     * 有四列，分别代表ID、名字、年龄、生日
     */
    val colNames = Array("id", "name", "age", "birth")
    //为了简单起见，字段类型都为String
    val schema = StructType(colNames.map(fieldName => StructField(fieldName, StringType, true)))
    //主要是利用了spark.sparkContext.emptyRDD
    val emptyDf = spark.createDataFrame(spark.sparkContext.emptyRDD[Row], schema)

    emptyDf.show

    /**
     * 也可以给每列指定相对应的类型
     */
    val schema1 = StructType(
      Seq(
        StructField("id", IntegerType, true),
        StructField("name", StringType, true),
        StructField("age", IntegerType, true),
        StructField("birth", StringType, true)))
    val emptyDf1 = spark.createDataFrame(spark.sparkContext.emptyRDD[Row], schema1)
    emptyDf1.show

    //还有一种空的DataFrame，没有任何行任何列
    spark.emptyDataFrame.show

    spark.stop()
  }

}
```

