# Spark SQL Data Types with Examples

> Spark SQL `DataType` class is a base class of all data types in Spark which defined in a package `org.apache.spark.sql.types.DataType` and they are primarily used while working on DataFrames, In this article, you will learn different Data Types and their utility methods with Scala examples.

Spark SQL `DataType` 类是 Spark 中所有数据类型的基类，<font color=red>定义在 `org.apache.spark.sql.types.DataType` 包中，</font>主要用于处理 DataFrames，在本文中，您将 通过 Scala 示例学习不同的数据类型及其实用方法。

## Spark SQL DataType——所有数据类型的基类

> All data types from the below table are supported in Spark SQL and `DataType` class is a base class for all these. For some types like `IntegerType`, `DecimalType`, `ByteType `e.t.c are subclass of `NumericType` which is a subclass of DataType.

Spark SQL 支持下表中的所有数据类型，“DataType”类是所有这些的基类。 对于某些类型，例如 <font color=red>`IntegerType`、`DecimalType`、`ByteType`e.t.c 是 `NumericType` 的子类，后者是 DataType 的子类</font>。

| [StringType](https://sparkbyexamples.com/spark/spark-sql-dataframe-data-types/#string-type) | ShortType      |
| ------------------------------------------------------------ | -------------- |
| [ArrayType](https://sparkbyexamples.com/spark/spark-sql-dataframe-data-types/#array-type) | IntegerType    |
| [MapType](https://sparkbyexamples.com/spark/spark-sql-dataframe-data-types/#map-type) | LongType       |
| [StructType](https://sparkbyexamples.com/spark/spark-sql-dataframe-data-types/#struct-type) | FloatType      |
| [DateType](https://sparkbyexamples.com/spark/spark-sql-dataframe-data-types/#date-type) | DoubleType     |
| [TimestampType](https://sparkbyexamples.com/spark/spark-sql-dataframe-data-types/#timestamp-type) | DecimalType    |
| BooleanType                                                  | ByteType       |
| CalendarIntervalType                                         | HiveStringType |
| BinaryType                                                   | ObjectType     |
| NumericType                                                  | NullType       |

## DataType常用方法

> All Spark SQL Data Types extends `DataType` class and should provide implementation to the methods explained in this example.

所有 Spark SQL 数据类型都扩展了“DataType”类，并提供了本示例中方法的实现。

例如：ArrayType继承了AbstractDataType

```scala
object ArrayType extends AbstractDataType {
  /**
   * Construct a [[ArrayType]] object with the given element type. The `containsNull` is true.
   */
  def apply(elementType: DataType): ArrayType = ArrayType(elementType, containsNull = true)
```

**DataType提供的常见方法：**

```scala
 val arr = ArrayType(IntegerType,false)
 println("json() : "+arrayType.json)  // Represents json string of datatype
 println("prettyJson() : "+arrayType.prettyJson) // Gets json in pretty format
 println("simpleString() : "+arrayType.simpleString) // simple string
 println("sql() : "+arrayType.sql) // SQL format
 println("typeName() : "+arrayType.typeName) // type name
 println("catalogString() : "+arrayType.catalogString) 
// catalog string
 println("defaultSize() : "+arrayType.defaultSize)
// default size
```

Yields below output.

```scala
json() : {"type":"array","elementType":"string","containsNull":true}
prettyJson() : {
  "type" : "array",
  "elementType" : "string",
  "containsNull" : true
}
simpleString() : array<string>
sql() : ARRAY<STRING>
typeName() : array
catalogString() : array<string>
defaultSize() : 20
```

> Besides these, the `DataType` class has the following static methods.

除此之外，`DataType` 类还有以下静态方法。

###  DataType.*fromJson*()

> If you have a JSON string and you wanted to convert to a DataType use `fromJson()` . For example you wanted to convert [JSON schema from a string to StructType](https://sparkbyexamples.com/spark/spark-schema-explained-with-examples/#schema-from-json).

如果你有一个 JSON 字符串并且你想转换为一个 DataType 使用 `fromJson()` 。 例如，您想将 [JSON 模式从字符串转换为 StructType](https://sparkbyexamples.com/spark/spark-schema-explained-with-examples/#schema-from-json)。

```scala
val typeFromJson = DataType.fromJson(
    """{"type":"array",
      |"elementType":"string","containsNull":false}""".stripMargin)
println(typeFromJson.getClass)
val typeFromJson2 = DataType.fromJson("\"string\"")
println(typeFromJson2.getClass)

//This prints
class org.apache.spark.sql.types.ArrayType
class org.apache.spark.sql.types.StringType$
```

### DataType.fromDDL()

> Like loading structure from JSON string, we can also create it `fromDDL()`,

就像从 JSON 字符串加载结构一样，我们也可以创建它 `fromDDL()`，

```scala
val ddlSchemaStr = "`fullName` STRUCT<`first`: STRING, `last`: STRING," +
    "`middle`: STRING>,`age` INT,`gender` STRING"
val ddlSchema = DataType.fromDDL(ddlSchemaStr)
println(ddlSchema.getClass)

// This prints
class org.apache.spark.sql.types.StructType
```



## 使用Spark SQL DataTypes类获取类型对象

> In order to get or create a specific data type, we should use the objects and factory methods provided by `org.apache.spark.sql.types.DataTypes` class. for example, use object `DataTypes.StringType` to get `StringType` and the factory method `DataTypes.createArrayType(StirngType)` to get [ArrayType](https://sparkbyexamples.com/spark/spark-array-arraytype-dataframe-column/) of string.

<font color=red>为了获取或创建特定的数据类型，我们应该使用 org.apache.spark.sql.types.DataTypes 类提供的对象和工厂方法</font>。 例如，使用对象`DataTypes.StringType`获取`StringType`和工厂方法`DataTypes.createArrayType(StirngType)`获取[ArrayType](https://sparkbyexamples.com/spark/spark-array-arraytype-dataframe -column/) 的字符串。

```scala
//Below are some examples  
val strType = DataTypes.StringType
val arrayType = DataTypes.createArrayType(StringType)
val structType = DataTypes.createStructType(
    Array(DataTypes.createStructField("fieldName",StringType,true)))
```

## StringType

> StringType “`org.apache.spark.sql.types.StringType`” is used to represent string values, To create a string type use either `DataTypes.StringType` or `StringType()`, both of these returns object of String type.

StringType “`org.apache.spark.sql.types.StringType`” 用于表示字符串值，要创建字符串类型，请使用 `DataTypes.StringType` 或 `StringType()`，这两者都返回 String 类型的对象 .

```scala
  val strType = DataTypes.StringType
  println("json : "+strType.json)
  println("prettyJson : "+strType.prettyJson)
  println("simpleString : "+strType.simpleString)
  println("sql : "+strType.sql)
  println("typeName : "+strType.typeName)
  println("catalogString : "+strType.catalogString)
  println("defaultSize : "+strType.defaultSize)
```

Outputs

```scala
json : "string"
prettyJson : "string"
simpleString : string
sql : STRING
typeName : string
catalogString : string
defaultSize : 20
```

## ArrayType

使用 [ArrayType](https://sparkbyexamples.com/spark/spark-array-arraytype-dataframe-column/) 在 DataFrame 中表示数组并使用工厂方法 `DataTypes.createArrayType()` 或 `ArrayType()` 构造函数**来获取特定类型的数组对象**。

> Use [ArrayType](https://sparkbyexamples.com/spark/spark-array-arraytype-dataframe-column/) to represent arrays in a DataFrame and use either factory method `DataTypes.createArrayType()` or `ArrayType()` constructor to get an array object of a specific type.

在 Array 类型对象上，您可以访问第 1.1 节中定义的所有方法，此外，<font color=red>它还提供了 `containsNull()`、`elementType()`、`productElement()` 等等</font>。

> On Array type object you can access all methods defined in section 1.1 and additionally, it provides `containsNull()`, `elementType()`, `productElement()` to name a few.



```scala
val arr = ArrayType(IntegerType,false)
val arrayType = DataTypes.createArrayType(StringType,true)
println("containsNull : "+arrayType.containsNull)
println("elementType : "+arrayType.elementType)
println("productElement : "+arrayType.productElement(0))
```

Yields below output.

```scala
containsNull : true
elementType : StringType
productElement : StringType
```

For more example and usage, please refer [Using ArrayType on DataFrame](https://sparkbyexamples.com/spark/spark-array-arraytype-dataframe-column/)

## MapType

> Use [MapType](https://sparkbyexamples.com/spark/spark-dataframe-map-maptype-column/) to represent maps with key-value pair in a DataFrame and use either factory method `DataTypes.createMapType()` or `MapType()`constructor to get a map object of a specific key and value type.

使用 [MapType](https://sparkbyexamples.com/spark/spark-dataframe-map-maptype-column/) 在 DataFrame 中表示具有键值对的数据结构，并使用工厂方法 **`DataTypes.createMapType()` 或 `MapType()`构造函数获取特定键值类型的对象**。

> On Map type object you can access all methods defined in section 1.1 and additionally, it provides `keyType()`, `valueType()`, `valueContainsNull()`, `productElement()` to name a few.

在 Map 类型对象上，您可以访问第 1.1 节中定义的所有方法，此外，<font color=red>它还提供了 `keyType()`、`valueType()`、`valueContainsNull()`、`productElement()` 等等</font>。

```scala
val mapType1 = MapType(StringType,IntegerType)
val mapType = DataTypes.createMapType(StringType,IntegerType)
println("keyType() : "+mapType.keyType)
println("valueType() : "+mapType.valueType)
println("valueContainsNull() : "+mapType.valueContainsNull)
println("productElement(1) : "+mapType.productElement(1))
```

Yields below output.

```scala
keyType() : StringType
valueType() : IntegerType
valueContainsNull() : true
productElement(1) : IntegerType
```

For more example and usage, please refer [Using MapType on DataFrame](https://sparkbyexamples.com/spark/spark-dataframe-map-maptype-column/)

## DateType

> Use DateType “`org.apache.spark.sql.types.DataType`” to represent the date on a DataFrame and use either `DataTypes.DateType` or `DateType()` constructor to get a date object.

使用 DateType “`org.apache.spark.sql.types.DataType`” 来**表示 DataFrame 上的日期**，<font color=red>并使用 `DataTypes.DateType` 或 `DateType()` 构造函数来获取日期对象</font>。

> On Date type object you can access all methods defined in section 1.1

在 Date 类型对象上，您可以访问第 1.1 节中定义的所有方法

## TimestampType

> Use TimestampType “`org.apache.spark.sql.types.TimestampType`” to represent the time on a DataFrame and use either `DataTypes.TimestampType` or `TimestampType()` constructor to get a time object.

使用 TimestampType “`org.apache.spark.sql.types.TimestampType`” 来表示 DataFrame 上的时间，<font color=red>并使用 `DataTypes.TimestampType` 或 `TimestampType()` 构造函数来获取时间对象</font>。

> On Timestamp type object you can access all methods defined in section 1.1

在 Timestamp 类型对象上，您可以访问第 1.1 节中定义的所有方法

## SructType

> Use StructType “`org.apache.spark.sql.types.StructType`” to define the nested structure or schema of a DataFrame, use either `DataTypes.createStructType()` or `StructType()` constructor to get a struct object.

使用 StructType “`org.apache.spark.sql.types.StructType`” <font color=red>定义 DataFrame 的嵌套结构或模式</font>，使用 `DataTypes.createStructType()` 或 `StructType()` 构造函数来获取结构对象。

> StructType object provides lot of functions like `toDDL()`, `fields()`, `fieldNames()`, `length()` to name few.

StructType 对象提供了许多函数，**例如`toDDL()`、`fields()`、`fieldNames()`、`length()` 等等**。

```scala
  //StructType
  val structType = DataTypes.createStructType(
    Array(DataTypes.createStructField("fieldName",StringType,true)))

  val simpleSchema = StructType(Array(
    StructField("name",StringType,true),
    StructField("id", IntegerType, true),
    StructField("gender", StringType, true),
    StructField("salary", DoubleType, true)
  ))

  val anotherSchema = new StructType()
    .add("name",new StructType()
      .add("firstname",StringType)
      .add("lastname",StringType))
    .add("id",IntegerType)
    .add("salary",DoubleType)
```

For more example and usage, please refer [StructType](https://sparkbyexamples.com/spark/spark-sql-structtype-on-dataframe/)

## All other remaining Spark SQL Data Types

> Similar to the above-described types, for the rest of the datatypes use the appropriate method on DataTypes class or data type constructor to create an object of the desired Data Type, And all common methods described in section 1.1 are available with these types.

与上述类型类似，对于其余数据类型，使用 DataTypes 类或数据类型构造函数上的适当方法来创建所需数据类型的对象，并且 1.1 节中描述的所有常用方法都可用于这些类型。

## 完整代码

```scala
package sparkScalaExamples.SQL

import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.types._

object DataTypeDemo {
  def main(args: Array[String]): Unit = {
    val spark: SparkSession =
      SparkSession
        .builder()
        .appName("DataTypeDemo")
        .master("local").getOrCreate()
    /** 1 提供常用方法*/
    //定义一个array，其中元素类型为Integer,The `containsNull` is true，
    //`containsNull` 字段用于指定数组是否具有 `null` 值。
    val arrayType = ArrayType(IntegerType,false)//false为数组不包含Null

    println("json() : "+arrayType.json)  // 表示数据类型的json字符串
    println("prettyJson() : "+arrayType.prettyJson) // 以漂亮的格式获取 json
    println("simpleString() : "+arrayType.simpleString) // 简单字符串
    println("sql() : "+arrayType.sql) // SQL format
    println("typeName() : "+arrayType.typeName) // type name
    println("catalogString() : "+arrayType.catalogString) // 目录字符串
    println("defaultSize() : "+arrayType.defaultSize) // 默认大小


    //stringType类型
    val stringType = DataTypes.StringType
    println("json : "+ stringType.json)
    println("prettyJson : "+stringType.prettyJson)
    println("simpleString : "+stringType.simpleString)
    println("sql : "+ stringType.sql)
    println("typeName : "+stringType.typeName)
    println("catalogString : "+stringType.catalogString)
    println("defaultSize : "+stringType.defaultSize)

    //arrayType类型
    val arrayType2 = ArrayType(IntegerType,false)
    val arrayType3: ArrayType = DataTypes.createArrayType(StringType,true)
    println("containsNull : "+arrayType3.containsNull)
    println("elementType : "+arrayType3.elementType)
    println("productElement : "+arrayType3.productElement(0))

    val mapType = MapType(StringType,IntegerType)
    val mapType1: MapType = DataTypes.createMapType(StringType,IntegerType)
    println("keyType()" + mapType1.keyType)
    println("valueType()" + mapType1.valueType)
    println("valueContainsNull:" + mapType1.valueContainsNull)
    println("productElement(1):" + mapType1.productElement(1))

    val structType1: StructType = DataTypes.createStructType(
      Array(DataTypes.createStructField("fieldName", StringType, true))
    )

    val simpleSchema = StructType(Array(
      StructField("name", StringType, true),
      StructField("id", IntegerType, true),
      StructField("gender", StringType, true),
      StructField("salary", DoubleType, true)
    ))

    val anotherSchema: StructType = new StructType()
      .add("name", new StructType()
        .add("firtstname", StringType)
        .add("lastname", StringType))
      .add("id", IntegerType)
      .add("salary", DoubleType)



    /** 2
      * fromJson方法：将json格式的数据转为DataType数据格式
      * */
    val typeFromJson: DataType = DataType.fromJson(
      """{"type":"array",
"elementType":"String","containsNull":false}""".stripMargin
    )
    println("数据类型为"+typeFromJson.getClass)
    val typeFromJson2: DataType = DataType.fromJson("\"string\"")
    println("数据类型为"+typeFromJson2.getClass)

    /**3
      * fromDDL
      * */

   /* val ddlSchemaStr = "`fullName` STRUCT<`first`: STRING, `last`: STRING," +
      "`middle`: STRING>,`age` INT,`gender` STRING"
    val ddlSchema = DataType.fromDDL(ddlSchemaStr)
    println(ddlSchema.getClass)*/

    /** 4 使用DataTypes创建类型对象*/
    val strType: DataType = DataTypes.StringType//获取字符对象
    //获取字符数组对象
    val arrayType1: ArrayType = DataTypes.createArrayType(StringType)
    val structType: StructType = DataTypes.createStructType(
      Array(DataTypes.createStructField("fieldName",StringType,true))
    )









  }

}

```



## 总结

> In this article, you have learned all different Spark SQL DataTypes, DataType, DataTypes classes and their methods using Scala examples. I would recommend referring to [DataType](https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/types/DataType.html) and [DataTypes](https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/types/DataTypes.html) API for more details.

在本文中，您使用 Scala 示例学习了所有不同的 Spark SQL 数据类型、数据类型、数据类型类及其方法。 我建议参考 [DataType](https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/types/DataType.html) 和 [DataTypes](https:// spark.apache.org/docs/latest/api/java/org/apache/spark/sql/types/DataTypes.html) API 了解更多详情。

> Thanks for reading. If you like it, please do share the article by following the below social links and any comments or suggestions are welcome in the comments sections! 

谢谢阅读。 如果您喜欢它，请通过以下社交链接分享文章，欢迎在评论部分提出任何意见或建议！
