# Spark 之 StructType & StructField 



> Spark SQL StructType & StructField classes are used to programmatically specify the schema to the DataFrame and creating complex columns like nested struct, [array](https://sparkbyexamples.com/spark/spark-array-arraytype-dataframe-column/) and [map](https://sparkbyexamples.com/spark/spark-dataframe-map-maptype-column/) columns. [StructType](https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/types/StructType.scala) is a collection of [StructField’s](https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/types/StructField.scala). Using StructField we can define column name, column data type, nullable column (boolean to specify if the field can be nullable or not) and metadata.

<font color=orange>Spark SQL StructType 和 StructField 类用于以编程方式指定 DataFrame 的架构并创建复杂的列，如嵌套结构、[array](https://sparkbyexamples.com/spark/spark-array-arraytype-dataframe-column/) 和 [Map](https://sparkbyexamples.com/spark/spark-dataframe-map-maptype-column/) 列。 [StructType](https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/types/StructType.scala)是[StructField的集合](https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/types/StructField.scala)</font>。 使用 StructField 我们可以定义列名、列数据类型、可空列（布尔值以指定字段是否可以为空）和元数据。

> In this article, we will learn different ways to define the structure of DataFrame using Spark SQL StructType with scala examples. Though Spark infers a schema from data, some times we may need to define our own column names and data types and this article explains how to define simple, nested, and complex schemas.

在本文中，我们将学习使用 Spark SQL StructType 和 scala 示例定义 DataFrame 结构的不同方法。 虽然 Spark 从数据中推断出模式，但有时我们可能需要定义自己的列名和数据类型，本文将解释如何定义简单、嵌套和复杂的模式。

## StructType – 定义Dataframe的结构

> Spark provides `spark.sql.types.StructType` class to define the structure of the DataFrame and It is a collection or list on StructField objects. By calling `printSchema()` method on the DataFrame, StructType columns are represents as “struct”.

Spark 提供了`spark.sql.types.StructType` 类来定义DataFrame 的结构，<font color=red>它是StructField 对象的集合或列表</font>。 通过在 DataFrame 上调用 printSchema() 方法，将 StructType 列表示为“struct”。

## StructField – 定义 DataFrame 列的元数据

Spark provides `spark.sql.types.StructField` class to define the column name(String), column type ([DataType](https://sparkbyexamples.com/spark/spark-sql-dataframe-data-types/)), nullable column (Boolean) and metadata (MetaData)

Spark提供`spark.sql.types.StructField`类来<font color=++red>定义列名(String)、列类型([DataType](https://sparkbyexamples.com/spark/spark-sql-dataframe-data-types/)) 、可为空的列 (Boolean) 和元数据 (MetaData)</font>

## 目录

- [1 使用 Spark StructType 和 StructField 创建DataFrame ](#1 使用 Spark StructType 和 StructField 创建DataFrame )
- [2 定义嵌套的 StructType 对象结构](#2 定义嵌套的 StructType 对象结构)
- [3 从 JSON 文件创建 StructType 对象结构](#3 从 JSON 文件创建 StructType 对象结构)
- [4 添加和更改 DataFrame 的结构](#4 添加和更改 DataFrame 的结构)
- [5 使用 SQL ArrayType 和 MapType](#5 使用 SQL ArrayType 和 MapType)
- [6 将案例类转换为 Spark StructType](#6 将案例类转换为 Spark StructType)
- [7 从 DDL 字符串创建 StructType 对象结构](#7 从 DDL 字符串创建 StructType 对象结构)
- [8 检查 DataFrame 中是否存在字段](#8 检查 DataFrame 中是否存在字段)
- [完整代码：](#完整代码：)

## 使用 Spark StructType 和 StructField 创建DataFrame 

> While [creating a Spark DataFrame](https://sparkbyexamples.com/spark/different-ways-to-create-a-spark-dataframe/) we can specify the structure using StructType and StructField classes. As specified in the introduction, StructType is a collection of StructField’s which is used to define the column name, data type and a flag for nullable or not. Using StructField we can also add nested struct schema, [ArrayType](https://sparkbyexamples.com/spark/spark-array-arraytype-dataframe-column/) for arrays and [MapType](https://sparkbyexamples.com/spark/spark-dataframe-map-maptype-column/) for key-value pairs which we will discuss detail in later sections.

在 [创建 Spark DataFrame](https://sparkbyexamples.com/spark/different-ways-to-create-a-spark-dataframe/) 时，我们可以使用 StructType 和 StructField 类指定结构。 如介绍中所述，StructType 是 StructField 的集合，用于定义列名、数据类型和是否可以为空的标志。 <font color=oragne>使用 StructField 我们还可以添加嵌套结构模式</font>，[ArrayType](https://sparkbyexamples.com/spark/spark-array-arraytype-dataframe-column/) 用于数组和 [MapType](https://sparkbyexamples.com/ spark/spark-dataframe-map-maptype-column/) 用于键值对，我们将在后面的部分中详细讨论。

> StructType & StructField case class as follows.

StructType & StructField 案例类如下。

```scala
case class StructType(fields: Array[StructField])

case class StructField(
    name: String,
    dataType: DataType,
    nullable: Boolean = true,
    metadata: Metadata = Metadata.empty)
```

> The below example demonstrates a very simple example of how to create a struct using StructType & StructField on DataFrame and it’s usage with sample data to support it.

下面的示例演示了一个非常简单的示例，该示例演示了如何在 DataFrame 上使用 StructType 和 StructField 创建结构，并使用示例数据来支持它。

```scala
val simpleData = Seq(Row("James ","","Smith","36636","M",3000),
    Row("Michael ","Rose","","40288","M",4000),
    Row("Robert ","","Williams","42114","M",4000),
    Row("Maria ","Anne","Jones","39192","F",4000),
    Row("Jen","Mary","Brown","","F",-1)
  )

val simpleSchema = StructType(Array(
    StructField("firstname",StringType,true),
    StructField("middlename",StringType,true),
    StructField("lastname",StringType,true),
    StructField("id", StringType, true),
    StructField("gender", StringType, true),
    StructField("salary", IntegerType, true)
  ))

  val df = spark.createDataFrame(
      spark.sparkContext.parallelize(simpleData),simpleSchema)
  df.printSchema()
  df.show()
```

> By running the above snippet, it displays below outputs.

通过运行上面的代码片段，它会显示在下面的输出中。

```scala
root
 |-- firstname: string (nullable = true)
 |-- middlename: string (nullable = true)
 |-- lastname: string (nullable = true)
 |-- id: string (nullable = true)
 |-- gender: string (nullable = true)
 |-- salary: integer (nullable = true)

+---------+----------+--------+-----+------+------+
|firstname|middlename|lastname|   id|gender|salary|
+---------+----------+--------+-----+------+------+
|   James |          |   Smith|36636|     M|  3000|
| Michael |      Rose|        |40288|     M|  4000|
|  Robert |          |Williams|42114|     M|  4000|
|   Maria |      Anne|   Jones|39192|     F|  4000|
|      Jen|      Mary|   Brown|     |     F|    -1|
+---------+----------+--------+-----+------+------+
```



## 定义嵌套的 StructType 对象结构

> While working on DataFrame we often need to work with the nested struct column and this can be defined using SQL StructType.

在处理 DataFrame 时，我们经常需要使用嵌套的结构列，这可以使用 SQL StructType 来定义。

> On the below example I have instantiated StructType and use add method (instead of StructField) to add column names and datatype. Notice that for column “name” data type is StructType which is nested.

在下面的示例中，我实例化了 StructType 并使用 add 方法（而不是 StructField）来添加列名和数据类型。 请注意，对于列“名称”，数据类型是嵌套的 StructType。

```scala
  val structureData = Seq(
    Row(Row("James ","","Smith"),"36636","M",3100),
    Row(Row("Michael ","Rose",""),"40288","M",4300),
    Row(Row("Robert ","","Williams"),"42114","M",1400),
    Row(Row("Maria ","Anne","Jones"),"39192","F",5500),
    Row(Row("Jen","Mary","Brown"),"","F",-1)
  )

  val structureSchema = new StructType()
    .add("name",new StructType()
      .add("firstname",StringType)
      .add("middlename",StringType)
      .add("lastname",StringType))
    .add("id",StringType)
    .add("gender",StringType)
    .add("salary",IntegerType)

  val df2 = spark.createDataFrame(
     spark.sparkContext.parallelize(structureData),structureSchema)
  df2.printSchema()
  df2.show()
```



Outputs below schema and the DataFrame

```scala
root
 |-- name: struct (nullable = true)
 |    |-- firstname: string (nullable = true)
 |    |-- middlename: string (nullable = true)
 |    |-- lastname: string (nullable = true)
 |-- id: string (nullable = true)
 |-- gender: string (nullable = true)
 |-- salary: integer (nullable = true)

+--------------------+-----+------+------+
|                name|   id|gender|salary|
+--------------------+-----+------+------+
|   [James , , Smith]|36636|     M|  3100|
|  [Michael , Rose, ]|40288|     M|  4300|
|[Robert , , Willi...|42114|     M|  1400|
|[Maria , Anne, Jo...|39192|     F|  5500|
|  [Jen, Mary, Brown]|     |     F|    -1|
+--------------------+-----+------+------+
```



## 从 JSON 文件创建 StructType 对象结构

> If you have too many columns and the structure of the DataFrame changes now and then, it’s a good practice to load the SQL StructType schema from JSON file. Note the definition in JSON uses the different layout and you can get this by using `schema.prettyJson()` 

如果您有太多列并且 DataFrame 的结构不时发生变化，那么从 JSON 文件加载 SQL StructType 模式是一个很好的做法。 注意 JSON 中的定义使用了不同的布局，你可以通过使用 `schema.prettyJson()` 来获得它

```json
{
  "type" : "struct",
  "fields" : [ {
    "name" : "name",
    "type" : {
      "type" : "struct",
      "fields" : [ {
        "name" : "firstname",
        "type" : "string",
        "nullable" : true,
        "metadata" : { }
      }, {
        "name" : "middlename",
        "type" : "string",
        "nullable" : true,
        "metadata" : { }
      }, {
        "name" : "lastname",
        "type" : "string",
        "nullable" : true,
        "metadata" : { }
      } ]
    },
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "dob",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "gender",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "salary",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
```



```scala
  val url = ClassLoader.getSystemResource("schema.json")
  val schemaSource = Source.fromFile(url.getFile).getLines.mkString
  val schemaFromJson = DataType.fromJson(schemaSource).asInstanceOf[StructType]
  val df3 = spark.createDataFrame(
        spark.sparkContext.parallelize(structureData),schemaFromJson)
  df3.printSchema()
```

> This prints the same output as the previous section. You can also, have a name, type, and flag for nullable in a comma-separated file and we can use these to create a StructType programmatically, I will leave this to you to explore.

这将打印与上一节相同的输出。 您还可以在逗号分隔的文件中为可为空的文件提供名称、类型和标志，我们可以使用这些以编程方式创建 StructType，我将留给您探索。

## 添加和更改 DataFrame 的结构

> Using [Spark SQL function](https://sparkbyexamples.com/spark/spark-sql-functions-understanding/) struct(), we can change the struct of the existing DataFrame and add a new StructType to it. The below example demonstrates how to copy the columns from one structure to another and adding a new column.

使用 [Spark SQL 函数](https://sparkbyexamples.com/spark/spark-sql-functions-understanding/) struct()，我们可以更改现有 DataFrame 的结构并为其添加新的 StructType。 下面的示例演示如何将列从一个结构复制到另一个结构并添加新列。

```scala
 val updatedDF = df4.withColumn("OtherInfo", 
    struct(  col("id").as("identifier"),
    col("gender").as("gender"),
    col("salary").as("salary"),
    when(col("salary").cast(IntegerType) &lt 2000,"Low")
      .when(col("salary").cast(IntegerType) &lt 4000,"Medium")
      .otherwise("High").alias("Salary_Grade")
  )).drop("id","gender","salary")

  updatedDF.printSchema()
  updatedDF.show(false)
```

> Here, it copies “`gender`“, “`salary`” and “`id`” to the new struct “`otherInfo`” and add’s a new column “`Salary_Grade`“.

在这里，它将“`gender`”、“`salary`”和“`id`”复制到新结构“`otherInfo`”并添加一个新列“`Salary_Grade`”。

```scala
root
 |-- name: struct (nullable = true)
 |    |-- firstname: string (nullable = true)
 |    |-- middlename: string (nullable = true)
 |    |-- lastname: string (nullable = true)
 |-- OtherInfo: struct (nullable = false)
 |    |-- identifier: string (nullable = true)
 |    |-- gender: string (nullable = true)
 |    |-- salary: integer (nullable = true)
 |    |-- Salary_Grade: string (nullable = false)
```



## 使用 SQL ArrayType 和 MapType

> SQL StructType also supports [ArrayType](https://sparkbyexamples.com/spark/spark-array-arraytype-dataframe-column/) and [MapType](https://sparkbyexamples.com/spark/spark-dataframe-map-maptype-column/) to define the DataFrame columns for array and map collections respectively. On the below example, column “hobbies” defined as ArrayType(StringType) and “properties” defined as MapType(StringType,StringType) meaning both key and value as String.

SQL StructType 还支持 [ArrayType](https://sparkbyexamples.com/spark/spark-array-arraytype-dataframe-column/) 和 [MapType](https://sparkbyexamples.com/spark/spark-dataframe-map- maptype-column/) 分别为数组和Map集合定义 DataFrame 列。 在下面的示例中，列“爱好”定义为 `ArrayType(StringType)`，“属性”定义为 `MapType(StringType,StringType)`，表示键和值都为字符串。

```scala
  val arrayStructureData = Seq(
    Row(Row("James ","","Smith"),List("Cricket","Movies"),Map("hair"->"black","eye"->"brown")),
    Row(Row("Michael ","Rose",""),List("Tennis"),Map("hair"->"brown","eye"->"black")),
    Row(Row("Robert ","","Williams"),List("Cooking","Football"),Map("hair"->"red","eye"->"gray")),
    Row(Row("Maria ","Anne","Jones"),null,Map("hair"->"blond","eye"->"red")),
    Row(Row("Jen","Mary","Brown"),List("Blogging"),Map("white"->"black","eye"->"black"))
  )

  val arrayStructureSchema = new StructType()
    .add("name",new StructType()
      .add("firstname",StringType)
      .add("middlename",StringType)
      .add("lastname",StringType))
    .add("hobbies", ArrayType(StringType))
    .add("properties", MapType(StringType,StringType))

  val df5 = spark.createDataFrame(
     spark.sparkContext.parallelize(arrayStructureData),arrayStructureSchema)
  df5.printSchema()
  df5.show()
```

> Outputs the below schema and the DataFrame data. Note that field “`Hobbies`” is array type and “`properties`” is map type.

输出以下模式和 DataFrame 数据。 请注意，字段“`Hobbies`”是数组类型，“`properties`”是地图类型。

```scala
root
 |-- name: struct (nullable = true)
 |    |-- firstname: string (nullable = true)
 |    |-- middlename: string (nullable = true)
 |    |-- lastname: string (nullable = true)
 |-- hobbies: array (nullable = true)
 |    |-- element: string (containsNull = true)
 |-- properties: map (nullable = true)
 |    |-- key: string
 |    |-- value: string (valueContainsNull = true)

+---------------------+-------------------+------------------------------+
|name                 |hobbies            |properties                    |
+---------------------+-------------------+------------------------------+
|[James , , Smith]    |[Cricket, Movies]  |[hair -> black, eye -> brown] |
|[Michael , Rose, ]   |[Tennis]           |[hair -> brown, eye -> black] |
|[Robert , , Williams]|[Cooking, Football]|[hair -> red, eye -> gray]    |
|[Maria , Anne, Jones]|null               |[hair -> blond, eye -> red]   |
|[Jen, Mary, Brown]   |[Blogging]         |[white -> black, eye -> black]|
+---------------------+-------------------+------------------------------+
```



## 将案例类转换为 Spark StructType

> Spark SQL also provides Encoders to convert case class to StructType object. If you are using older versions of Spark, you can also transform the case class to the schema using the Scala hack. Both examples are present here.

Spark SQL 还提供了编码器来将案例类转换为 StructType 对象。 如果您使用的是旧版本的 Spark，您还可以使用 Scala hack 将案例类转换为模式。 这两个例子都在这里。

```scala
  case class Name(first:String,last:String,middle:String)
  case class Employee(fullName:Name,age:Integer,gender:String)

  import org.apache.spark.sql.catalyst.ScalaReflection
  val schema = ScalaReflection.schemaFor[Employee].dataType.asInstanceOf[StructType]

  val encoderSchema = Encoders.product[Employee].schema
  encoderSchema.printTreeString()
```

> printTreeString() outputs the below schema.

printTreeString() 输出以下模式。

```scala
root
 |-- fullName: struct (nullable = true)
 |    |-- first: string (nullable = true)
 |    |-- last: string (nullable = true)
 |    |-- middle: string (nullable = true)
 |-- age: integer (nullable = true)
 |-- gender: string (nullable = true)
```



## 从 DDL 字符串创建 StructType 对象结构

> Like loading structure from JSON string, we can also create it from DLL ( by using `fromDDL()` static function on SQL StructType class `StructType.fromDDL`). You can also generate DDL from a schema using `toDDL()`. printTreeString() on struct object prints the schema similar to `printSchema`function returns.

就像从 JSON 字符串加载结构一样，我们也可以从 DLL 中创建它（通过在 SQL StructType 类 `StructType.fromDDL` 上使用`fromDDL()` 静态函数）。 您还可以使用 `toDDL()` 从模式生成 DDL。 struct 对象上的 printTreeString() 打印类似于 `printSchema` 函数返回的模式。

```scala
 val ddlSchemaStr = "`fullName` STRUCT<`first`: STRING, `last`: STRING,
 `middle`: STRING>,`age` INT,`gender` STRING"
  val ddlSchema = StructType.fromDDL(ddlSchemaStr)
  ddlSchema.printTreeString()
```



## 检查 DataFrame 中是否存在字段

> If you want to perform some checks on metadata of the DataFrame, for example, if a column or field exists in a DataFrame or data type of column; we can easily do this using several functions on SQL StructType and StructField.

如果要对DataFrame的元数据进行一些检查，例如，DataFrame中是否存在列或字段或列的数据类型； 我们可以使用 SQL StructType 和 StructField 上的几个函数轻松地做到这一点。

```scala
println(df.schema.fieldNames.contains("firstname"))
println(df.schema.contains(StructField("firstname",StringType,true)))
```

This example returns “true” for both scenarios. And for the second one if you have IntegetType instead of StringType it returns false as the datatype for first name column is String, as it checks every property ins field. Similarly, you can also check if two schemas are equal and more.

> 此示例在两种情况下都返回“true”。 对于第二个，如果你有 IntegetType 而不是 StringType，它会返回 false，因为名字列的数据类型是 String，因为它会检查每个属性 ins 字段。 同样，您还可以检查两个模式是否相等或更多。

## 完整代码

```scala
package sparkScalaExamples.SQL
import java.net.URL
import javax.xml.transform.stream.StreamSource
import org.apache.spark.sql.{DataFrame, Encoders, Row, SparkSession}
import org.apache.spark.sql.types.{ArrayType, DataType, DataTypes, DoubleType, IntegerType, MapType, StringType, StructField, StructType}
import org.apache.spark.sql.functions.{col, from_json, struct, when}
import scala.io.Source

case class Name(first:String,last:String,middle:String)
case class Employee(fullName:Name,age:Integer,gender:String)

object StructTypeAndStructField {
  def main(args: Array[String]): Unit = {
    val spark: SparkSession =
      SparkSession.builder()
        .appName("StructTypeAndStructFieldDemo")
        .master("local")
        .getOrCreate()
    /**
      * 1 定义dataframe的数据结构
      * */
    val simpleData = Seq(Row("James ","","Smith","36636","M",3000),
      Row("Michael ","Rose","","40288","M",4000),
      Row("Robert ","","Williams","42114","M",4000),
      Row("Maria ","Anne","Jones","39192","F",4000),
      Row("Jen","Mary","Brown","","F",-1)
    )
    //定义dataframe的每一列书籍类型以及列名
    val simpleSchema = StructType(Array(
      StructField("firstname",StringType,true),
      StructField("middlename",StringType,true),
      StructField("lastname",StringType,true),
      StructField("id", StringType, true),
      StructField("gender", StringType, true),
      StructField("salary", IntegerType, true)
    ))
    //创建dataframe
    val df: DataFrame =
      spark.createDataFrame(
        spark.sparkContext.parallelize(simpleData),simpleSchema)
    println("-------基础-------")
    df.printSchema()//展示dataframe的数据结构
    df.show()//展示数据
    
    /**2 构建具有内嵌的dataframe数据结构
      * */
    val structureData = Seq(
      Row(Row("James ","","Smith"),"36636","M",3100),
      Row(Row("Michael ","Rose",""),"40288","M",4300),
      Row(Row("Robert ","","Williams"),"42114","M",1400),
      Row(Row("Maria ","Anne","Jones"),"39192","F",5500),
      Row(Row("Jen","Mary","Brown"),"","F",-1)
    )

    val structureSchema = new StructType()
      .add("name",new StructType()
        .add("firstname",StringType)
        .add("middlename",StringType)
        .add("lastname",StringType))
      .add("id",StringType)
      .add("gender",StringType)
      .add("salary",IntegerType)

    val df2: DataFrame = spark.createDataFrame(
      spark.sparkContext.parallelize(structureData),structureSchema)
    println("------内嵌----------")
    df2.printSchema()
    df2.show()

    /**
      * 3 使用json构建dataframe结构
      * */

    val url: URL = ClassLoader.getSystemResource("schema.json")
    val schemaSource: String = Source.fromFile(url.getFile).getLines().mkString
    val schemaFromJson: StructType = DataType.fromJson(schemaSource).asInstanceOf[StructType]
    val df3: DataFrame = spark.createDataFrame(
      spark.sparkContext.parallelize(structureData), schemaFromJson
    )
    println("-----从json创建dataframe的数据格式------")
    df3.printSchema()
    df3.show()
    /**
      * 4 添加和更改 DataFrame 的结构
      * */
    val schemaFromCase = StructType(Array(
      StructField("name", StructType(Array(
        StructField("firstname",StringType,true),
        StructField("middlename",StringType,true),
        StructField("lastname",StringType,true))),true),
      StructField("id", StringType, true),
      StructField("gender", StringType, true),
      StructField("salary", IntegerType, true)
    ))
    val df4 = spark.createDataFrame(spark.sparkContext.parallelize(structureData),schemaFromCase)
    df4.printSchema()
    println("----更改之前的数据----")
    df4.show()

    import org.apache.spark.sql.functions._

    val updatedDF = df4.withColumn("OtherInfo", struct(  col("id").as("identifier"),
      col("gender").as("gender"),
      col("salary").as("salary"),
      when(col("salary").cast(IntegerType) < 2000,"Low")
        .when(col("salary").cast(IntegerType) < 4000,"Medium")
        .otherwise("High").alias("Salary_Grade")
    )).drop("id","gender","salary")
    println("---更改之后的数据-------")
    updatedDF.printSchema()
    updatedDF.show(false)


    /**
      * 5 在dataframe中使用ArrayType和Mao等数据结构
      * Using SQL ArrayType and MapType
      * */
    val arrayStructureData = Seq(
      Row(Row("James ","","Smith"),List("Cricket","Movies"),Map("hair"->"black","eye"->"brown")),
      Row(Row("Michael ","Rose",""),List("Tennis"),Map("hair"->"brown","eye"->"black")),
      Row(Row("Robert ","","Williams"),List("Cooking","Football"),Map("hair"->"red","eye"->"gray")),
      Row(Row("Maria ","Anne","Jones"),null,Map("hair"->"blond","eye"->"red")),
      Row(Row("Jen","Mary","Brown"),List("Blogging"),Map("white"->"black","eye"->"black"))
    )

    val arrayStructureShcmea: StructType = new StructType()
      .add("name", new StructType()
        .add("firstname", StringType)
        .add("middlename", StringType)
        .add("lastname", StringType))
      .add("hobbies", ArrayType(StringType))
      .add("properties", MapType(StringType, StringType))

    val df5: DataFrame =
      spark.createDataFrame(
        spark.sparkContext.parallelize(arrayStructureData),arrayStructureShcmea)
    df5.printSchema()
    df5.show()

    /**6 使用样例类，构建dataframe的数据结构
      * */

    import org.apache.spark.sql.catalyst.ScalaReflection
    val schema: StructType = ScalaReflection.schemaFor[Employee].dataType.asInstanceOf[StructType]

    val encoderSchema: StructType = Encoders.product[Employee].schema
    println("---------从样例类构建Dataframe的结构-------")
    encoderSchema.printTreeString()
    /**
      * 7 使用DDl语法构建dataframe的数据结构
      * */
    /* Creating StructType schema from String DDL */
    val ddlSchemaStr = "`fullName` STRUCT<`first`: STRING, `last`: STRING, `middle`: STRING>,`age` INT,`gender` STRING"
    val ddlSchema = StructType.fromDDL(ddlSchemaStr)
    println("-------DDL语句-------------")
    ddlSchema.printTreeString()

    /**
      * 8 判断dataframe中是否存在列结构或列名
      * */
    println("------判断是否存在列结构或列名---------")
    println(df.schema.fieldNames.contains("firtname"))
    println(df.schema.contains(StructField("firstname",StringType,true)))


  }

}

```



## 总结

> In this article, you have learned the usage of SQL StructType, StructField and how to change the structure of the spark DataFrame at runtime, converting case class to the schema and using ArrayType, MapType.

在本文中，您学习了 SQL StructType、StructField 的用法以及如何在运行时更改 spark DataFrame 的结构、将案例类转换为 schema 并使用 ArrayType、MapType。
