# Spark 之shema

> Spark Schema defines the structure of the DataFrame which you can get by calling printSchema() method on the DataFrame object. Spark SQL provides [StructType & StructField classes](https://sparkbyexamples.com/spark/spark-sql-structtype-on-dataframe/) to programmatically specify the schema.

Spark Schema 定义了 DataFrame 的结构，您可以通过在 DataFrame 对象上调用 printSchema() 方法来获取该结构。 Spark SQL 提供 [StructType & StructField 类](https://sparkbyexamples.com/spark/spark-sql-structtype-on-dataframe/) 以编程方式指定架构。

> By default, Spark infers the schema from the data, however, sometimes we may need to define our own schema (column names and data types), especially while working with unstructured and semi-structured data, this article explains how to define simple, nested, and complex schemas with examples.

默认情况下，Spark 从数据中推断模式，但是，**有时我们可能需要定义自己的模式（列名和数据类型），尤其是在处理非结构化和半结构化数据时，本文解释了如何定义简单的、嵌套的 ，以及带有示例的复杂模式**。

## 目录

1. [Schema——定义DataFrame的结构](#1. Schema——定义DataFrame的结构)
2. [使用 StructType & StructField 创建 Schema](#使用 StructType & StructField 创建 Schema)
3. [Spark DataFrame printSchema()](#Spark DataFrame printSchema())
4. [创建嵌套结构模式](#创建嵌套结构模式)
5. [从 JSON 加载 SQL Schema](#从 JSON 加载 SQL Schema)
6. [使用Arrays & Map Columns](#使用Arrays & Map Columns)

## Schema——定义DataFrame的结构

**What is Spark Schema**

> Spark schema is the structure of the DataFrame or Dataset, we can define it using StructType class which is a collection of StructField that define the column name(String), column type (DataType), nullable column (Boolean) and metadata (MetaData)

Spark schema 是 DataFrame 或 Dataset 的结构，我们可以使用 StructType 类来定义它，<font color=red>它是 StructField 的集合，定义了列名（String）、列类型（DataType）、可空列（Boolean）和元数据（MetaData）</font>

> For the rest of the article I’ve explained by using Scala example, a similar method could be used with PySpark, and if time permits I will cover it in the future. If you are looking for PySpark, I would still recommend reading through this article as it would give you an idea of its usage.

对于本文的其余部分，使用 Scala 示例进行了解释，PySpark 可以使用类似的方法，如果时间允许，我将在以后介绍它。 如果您正在寻找 PySpark，我仍然建议您通读这篇文章，因为它会让您了解它的用法。

## 使用 StructType & StructField 创建 Schema

> While [creating a Spark DataFrame](https://sparkbyexamples.com/spark/different-ways-to-create-a-spark-dataframe/) we can specify the schema using StructType and StructField classes. we can also add nested struct StructType, [ArrayType](https://sparkbyexamples.com/spark/spark-array-arraytype-dataframe-column/) for arrays, and [MapType for key-value pairs](https://sparkbyexamples.com/spark/spark-dataframe-map-maptype-column/) which we will discuss in detail in later sections.

在 [创建 Spark DataFrame](https://sparkbyexamples.com/spark/different-ways-to-create-a-spark-dataframe/) 时，我们可以使用 StructType 和 StructField 类指定架构。 我们还可以添加嵌套结构 `StructType`、`ArrayType`用于数组，以及 `MapType` 用于键值对],我们将在后面的部分中详细讨论。

> Spark defines StructType & StructField case class as follows.

Spark 定义了 StructType & StructField 案例类如下。

```scala
case class StructType(fields: Array[StructField])

case class StructField(
    name: String,
    dataType: DataType,
    nullable: Boolean = true,
    metadata: Metadata = Metadata.empty)
```

> The below example demonstrates a very simple example of using StructType & StructField on DataFrame and its usage with sample data to support it.

下面的示例演示了在 DataFrame 上使用 StructType 和 StructField 的一个非常简单的示例，以及它与示例数据的使用来支持它。

```scala
import org.apache.spark.sql.types.{IntegerType,StringType,StructType,StructField}
import org.apache.spark.sql.{Row, SparkSession}

val simpleData = Seq(Row("James","","Smith","36636","M",3000),
    Row("Michael","Rose","","40288","M",4000),
    Row("Robert","","Williams","42114","M",4000),
    Row("Maria","Anne","Jones","39192","F",4000),
    Row("Jen","Mary","Brown","","F",-1)
  )

val simpleSchema = StructType(Array(
    StructField("firstname",StringType,true),
    StructField("middlename",StringType,true),
    StructField("lastname",StringType,true),
    StructField("id", StringType, true),
    StructField("gender", StringType, true),
    StructField("salary", IntegerType, true)
  ))

val df = spark.createDataFrame(
     spark.sparkContext.parallelize(simpleData),simpleSchema)
```

## Spark DataFrame printSchema()

> To get the schema of the Spark DataFrame, use `printSchema()` on DataFrame object.

要获取 Spark DataFrame 的架构，请在 DataFrame 对象上使用`printSchema()`。

```scala
df.printSchema()
df.show()
```

> From the above example, `printSchema()` prints the schema to console(`stdout`) and [show() displays the content of the Spark DataFrame](https://sparkbyexamples.com/spark/spark-show-display-dataframe-contents-in-table/).

从上面的例子中，`printSchema()` 将模式打印到控制台(`stdout`) 并且 [show() 显示 Spark DataFrame 的内容]数据.

```scala
root
 |-- firstname: string (nullable = true)
 |-- middlename: string (nullable = true)
 |-- lastname: string (nullable = true)
 |-- id: string (nullable = true)
 |-- gender: string (nullable = true)
 |-- salary: integer (nullable = true)

+---------+----------+--------+-----+------+------+
|firstname|middlename|lastname|   id|gender|salary|
+---------+----------+--------+-----+------+------+
|    James|          |   Smith|36636|     M|  3000|
|  Michael|      Rose|        |40288|     M|  4000|
|   Robert|          |Williams|42114|     M|  4000|
|    Maria|      Anne|   Jones|39192|     F|  4000|
|      Jen|      Mary|   Brown|     |     F|    -1|
+---------+----------+--------+-----+------+------+
```

## 创建嵌套结构模式

> While working on Spark DataFrame we often need to work with the nested struct columns. On the below example I am using a different approach to instantiating StructType and use add method (instead of StructField) to add column names and datatype.

在处理 Spark DataFrame 时，我们经常需要使用嵌套的结构列。 在下面的示例中，我使用不同的方法来实例化 StructType 并使用 add 方法（而不是 StructField）来添加列名和数据类型。

```scala
val structureData = Seq(
    Row(Row("James","","Smith"),"36636","M",3100),
    Row(Row("Michael","Rose",""),"40288","M",4300),
    Row(Row("Robert","","Williams"),"42114","M",1400),
    Row(Row("Maria","Anne","Jones"),"39192","F",5500),
    Row(Row("Jen","Mary","Brown"),"","F",-1)
)

val structureSchema = new StructType()
    .add("name",new StructType()
      .add("firstname",StringType)
      .add("middlename",StringType)
      .add("lastname",StringType))
    .add("id",StringType)
    .add("gender",StringType)
    .add("salary",IntegerType)

val df2 = spark.createDataFrame(
    spark.sparkContext.parallelize(structureData),structureSchema)
df2.printSchema()
df2.show()
```

> Prints below schema and DataFrame. Note that printSchema() displays `struct` for nested structure fields.

打印dataframe的数据结构以及内容 。 请注意， printSchema() 为嵌套结构字段显示 `struct`。

```scala
root
 |-- name: struct (nullable = true)
 |    |-- firstname: string (nullable = true)
 |    |-- middlename: string (nullable = true)
 |    |-- lastname: string (nullable = true)
 |-- id: string (nullable = true)
 |-- gender: string (nullable = true)
 |-- salary: integer (nullable = true)

+--------------------+-----+------+------+
|                name|   id|gender|salary|
+--------------------+-----+------+------+
|    [James, , Smith]|36636|     M|  3100|
|   [Michael, Rose, ]|40288|     M|  4300|
| [Robert, , Willi...|42114|     M|  1400|
| [Maria, Anne, Jo...|39192|     F|  5500|
|  [Jen, Mary, Brown]|     |     F|    -1|
+--------------------+-----+------+------+
```

## 从 JSON 加载 SQL Schema

> If you have too many fields and the structure of the DataFrame changes now and then, it’s a good practice to load the SQL schema from JSON file. Note the definition in JSON uses the different layout and you can get this by using `schema.prettyJson()` 

如果您有太多字段并且 DataFrame 的结构不时发生变化，那么从 JSON 文件加载 SQL 模式是一个很好的做法。 注意 JSON 中的定义使用了不同的布局，你可以通过使用 `schema.prettyJson()` 来获得它

```json
{
  "type" : "struct",
  "fields" : [ {
    "name" : "name",
    "type" : {
      "type" : "struct",
      "fields" : [ {
        "name" : "firstname",
        "type" : "string",
        "nullable" : true,
        "metadata" : { }
      }, {
        "name" : "middlename",
        "type" : "string",
        "nullable" : true,
        "metadata" : { }
      }, {
        "name" : "lastname",
        "type" : "string",
        "nullable" : true,
        "metadata" : { }
      } ]
    },
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "dob",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "gender",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "salary",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
```



```scala
val url = ClassLoader.getSystemResource("schema.json")
val schemaSource = Source.fromFile(url.getFile).getLines.mkString
val schemaFromJson = DataType.fromJson(schemaSource).asInstanceOf[StructType]
val df3 = spark.createDataFrame(
      spark.sparkContext.parallelize(structureData),schemaFromJson)
df3.printSchema()
```

> This prints the same output as the previous section. You can also, have a name, type, and flag for nullable in a comma-separated file and we can use these to create a struct programmatically, I will leave this to you to explore.

这将打印与上一节相同的输出。 您还可以在逗号分隔的文件中为 nullable 指定名称、类型和标志。

## 使用Arrays & Map Columns

> Spark SQL also supports [ArrayType](https://sparkbyexamples.com/spark/spark-array-arraytype-dataframe-column/) and [MapType](https://sparkbyexamples.com/spark/spark-dataframe-map-maptype-column/) to define the schema with array and map collections respectively. On the below example, column “hobbies” defined as ArrayType(StringType) and “properties” defined as MapType(StringType,StringType) meaning both key and value as String.

Spark SQL 还支持 [ArrayType](https://sparkbyexamples.com/spark/spark-array-arraytype-dataframe-column/) 和 [MapType](https://sparkbyexamples.com/spark/spark-dataframe-map- maptype-column/) 分别用数组和Map集合定义模式。 在下面的示例中，列“爱好”定义为 ArrayType(StringType)，“属性”定义为 MapType(StringType,StringType)，表示键和值都为字符串。

```scala
val arrayStructureData = Seq(
    Row(Row("James","","Smith"),List("Cricket","Movies"),Map("hair"->"black","eye"->"brown")),
    Row(Row("Michael","Rose",""),List("Tennis"),Map("hair"->"brown","eye"->"black")),
    Row(Row("Robert","","Williams"),List("Cooking","Football"),Map("hair"->"red","eye"->"gray")),
    Row(Row("Maria","Anne","Jones"),null,Map("hair"->"blond","eye"->"red")),
    Row(Row("Jen","Mary","Brown"),List("Blogging"),Map("white"->"black","eye"->"black"))
)

val arrayStructureSchema = new StructType()
    .add("name",new StructType()
      .add("firstname",StringType)
      .add("middlename",StringType)
      .add("lastname",StringType))
    .add("hobbies", ArrayType(StringType))
    .add("properties", MapType(StringType,StringType))

val df5 = spark.createDataFrame(
   spark.sparkContext.parallelize(arrayStructureData),arrayStructureSchema)
df5.printSchema()
df5.show()
```

> Outputs the below schema and the DataFrame data. Note that field `Hobbies` is an array type and `properties` is map type.

输出以下模式和 DataFrame 数据。 请注意，“爱好”字段是数组类型，“属性”是Map类型。

```scala
root
 |-- name: struct (nullable = true)
 |    |-- firstname: string (nullable = true)
 |    |-- middlename: string (nullable = true)
 |    |-- lastname: string (nullable = true)
 |-- hobbies: array (nullable = true)
 |    |-- element: string (containsNull = true)
 |-- properties: map (nullable = true)
 |    |-- key: string
 |    |-- value: string (valueContainsNull = true)

+---------------------+-------------------+------------------------------+
|name                 |hobbies            |properties                    |
+---------------------+-------------------+------------------------------+
|[James, , Smith]    |[Cricket, Movies]  |[hair -> black, eye -> brown] |
|[Michael, Rose, ]   |[Tennis]           |[hair -> brown, eye -> black] |
|[Robert, , Williams]|[Cooking, Football]|[hair -> red, eye -> gray]    |
|[Maria, Anne, Jones]|null               |[hair -> blond, eye -> red]   |
|[Jen, Mary, Brown]   |[Blogging]         |[white -> black, eye -> black]|
+---------------------+-------------------+------------------------------+
```

## 将案例类转换为 Spark StructType

> Spark SQL also provides Encoders to convert case class to StructType object. If you are using older versions of Spark, you can also transform the case class to the schema using the Scala hack. Both examples are present here.

Spark SQL 还提供了编码器来将案例类转换为 StructType 对象。 如果您使用的是旧版本的 Spark，您还可以使用 Scala hack 将案例类转换为模式。 这两个例子都在这里。

```scala
  case class Name(first:String,last:String,middle:String)
  case class Employee(fullName:Name,age:Integer,gender:String)

  import org.apache.spark.sql.catalyst.ScalaReflection
  val schema = ScalaReflection.schemaFor[Employee].dataType.asInstanceOf[StructType]

  val encoderSchema = Encoders.product[Employee].schema
  encoderSchema.printTreeString()
```

> printTreeString() outputs the below schema.

printTreeString() 输出以下模式。

```scala
root
 |-- fullName: struct (nullable = true)
 |    |-- first: string (nullable = true)
 |    |-- last: string (nullable = true)
 |    |-- middle: string (nullable = true)
 |-- age: integer (nullable = true)
 |-- gender: string (nullable = true)
```



## 从 DDL 字符串创建 StructType 对象结构

> Like loading structure from JSON string, we can also create it from DLL ( by using `fromDDL()` static function on SQL StructType class `StructType.fromDDL`). You can also generate DDL from a schema using `toDDL()`. printTreeString() on struct object prints the schema similar to `printSchema`function returns.

就像从 JSON 字符串加载结构一样，我们也可以从 DLL 中创建它（通过在 SQL StructType 类 `StructType.fromDDL` 上使用`fromDDL()` 静态函数）。 您还可以使用 `toDDL()` 从模式生成 DDL。 struct 对象上的 printTreeString() 打印类似于 `printSchema` 函数返回的模式。

```scala
 val ddlSchemaStr = "`fullName` STRUCT<`first`: STRING, `last`: STRING,
 `middle`: STRING>,`age` INT,`gender` STRING"
  val ddlSchema = StructType.fromDDL(ddlSchemaStr)
  ddlSchema.printTreeString()
```



## 检查 DataFrame 中是否存在字段

> If you want to perform some checks on metadata of the DataFrame, for example, if a column or field exists in a DataFrame or data type of column; we can easily do this using several functions on SQL StructType and StructField.

如果要对DataFrame的元数据进行一些检查，例如，DataFrame中是否存在列或字段或列的数据类型； 我们可以使用 SQL StructType 和 StructField 上的几个函数轻松地做到这一点。

```scala
println(df.schema.fieldNames.contains("firstname"))
println(df.schema.contains(StructField("firstname",StringType,true)))
```

This example returns “true” for both scenarios. And for the second one if you have IntegetType instead of StringType it returns false as the datatype for first name column is String, as it checks every property ins field. Similarly, you can also check if two schemas are equal and more.

> 此示例在两种情况下都返回“true”。 对于第二个，如果你有 IntegetType 而不是 StringType，它会返回 false，因为名字列的数据类型是 String，因为它会检查每个属性 ins 字段。 同样，您还可以检查两个模式是否相等或更多。

##  总结:

> In this article, you have learned the usage of Spark SQL schema, create it programmatically using StructType and StructField, convert case class to the schema, using ArrayType, MapType, and finally how to display the DataFrame schema using printSchema() and printTreeString().

在本文中，您学习了 Spark SQL 模式的用法，使用 StructType 和 StructField 以编程方式创建它，将案例类转换为模式，使用 ArrayType、MapType，最后如何使用 printSchema() 和 printTreeString() 显示 DataFrame 模式 .

