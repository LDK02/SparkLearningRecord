# spark Dataframe Group By

> Similar to SQL “GROUP BY” clause, Spark groupBy() function is used to collect the identical data into groups on DataFrame/Dataset and perform aggregate functions on the grouped data. In this article, I will explain several `groupBy()` examples with the Scala language.

与 SQL 的“GROUP BY”子句类似，Spark groupBy() 函数用于将相同的数据收集到 DataFrame/Dataset 上的组中，并对分组后的数据执行聚合函数。 在本文中，我将使用 Scala 语言解释几个 `groupBy()` 示例。

**Syntax:**

```scala
groupBy(col1 : scala.Predef.String, cols : scala.Predef.String*) :
      org.apache.spark.sql.RelationalGroupedDataset
```

> When we perform `groupBy()` on Spark Dataframe, it returns `RelationalGroupedDataset` object which contains below aggregate functions.

当我们在 Spark Dataframe 上执行 `groupBy()` 时，它会返回 `RelationalGroupedDataset` 对象，其中包含以下聚合函数。

`count() -` 返回每个组的行数。

`mean() -` 返回每组值的平均值。

`max() -` 返回每个组的最大值。

`min() -` 返回每个组的最小值。

`sum() -` 返回每个组的值的总和。

`avg() -` 返回每组值的平均值。

`agg() -` 使用 [agg()](https://sparkbyexamples.com/spark/using-groupby-on-dataframe/#agg) 函数，我们可以一次计算多个聚合。

`pivot() -` 这个函数用于对 DataFrame 进行 Pivot，可参考文章 [Pivot & Unvot DataFrame](https://sparkbyexamples.com/spark/how- to-pivot-table-and-unpivot-a-spark-dataframe/)。

## 准备数据和Dataframe

在开始之前，让我们从要处理的数据序列中[创建 DataFrame](https://sparkbyexamples.com/spark/different-ways-to-create-a-spark-dataframe/)。 此 DataFrame 包含列“`employee_name`”、“`department`”、“`state`”、“`salary`”、“`age`”和“`bonus`”列。

> Before we start, let’s [create the DataFrame](https://sparkbyexamples.com/spark/different-ways-to-create-a-spark-dataframe/) from a sequence of the data to work with. This DataFrame contains columns “`employee_name`”, “`department`”, “`state`“, “`salary`”, “`age`” and “`bonus`” columns.

我们将使用这个 Spark DataFrame 在“部门”列上运行 groupBy()，并分别使用 min()、max() 和 sum() 聚合函数计算每个组的最小值、最大值、平均值、总工资等聚合。 最后，我们还将看到如何在多个列上进行分组和聚合。

> We will use this Spark DataFrame to run groupBy() on “department” columns and calculate aggregates like minimum, maximum, average, total salary for each group using min(), max() and sum() aggregate functions respectively. and finally, we will also see how to do group and aggregate on multiple columns.

```scala
import spark.implicits._
val simpleData = Seq(("James","Sales","NY",90000,34,10000),
    ("Michael","Sales","NY",86000,56,20000),
    ("Robert","Sales","CA",81000,30,23000),
    ("Maria","Finance","CA",90000,24,23000),
    ("Raman","Finance","CA",99000,40,24000),
    ("Scott","Finance","NY",83000,36,19000),
    ("Jen","Finance","NY",79000,53,15000),
    ("Jeff","Marketing","CA",80000,25,18000),
    ("Kumar","Marketing","NY",91000,50,21000)
  )
val df = simpleData.toDF("employee_name","department","state","salary","age","bonus")
df.show()
```

Yields below output.

```scala
+-------------+----------+-----+------+---+-----+
|employee_name|department|state|salary|age|bonus|
+-------------+----------+-----+------+---+-----+
|        James|     Sales|   NY| 90000| 34|10000|
|      Michael|     Sales|   NY| 86000| 56|20000|
|       Robert|     Sales|   CA| 81000| 30|23000|
|        Maria|   Finance|   CA| 90000| 24|23000|
|        Raman|   Finance|   CA| 99000| 40|24000|
|        Scott|   Finance|   NY| 83000| 36|19000|
|          Jen|   Finance|   NY| 79000| 53|15000|
|         Jeff| Marketing|   CA| 80000| 25|18000|
|        Kumar| Marketing|   NY| 91000| 50|21000|
+-------------+----------+-----+------+---+-----+
```



## 在 DataFrame 列上使用groupBy 和aggregate

让我们对 DataFrame 的 `department` 列执行 `groupBy()`，然后使用 `sum()` 聚合函数找到每个部门的工资总和。

> Let’s do the `groupBy()` on `department` column of DataFrame and then find the sum of salary for each department using `sum() `aggregate function.

```scala
df.groupBy("department").sum("salary").show(false)
+----------+-----------+
|department|sum(salary)|
+----------+-----------+
|Sales     |257000     |
|Finance   |351000     |
|Marketing |171000     |
+----------+-----------+
```

同样，我们可以使用 count() 计算每个部门的员工人数

> Similarly, we can calculate the number of employee in each department using `count()`

```scala
df.groupBy("department").count()
```

使用`min()`计算每个部门的最低工资

> Calculate the minimum salary of each department using `min()`

```scala
df.groupBy("department").min("salary")
```

使用 `max()` 计算每个部门的最高工资

> Calculate the maximin salary of each department using `max()`

```scala
df.groupBy("department").max("salary")
```

使用`avg()`计算每个部门的平均工资

> Calculate the average salary of each department using `avg()`

```scala
df.groupBy("department").avg( "salary")
```

使用 `mean()` 计算每个部门的平均工资

> Calculate the mean salary of each department using `mean()`

```scala
df.groupBy("department").mean( "salary") 
```



## 在 DataFrame 多列上使用groupBy 和aggregate

同样，我们也可以在两个或多个 DataFrame 列上运行 groupBy 和聚合，下面的示例对 `department`、`state` 进行 group by，并对 `salary` 和 `bonus` 列进行 sum()。

> Similarly, we can also run groupBy and aggregate on two or more DataFrame columns, below example does group by on `department`,`state`and does sum() on `salary` and `bonus` columns.

```scala
//GroupBy on multiple columns
df.groupBy("department","state")
    .sum("salary","bonus")
    .show(false)
```

This yields the below output.

```scala
+----------+-----+-----------+----------+
|department|state|sum(salary)|sum(bonus)|
+----------+-----+-----------+----------+
|Finance   |NY   |162000     |34000     |
|Marketing |NY   |91000      |21000     |
|Sales     |CA   |81000      |23000     |
|Marketing |CA   |80000      |18000     |
|Finance   |CA   |189000     |47000     |
|Sales     |NY   |176000     |30000     |
+----------+-----+-----------+----------+
```

## 一次性调用多个聚合函数

使用 `agg()` 聚合函数，我们可以使用 [Spark SQL 聚合](https://sparkbyexamples.com/spark/spark-sql-aggregate-functions/) 函数 [sum( )](https://sparkbyexamples.com/spark/spark-sql-aggregate-functions/#sum), [avg()](https://sparkbyexamples.com/spark/spark-sql-aggregate-functions/# avg), [min()](https://sparkbyexamples.com/spark/spark-sql-aggregate-functions/#min), [max()](https://sparkbyexamples.com/spark/spark-sql -aggregate-functions/#max) [mean()](https://sparkbyexamples.com/spark/spark-sql-aggregate-functions/#mean) 等。 为了使用这些，我们应该导入`"import org.apache.spark.sql.functions._"`

> Using `agg()` aggregate function we can calculate many aggregations at a time on a single statement using [Spark SQL aggregate](https://sparkbyexamples.com/spark/spark-sql-aggregate-functions/) functions [sum()](https://sparkbyexamples.com/spark/spark-sql-aggregate-functions/#sum), [avg()](https://sparkbyexamples.com/spark/spark-sql-aggregate-functions/#avg), [min()](https://sparkbyexamples.com/spark/spark-sql-aggregate-functions/#min), [max()](https://sparkbyexamples.com/spark/spark-sql-aggregate-functions/#max) [mean()](https://sparkbyexamples.com/spark/spark-sql-aggregate-functions/#mean) e.t.c. In order to use these, we should import `"import org.apache.spark.sql.functions._"`

```scala
import org.apache.spark.sql.functions._
df.groupBy("department")
    .agg(
      sum("salary").as("sum_salary"),
      avg("salary").as("avg_salary"),
      sum("bonus").as("sum_bonus"),
      max("bonus").as("max_bonus"))
    .show(false)
```

这个例子对`department`列进行分组，计算每个部门的`salary`的`sum()`和`avg()`，并计算每个部门的奖金`sum()`和`max()`。

> This example does group on `department` column and calculates `sum()` and `avg()` of `salary` for each department and calculates `sum()` and `max()` of bonus for each department.

```scala
+----------+----------+-----------------+---------+---------+
|department|sum_salary|avg_salary       |sum_bonus|max_bonus|
+----------+----------+-----------------+---------+---------+
|Sales     |257000    |85666.66666666667|53000    |23000    |
|Finance   |351000    |87750.0          |81000    |24000    |
|Marketing |171000    |85500.0          |39000    |21000    |
+----------+----------+-----------------+---------+---------+
```

## 5 对聚合数据使用filter函数

与 SQL “HAVING” 子句类似，在 Spark DataFrame 上，我们可以使用 [where()](https://sparkbyexamples.com/spark/working-with-spark-dataframe-filter/) 或 [filter()](https ://sparkbyexamples.com/spark/working-with-spark-dataframe-filter/) 函数来过滤聚合数据的行。

> Similar to SQL “HAVING” clause, On Spark DataFrame we can use either [where()](https://sparkbyexamples.com/spark/working-with-spark-dataframe-filter/) or [filter()](https://sparkbyexamples.com/spark/working-with-spark-dataframe-filter/) function to filter the rows of aggregated data.

```scala
df.groupBy("department")
    .agg(
      sum("salary").as("sum_salary"),
      avg("salary").as("avg_salary"),
      sum("bonus").as("sum_bonus"),
      max("bonus").as("max_bonus"))
    .where(col("sum_bonus") >= 50000)
    .show(false)
```

这将删除少于 50000 且产量低于产出的奖金的总和。

> This removes the sum of a bonus that has less than 50000 and yields below output.

```scala
+----------+----------+-----------------+---------+---------+
|department|sum_salary|avg_salary       |sum_bonus|max_bonus|
+----------+----------+-----------------+---------+---------+
|Sales     |257000    |85666.66666666667|53000    |23000    |
|Finance   |351000    |87750.0          |81000    |24000    |
+----------+----------+-----------------+---------+---------+
```

## 6 完整代码

```scala
package sparkScalaExamples.SQL

import org.apache.spark.sql.types.{IntegerType, StringType, StructType}
import org.apache.spark.sql.{DataFrame, Row, SparkSession}

object DropDemo {
  def main(args: Array[String]): Unit = {

    val spark: SparkSession =
      SparkSession
        .builder()
        .appName("dropDemo")
        .master("local")
        .getOrCreate()
    /**
      * 准备数据
      * */
    val structureData = Seq(
      Row("James","","Smith","36636","NewYork",3100),
      Row("Michael","Rose","","40288","California",4300),
      Row("Robert","","Williams","42114","Florida",1400),
      Row("Maria","Anne","Jones","39192","Florida",5500),
      Row("Jen","Mary","Brown","34561","NewYork",3000)
    )

    val structureSchema = new StructType()
      .add("firstname",StringType)
      .add("middlename",StringType)
      .add("lastname",StringType)
      .add("id",StringType)
      .add("location",StringType)
      .add("salary",IntegerType)

    val df = spark.createDataFrame(
      spark.sparkContext.parallelize(structureData),structureSchema)
    println("-------原始数据结果：-----------")
    df.printSchema()

    /** 1.1 删除一列数据
      * */
    val df2: DataFrame = df.drop("firstname")
    println("---删除一列的结果：")
    df2.printSchema()

    /** 1.2
      * */
    println("-----第二种方式---------")
    df.drop(df("firstname")).printSchema()

    /**1.3 第三种方式
      * */
    println("-----第三种方式---------")
    import org.apache.spark.sql.functions.col
    df.drop(col("firstname")).printSchema()

    /**2
      * 一次性删除多个列，方法1
      * */
    println("--------一次性删除多个列-方法1-----------")
    df.drop("firstname","middlename","lastname").printSchema()

    /**3 一次性删除多个列，方法2
      * */
    val cols = Seq("firstname","middlename","lastname")
    println("--------一次性删除多个列-方法12-----------")
    df.drop(cols:_*).printSchema()



  }

}

```

This example is also available at [GitHub](https://github.com/spark-examples/spark-scala-examples/blob/master/src/main/scala/com/sparkbyexamples/spark/dataframe/GroupbyExample.scala) project for reference.

## 7 总结：

In this tutorial, you have learned how to use `groupBy()` and aggregate functions on Spark DataFrame and also learned how to run these on multiple columns and finally filtering data on the aggregated column.

Thanks for reading. If you like it, please do share the article by following the below social links and any comments or suggestions are welcome in the comments sections! 

Happy Learning !!