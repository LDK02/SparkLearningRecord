# Spark 之Join

> Spark DataFrame supports all basic SQL Join Types like `INNER`, `LEFT OUTER`, `RIGHT OUTER`, `LEFT ANTI`, `LEFT SEMI`, `CROSS`, `SELF` JOIN. Spark SQL Joins are wider transformations that result in data shuffling over the network hence they have [huge performance issues](https://sparkbyexamples.com/spark/spark-performance-tuning/) when not designed with care.

Spark DataFrame 支持所有基本的 SQL 连接类型，例如 `INNER`、`LEFT OUTER`、`RIGHT OUTER`、`LEFT ANTI`、`LEFT SEMI`、`CROSS`、`SELF` JOIN。 Spark SQL 连接是更广泛的转换，会导致网络上的数据混洗，<font color=red>因此如果设计不小心，它们会出现 [巨大的性能问题](https://sparkbyexamples.com/spark/spark-performance-tuning/)</font>。

> On the other hand Spark SQL Joins comes with more optimization by default (thanks to DataFrames & Dataset) however still there would be some performance issues to consider while using.

另一方面，Spark SQL Joins 默认提供更多优化，但是在使用时仍然需要考虑一些性能问题。

在本教程中，您将学习不同的 Join 语法，并使用 Scala 示例在两个 DataFrames 和 Datasets 上使用不同的 Join 类型。 如果您想加入两个以上的 DataFrames，请访问 [Join on Multiple DataFrames](https://sparkbyexamples.com/spark/spark-join-multiple-dataframes/)。

> In this tutorial, you will learn different Join syntaxes and using different Join types on two DataFrames and Datasets using Scala examples. Please access [Join on Multiple DataFrames](https://sparkbyexamples.com/spark/spark-join-multiple-dataframes/) in case if you wanted to join more than two DataFrames.

## 目录

- [Join Syntax & Types](#SQL join支持的数据类型和语法)
- [准备数据](#准备数据)
- [Inner Join](https://sparkbyexamples.com/spark/spark-sql-dataframe-join/#sql-inner-join)
- [Full Outer Join](https://sparkbyexamples.com/spark/spark-sql-dataframe-join/#sql-full-outer-join)
- [Left Outer Join](https://sparkbyexamples.com/spark/spark-sql-dataframe-join/#sql-full-outer-join)
- [Right Outer Join](https://sparkbyexamples.com/spark/spark-sql-dataframe-join/#sql-right-outer-join)
- [Left Anti Join](https://sparkbyexamples.com/spark/spark-sql-dataframe-join/#sql-left-anti-join)
- [Left Semi Join](https://sparkbyexamples.com/spark/spark-sql-dataframe-join/#sql-left-semi-join)
- [Self Join](https://sparkbyexamples.com/spark/spark-sql-dataframe-join/#sql-self-join)
- [Using SQL Expression](https://sparkbyexamples.com/spark/spark-sql-dataframe-join/#spark-sql)

## SQL join支持的数据类型和语法

> Below are the list of all Spark SQL Join Types and Syntaxes.

下面是所有 Spark SQL 连接类型和语法的列表。

1) join(right: Dataset[_]): DataFrame 

2) join(right: Dataset[_], usingColumn: String): DataFrame 

3) join(right: Dataset[_], usingColumns: Seq[String]): DataFrame 

4) join(right: Dataset[_], usingColumns: Seq[String], joinType: String): DataFrame 

5) join(right: Dataset[_], joinExprs: Column): DataFrame  // joinExpr为连接表达式

6) join(right: Dataset[_], joinExprs: Column, joinType: String): DataFrame 

> The rest of the tutorial explains Join Types using syntax 6 which takes arguments right join DataFrame, join expression and type of join in String.

本教程的其余部分使用语法 6 连接类型，该语法使用参数右连接 `DataFrame`、`连接表达式`和`字符串中的连接类型`。

> For Syntax 4 & 5 you can use either “JoinType” or “Join String” defined on the above table for “joinType” string argument. When you use “JoinType”, you should `import org.apache.spark.sql.catalyst.plans._` as this package defines JoinType objects.

对于语法 4 和 5，您可以使用上表中定义的“JoinType”或“Join String”作为“joinType”字符串参数。 当你使用“JoinType”时， 你应该`import org.apache.spark.sql.catalyst.plans._`，因为这个包定义了JoinType对象。

|    连接方式    |            **链接方式**            | **等效的 SQL 连接** |
| :------------: | :--------------------------------: | :-----------------: |
|   Inner.sql    |               inner                |     INNER JOIN      |
| FullOuter.sql  | outer, full, fullouter, full_outer |   FULL OUTER JOIN   |
| LeftOuter.sql  |    left, leftouter, left_outer     |      LEFT JOIN      |
| RightOuter.sql |   right, rightouter, right_outer   |     RIGHT JOIN      |
|   Cross.sql    |               cross                |                     |
|  LeftAnti.sql  |     anti, leftanti, left_anti      |                     |
|  LeftSemi.sql  |     semi, leftsemi, left_semi      |                     |

> All Join objects are defined at [joinTypes](https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/joinTypes.scala) class, In order to use these you need to import `org.apache.spark.sql.catalyst.plans.{LeftOuter,Inner,....}`.

所有 Join 对象都定义在 [joinTypes](https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/joinTypes. scala) 类，为了使用这些，您需要导入 `org.apache.spark.sql.catalyst.plans.{LeftOuter,Inner,....}`。

## 准备数据

在我们进入 Spark SQL Join 示例之前，首先，让我们创建一个 `emp` 和 `dept` DataFrame. 在这里，`emp_id` 列在 emp 上是唯一的，而 `dept_id` 在 dept 数据集上是唯一的，并且来自 emp 的 emp_dept_id 引用了 dept 数据集上的 dept_id。

> Before we jump into Spark SQL Join examples, first, let’s create an `emp` and `dept` [DataFrame’s](https://sparkbyexamples.com/spark/different-ways-to-create-a-spark-dataframe/). here, column `emp_id` is unique on emp and `dept_id` is unique on the dept dataset’s and emp_dept_id from emp has a reference to dept_id on dept dataset.

```scala
  val emp = Seq((1,"Smith",-1,"2018","10","M",3000),
    (2,"Rose",1,"2010","20","M",4000),
    (3,"Williams",1,"2010","10","M",1000),
    (4,"Jones",2,"2005","10","F",2000),
    (5,"Brown",2,"2010","40","",-1),
      (6,"Brown",2,"2010","50","",-1)
  )
  val empColumns = Seq("emp_id","name","superior_emp_id","year_joined",
       "emp_dept_id","gender","salary")
  import spark.sqlContext.implicits._
  val empDF = emp.toDF(empColumns:_*)
  empDF.show(false)

  val dept = Seq(("Finance",10),
    ("Marketing",20),
    ("Sales",30),
    ("IT",40)
  )

  val deptColumns = Seq("dept_name","dept_id")
  val deptDF = dept.toDF(deptColumns:_*)
  deptDF.show(false)
```

This print “emp” and “dept” DataFrame to console.

```scala
Emp Dataset
+------+--------+---------------+-----------+-----------+------+------+
|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|
+------+--------+---------------+-----------+-----------+------+------+
|1     |Smith   |-1             |2018       |10         |M     |3000  |
|2     |Rose    |1              |2010       |20         |M     |4000  |
|3     |Williams|1              |2010       |10         |M     |1000  |
|4     |Jones   |2              |2005       |10         |F     |2000  |
|5     |Brown   |2              |2010       |40         |      |-1    |
|6     |Brown   |2              |2010       |50         |      |-1    |
+------+--------+---------------+-----------+-----------+------+------+

Dept Dataset
+---------+-------+
|dept_name|dept_id|
+---------+-------+
|Finance  |10     |
|Marketing|20     |
|Sales    |30     |
|IT       |40     |
+---------+-------+
```

## Inner Join

Spark `Inner` 连接是默认连接，它最常用，它用于在键列上连接两个 DataFrames/Datasets，并且键不匹配的行从两个数据集中删除（`emp` & `dept`）.

>  Spark `Inner` join is the default join and it’s mostly used, It is used to join two DataFrames/Datasets on key columns, and where keys don’t match the rows get dropped from both datasets (`emp` & `dept`).

```scala
 empDF.join(deptDF,empDF("emp_dept_id") ===  deptDF("dept_id"),"inner")
    .show(false)
```

> When we apply Inner join on our datasets, It drops “`emp_dept_id`” 50 from “`emp`” and “`dept_id`” 30 from “`dept`” datasets. Below is the result of the above Join expression.

当我们对数据集应用内连接时，它会从“`emp`”数据集中删除“`emp_dept_id`” 50，从“`dept`”数据集中删除“`dept_id`” 30。 下面是上述 Join 表达式的结果，也就是<font color=red>会取两个数据集的交集,即共有的部分</font>。

```scala
+------+--------+---------------+-----------+-----------+------+------+---------+-------+
|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|
+------+--------+---------------+-----------+-----------+------+------+---------+-------+
|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |
|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |
|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |
|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |
|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |
+------+--------+---------------+-----------+-----------+------+------+---------+-------+
```

## Full Outer Join

`Outer` ,`full`, `fullouter` join 返回来自 Spark DataFrame/Datasets 的所有行，<font color=orange>其中 join 表达式不匹配它在相应的记录列上返回 null,也就是求两个数据集的并集</font>。

> `Outer` a.k.a `full`, `fullouter` join returns all rows from both Spark DataFrame/Datasets, where join expression doesn’t match it returns null on respective record columns.

```scala
  empDF.join(deptDF,empDF("emp_dept_id") ===  deptDF("dept_id"),"outer")
    .show(false)
  empDF.join(deptDF,empDF("emp_dept_id") ===  deptDF("dept_id"),"full")
    .show(false)
  empDF.join(deptDF,empDF("emp_dept_id") ===  deptDF("dept_id"),"fullouter")
    .show(false)
```

> From our “`emp`” dataset’s “`emp_dept_id`” with value 50 doesn’t have a record on “`dept`” hence dept columns have null and “`dept_id`” 30 doesn’t have a record in “`emp`” hence you see null’s on emp columns. Below is the result of the above Join expression.

从我们的“`emp`”数据集中，值为 50 的“`emp_dept_id`”在“`dept`”上没有记录，因此，dept 列有 null，“`dept_id`”30 在“`emp”中没有记录 `” 因此您会在 emp 列上看到 null。 下面是上述 Join 表达式的结果。

```scala
+------+--------+---------------+-----------+-----------+------+------+---------+-------+
|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|
+------+--------+---------------+-----------+-----------+------+------+---------+-------+
|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |
|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |
|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |
|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |
|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |
|6     |Brown   |2              |2010       |50         |      |-1    |null     |null   |
|null  |null    |null           |null       |null       |null  |null  |Sales    |30     |
+------+--------+---------------+-----------+-----------+------+------+---------+-------+
```

## Left Outer Join

Spark `Left` a.k.a `Left Outer` join 返回左侧 DataFrame/Dataset 中的所有行,<font color=red>即会返回左侧数据集的所有数据，包含左表的所有行，对应的右表行可能为空</font>。无论在右侧数据集上找到匹配，当连接表达式不匹配时，它为该记录分配 null 并从右侧删除未找到匹配的记录 .

> Spark `Left` a.k.a `Left Outer` join returns all rows from the left DataFrame/Dataset regardless of match found on the right dataset when join expression doesn’t match, it assigns null for that record and drops records from right where match not found.

```scala
  empDF.join(deptDF,empDF("emp_dept_id") ===  deptDF("dept_id"),"left")
    .show(false)
  empDF.join(deptDF,empDF("emp_dept_id") ===  deptDF("dept_id"),"leftouter")
    .show(false)
```

从我们的数据集中，“`emp_dept_id`” 5o 在“`dept`”数据集上没有记录，因此，该记录在“`dept`”列（dept_name 和 dept_id）上包含空值。 和“`dept`”数据集中的“`dept_id`”30 从结果中删除。 下面是上述 Join 表达式的结果。

> From our dataset, “`emp_dept_id`” 5o doesn’t have a record on “`dept`” dataset hence, this record contains null on “`dept`” columns (dept_name & dept_id). and “`dept_id`” 30 from “`dept`” dataset dropped from the results. Below is the result of the above Join expression.

```scala
+------+--------+---------------+-----------+-----------+------+------+---------+-------+
|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|
+------+--------+---------------+-----------+-----------+------+------+---------+-------+
|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |
|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |
|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |
|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |
|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |
|6     |Brown   |2              |2010       |50         |      |-1    |null     |null   |
+------+--------+---------------+-----------+-----------+------+------+---------+-------+
```

## Right Outer Join

Spark `Right` a.k.a `Right Outer` join 与 `left` join 相反，这里它返回来自右 DataFrame/Dataset 的所有行，而不管在左数据集上找到的数学，当连接表达式不匹配时，它为 null 分配 该记录并从左侧删除未找到匹配项的记录。

> Spark `Right` a.k.a `Right Outer` join is opposite of `left` join, here it returns all rows from the right DataFrame/Dataset regardless of math found on the left dataset, when join expression doesn’t match, it assigns null for that record and drops records from left where match not found.

```scala
  empDF.join(deptDF,empDF("emp_dept_id") ===  deptDF("dept_id"),"right")
   .show(false)
  empDF.join(deptDF,empDF("emp_dept_id") ===  deptDF("dept_id"),"rightouter")
   .show(false)
```

在我们的示例中，右侧数据集“`dept_id`” 30 在左侧数据集“`emp`”中没有它，因此，该记录在“`emp`”列中包含空值。 并且“`emp_dept_id`” 50 作为左侧未找到的匹配项被丢弃。 下面是上述 Join 表达式的结果。

> From our example, the right dataset “`dept_id`” 30 doesn’t have it on the left dataset “`emp`” hence, this record contains null on “`emp`” columns. and “`emp_dept_id`” 50 dropped as a match not found on left. Below is the result of the above Join expression.

```scala
+------+--------+---------------+-----------+-----------+------+------+---------+-------+
|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|
+------+--------+---------------+-----------+-----------+------+------+---------+-------+
|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |
|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |
|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |
|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |
|null  |null    |null           |null       |null       |null  |null  |Sales    |30     |
|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |
+------+--------+---------------+-----------+-----------+------+------+---------+-------+
```

## Left Semi Join

Spark `Left Semi` 连接类似于 `inner` 连接，区别在于 `left semi` 连接返回左侧 DataFrame/Dataset 中的所有列，并忽略右侧数据集中的所有列。 换句话说，此连接从唯一的左侧数据集中返回与连接表达式右侧数据集中匹配的记录的列，在连接表达式上不匹配的记录从左侧和右侧数据集中都将被忽略。

> Spark `Left Semi` join is similar to `inner` join difference being `leftsemi` join returns all columns from the left DataFrame/Dataset and ignores all columns from the right dataset. In other words, this join returns columns from the only left dataset for the records match in the right dataset on join expression, records not matched on join expression are ignored from both left and right datasets.

在内部连接的结果上使用 select 可以实现相同的结果，但是，使用此连接将是有效的。

> The same result can be achieved using select on the result of the inner join however, using this join would be efficient.

```scala
  empDF.join(deptDF,empDF("emp_dept_id") ===  deptDF("dept_id"),"leftsemi")
    .show(false)
```

Below is the result of the above join expression.

```scala
leftsemi join
+------+--------+---------------+-----------+-----------+------+------+
|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|
+------+--------+---------------+-----------+-----------+------+------+
|1     |Smith   |-1             |2018       |10         |M     |3000  |
|2     |Rose    |1              |2010       |20         |M     |4000  |
|3     |Williams|1              |2010       |10         |M     |1000  |
|4     |Jones   |2              |2005       |10         |F     |2000  |
|5     |Brown   |2              |2010       |40         |      |-1    |
+------+--------+---------------+-----------+-----------+------+------+
```

## Left Anti Join

`Left Anti` 连接与 Spark `leftsemi` 连接完全相反，`leftanti` 连接仅返回左侧 DataFrame/Dataset 中未匹配记录的列。

> `Left Anti` join does the exact opposite of the Spark `leftsemi` join, `leftanti` join returns only columns from the left DataFrame/Dataset for non-matched records.

```scala
  empDF.join(deptDF,empDF("emp_dept_id") ===  deptDF("dept_id"),"leftanti")
    .show(false)
```

Yields below output

```scala
+------+-----+---------------+-----------+-----------+------+------+
|emp_id|name |superior_emp_id|year_joined|emp_dept_id|gender|salary|
+------+-----+---------------+-----------+-----------+------+------+
|6     |Brown|2              |2010       |50         |      |-1    |
+------+-----+---------------+-----------+-----------+------+------+y
```

## Self Join

如果没有自连接，Spark 连接是不完整的，虽然没有可用的自连接类型，但我们可以使用上述任何一种连接类型将 DataFrame 连接到自身。 下面的示例使用 `inner` 自连接

> Spark Joins are not complete without a self join, Though there is no self-join type available, we can use any of the above-explained join types to join DataFrame to itself. below example use `inner` self join

```scala
  empDF.as("emp1").join(empDF.as("emp2"),
    col("emp1.superior_emp_id") === col("emp2.emp_id"),"inner")
    .select(col("emp1.emp_id"),col("emp1.name"),
      col("emp2.emp_id").as("superior_emp_id"),
      col("emp2.name").as("superior_emp_name"))
      .show(false)
```

在这里，我们将 `emp` 数据集与其自身相结合，以找出所有员工的上级 `emp_id` 和 `name`。

> Here, we are joining `emp` dataset with itself to find out superior `emp_id` and `name` for all employees.

```scala
+------+--------+---------------+-----------------+
|emp_id|name    |superior_emp_id|superior_emp_name|
+------+--------+---------------+-----------------+
|2     |Rose    |1              |Smith            |
|3     |Williams|1              |Smith            |
|4     |Jones   |2              |Rose             |
|5     |Brown   |2              |Rose             |
|6     |Brown   |2              |Rose             |
+------+--------+---------------+-----------------+
```

## Using SQL Expression

由于 Spark SQL 支持原生 SQL 语法，我们还可以在 DataFrame 上创建临时表并使用 spark.sql() 后编写连接操作

> Since Spark SQL support native SQL syntax, we can also write join operations after creating temporary tables on DataFrame’s and using `spark.sql()`

```scala
  empDF.createOrReplaceTempView("EMP")
  deptDF.createOrReplaceTempView("DEPT")
//SQL JOIN
  val joinDF = spark.sql("select * from EMP e, DEPT d where e.emp_dept_id == d.dept_id")
  joinDF.show(false)

  val joinDF2 = spark.sql("select * from EMP e INNER JOIN DEPT d ON e.emp_dept_id == d.dept_id")
  joinDF2.show(false)
```

## 完整代码

```scala
package com.sparkbyexamples.spark.dataframe.join

import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.functions.col
object JoinExample extends App {

  val spark: SparkSession = SparkSession.builder()
    .master("local[1]")
    .appName("SparkByExamples.com")
    .getOrCreate()

  spark.sparkContext.setLogLevel("ERROR")

  val emp = Seq((1,"Smith",-1,"2018","10","M",3000),
    (2,"Rose",1,"2010","20","M",4000),
    (3,"Williams",1,"2010","10","M",1000),
    (4,"Jones",2,"2005","10","F",2000),
    (5,"Brown",2,"2010","40","",-1),
      (6,"Brown",2,"2010","50","",-1)
  )
  val empColumns = Seq("emp_id","name","superior_emp_id","year_joined","emp_dept_id","gender","salary")
  import spark.sqlContext.implicits._
  val empDF = emp.toDF(empColumns:_*)
  empDF.show(false)

  val dept = Seq(("Finance",10),
    ("Marketing",20),
    ("Sales",30),
    ("IT",40)
  )

  val deptColumns = Seq("dept_name","dept_id")
  val deptDF = dept.toDF(deptColumns:_*)
  deptDF.show(false)


  println("Inner join")
  empDF.join(deptDF,empDF("emp_dept_id") ===  deptDF("dept_id"),"inner")
    .show(false)

  println("Outer join")
  empDF.join(deptDF,empDF("emp_dept_id") ===  deptDF("dept_id"),"outer")
    .show(false)
  println("full join")
  empDF.join(deptDF,empDF("emp_dept_id") ===  deptDF("dept_id"),"full")
    .show(false)
  println("fullouter join")
  empDF.join(deptDF,empDF("emp_dept_id") ===  deptDF("dept_id"),"fullouter")
    .show(false)

  println("right join")
  empDF.join(deptDF,empDF("emp_dept_id") ===  deptDF("dept_id"),"right")
    .show(false)
  println("rightouter join")
  empDF.join(deptDF,empDF("emp_dept_id") ===  deptDF("dept_id"),"rightouter")
    .show(false)

  println("left join")
  empDF.join(deptDF,empDF("emp_dept_id") ===  deptDF("dept_id"),"left")
    .show(false)
  println("leftouter join")
  empDF.join(deptDF,empDF("emp_dept_id") ===  deptDF("dept_id"),"leftouter")
    .show(false)

  println("leftanti join")
  empDF.join(deptDF,empDF("emp_dept_id") ===  deptDF("dept_id"),"leftanti")
    .show(false)

  println("leftsemi join")
  empDF.join(deptDF,empDF("emp_dept_id") ===  deptDF("dept_id"),"leftsemi")
    .show(false)

  println("cross join")
  empDF.join(deptDF,empDF("emp_dept_id") ===  deptDF("dept_id"),"cross")
    .show(false)

  println("Using crossJoin()")
  empDF.crossJoin(deptDF).show(false)

  println("self join")
  empDF.as("emp1").join(empDF.as("emp2"),
    col("emp1.superior_emp_id") === col("emp2.emp_id"),"inner")
    .select(col("emp1.emp_id"),col("emp1.name"),
      col("emp2.emp_id").as("superior_emp_id"),
      col("emp2.name").as("superior_emp_name"))
      .show(false)

  empDF.createOrReplaceTempView("EMP")
  deptDF.createOrReplaceTempView("DEPT")

  //SQL JOIN
  val joinDF = spark.sql("select * from EMP e, DEPT d where e.emp_dept_id == d.dept_id")
  joinDF.show(false)

  val joinDF2 = spark.sql("select * from EMP e INNER JOIN DEPT d ON e.emp_dept_id == d.dept_id")
  joinDF2.show(false)

}
```



## 总结

在本教程中，您学习了 Spark SQL 连接类型 `INNER`、`LEFT OUTER`、`RIGHT OUTER`、`LEFT ANTI`、`LEFT SEMI`、`CROSS`、`SELF` 连接用法以及 Scala 示例。

> In this tutorial, you have learned Spark SQL Join types `INNER`, `LEFT OUTER`, `RIGHT OUTER`, `LEFT ANTI`, `LEFT SEMI`, `CROSS`, `SELF` joins usage, and examples with Scala.





