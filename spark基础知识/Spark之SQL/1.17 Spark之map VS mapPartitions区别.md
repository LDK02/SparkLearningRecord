# Spark 之map() and mapPartitions()

Spark `map()` 和 `mapPartitions()` 转换将函数应用于 DataFrame/Dataset 的每个元素/记录/行并返回新的 DataFrame/Dataset，在本文中，我将解释 `map() 之间的区别 ` vs `mapPartitions()` 转换、它们的语法以及 Scala 示例的用法。

> Spark `map()` and `mapPartitions()` transformations apply the function on each element/record/row of the DataFrame/Dataset and returns the new DataFrame/Dataset, In this article, I will explain the difference between `map()` vs `mapPartitions()` transformations, their syntax, and usages with Scala examples.

- [map()](https://sparkbyexamples.com/spark/spark-map-vs-mappartitions-transformation/#map) - Spark `map()` 转换将函数应用于 DataFrame/Dataset 中的每一行，并且 返回新转换的“数据集”。

- > [map()](https://sparkbyexamples.com/spark/spark-map-vs-mappartitions-transformation/#map) – Spark `map()` transformation applies a function to each row in a DataFrame/Dataset and returns the new transformed `Dataset`.

- [mapPartitions()](https://sparkbyexamples.com/spark/spark-map-vs-mappartitions-transformation/#mappartitions) - 这与 map() 完全相同； 不同之处在于，Spark `mapPartitions()` 提供了一种工具来为<font color=red>每个分区进行一次繁重的初始化（例如数据库连接），而不是在每个 DataFrame 行上进行</font>。 当您在较大的数据集上处理权重初始化时，这有助于提高工作的性能。

- > [[mapPartitions()](https://sparkbyexamples.com/spark/spark-map-vs-mappartitions-transformation/#mappartitions) – This is exactly the same as map(); the difference being, Spark `mapPartitions()` provides a facility to do heavy initializations (for example Database connection) once for each partition instead of doing it on every DataFrame row. This helps the performance of the job when you dealing with heavy-weighted initialization on larger datasets.

  **关键点：**

  - 要记住一个关键点，这两种转换都返回 `Dataset[U]` 而不是 `DataFrame`（在 Spark 2.0 中，`DataFrame = Dataset[Row]`）。

- > One key point to remember, these both transformations returns the `Dataset[U]` but not the `DataFrame` (In Spark 2.0,  `DataFrame = Dataset[Row]`) .

- 在输入 DataFrame/Dataset 的每一行上应用转换函数后，<font color=red>它们返回与输入相同的行数，但结果的数据结构或列数可能不同</font>。

- > After applying the transformation function on each row of the input DataFrame/Dataset, these return the same number of rows as input but the schema or number of the columns of the result could be different.

- 如果您知道 `flatMap()` 转换，这是 map 和 flatMap 之间的关键区别，<font color=red>其中 map 只为每个输入返回一个行/元素，而 flatMap() 可以返回行/元素列表</font>。

- > If you know `flatMap()` transformation, this is the key difference between map and flatMap where map returns only one row/element for every input, while flatMap() can return a list of rows/elements.

## Spark `map()` vs `mapPartitions()` 案例

让我们通过示例来看看差异。 首先让我们[创建一个 Spark DataFrame](https://sparkbyexamples.com/spark/different-ways-to-create-a-spark-dataframe/)

```scala
  val structureData = Seq(
    Row("James","","Smith","36636","NewYork",3100),
    Row("Michael","Rose","","40288","California",4300),
    Row("Robert","","Williams","42114","Florida",1400),
    Row("Maria","Anne","Jones","39192","Florida",5500),
    Row("Jen","Mary","Brown","34561","NewYork",3000)
  )

  val structureSchema = new StructType()
    .add("firstname",StringType)
    .add("middlename",StringType)
    .add("lastname",StringType)
    .add("id",StringType)
    .add("location",StringType)
    .add("salary",IntegerType)

  val df2 = spark.createDataFrame(
    spark.sparkContext.parallelize(structureData),structureSchema)
  df2.printSchema()
  df2.show(false)
```

Yields below output

```scala
root
 |-- firstname: string (nullable = true)
 |-- middlename: string (nullable = true)
 |-- lastname: string (nullable = true)
 |-- id: string (nullable = true)
 |-- location: string (nullable = true)
 |-- salary: integer (nullable = true)

+---------+----------+--------+-----+----------+------+
|firstname|middlename|lastname|id   |location  |salary|
+---------+----------+--------+-----+----------+------+
|James    |          |Smith   |36636|NewYork   |3100  |
|Michael  |Rose      |        |40288|California|4300  |
|Robert   |          |Williams|42114|Florida   |1400  |
|Maria    |Anne      |Jones   |39192|Florida   |5500  |
|Jen      |Mary      |Brown   |34561|NewYork   |3000  |
+---------+----------+--------+-----+----------+------+
```

## 准备工作

为了用一个例子来解释 `map()` 和 `mapPartitions()`，让我们也创建一个带有 `combine()` 方法的 “`Util`” 类，这是一个简单的方法，它接受三个字符串参数并组合 他们用逗号分隔符。 在实时中，这可能是一个进行复杂转换的第三方类。

> In order to explain `map()` and `mapPartitions()` with an example, let’s also create a “`Util`” class with a method `combine()`, this is a simple method that takes three string arguments and combines them with a comma delimiter. In realtime, this could be a third-party class that does complex transformation.

```scala
class Util extends Serializable {
  def combine(fname:String,mname:String,lname:String):String = {
    fname+","+mname+","+lname
  }
}
```

我们将为这个类创建一个对象，方法是为 DataFrame 中的每一行初始化和调用“combine()”方法。

> We will create an object for this class by initializing and call the `combine()` method for each row in a DataFrame.

## Spark map() 案例

Spark `map()` 转换将一个函数应用于 DataFrame/Dataset 中的每一行并返回新转换的 `Dataset`。 如前所述，map() 为输入 DataFrame 中的每一行返回一行，换句话说，<font color=red>输入和结果完全包含相同数量的行</font>。

> Spark `map()` transformation applies a function to each row in a DataFrame/Dataset and returns the new transformed `Dataset`. As mentioned earlier, map() returns one row for every row in a input DataFrame, in other words, input and the result exactly contains the same number of rows.

例如，如果您在 DataFrame 中有 100 行，则在应用函数 map() 后返回正好 100 行。 但是，结果的结构或模式可能不同。

> For example, if you have 100 rows in a DataFrame, after applying the function map() return with exactly 100 rows. However, the structure or schema of the result could be different.

**Syntax:**

```scala
1) map[U](func : scala.Function1[T, U])(implicit evidence$6 : org.apache.spark.sql.Encoder[U]) 
        : org.apache.spark.sql.Dataset[U]

2) map[U](func : org.apache.spark.api.java.function.MapFunction[T, U], encoder : org.apache.spark.sql.Encoder[U]) 
        : org.apache.spark.sql.Dataset[U]
```

Spark 提供了 2 个映射转换参数，一个将 scala.function1 作为参数，另一个使用 MapFunction，如果您注意到这两个函数都返回 Dataset[U] 而不是 DataFrame（即 Dataset[Row]）。 如果您想要一个 DataFrame 作为输出，那么您需要使用 `toDF()` 函数将 Dataset 转换为 DataFrame。

> Spark provides 2 map transformation signatures one takes `scala.function1` as argument and the other takes `MapFunction` and if you notice both these functions return Dataset[U] but not DataFrame (which is Dataset[Row]). If you want a DataFrame as output then you need to convert the Dataset to DataFrame using `toDF()` function.

**Usage:**

```scala
import spark.implicits._
val df3 = df2.map(row=>{
  // This initialization happens to every records
  // If it is heavy initilizations like Database connects
  // It degrates the performance 
  val util = new Util() 
  val fullName = util.combine(row.getString(0),row.getString(1),row.getString(2))
             (fullName, row.getString(3),row.getInt(5))
})
val df3Map =  df3.toDF("fullName","id","salary")

df3Map.printSchema()
df3Map.show(false)
```

由于`Map`转换在工作节点上执行，我们已经在 `map()` 函数中初始化并创建了一个 Util 类的对象，并且初始化发生在 DataFrame 中的每一行。 当您进行大量加权初始化时，这会导致性能问题。

> Since map transformations execute on worker nodes, we have initialized and create an object of the `Util` class inside the map() function and the initialization happens for every row in a DataFrame. This causes performance issues when you have heavily weighted initializations.

**注意：** 当您在独立模式下运行它时，在 map() 之外初始化类仍然有效，因为执行程序和驱动程序都在同一个 JVM 上运行，但在集群上运行它会失败并出现异常。

> **Note:** When you running it on Standalone mode, initializing the class outside of the map() still works as both executors and driver run on the same JVM but running this on cluster fails with exception.

上面的例子产生的输出。

```scala
root
 |-- fullName: string (nullable = true)
 |-- id: string (nullable = true)
 |-- salary: integer (nullable = false)

+----------------+-----+------+
|fullName        |id   |salary|
+----------------+-----+------+
|James,,Smith    |36636|3100  |
|Michael,Rose,   |40288|4300  |
|Robert,,Williams|42114|1400  |
|Maria,Anne,Jones|39192|5500  |
|Jen,Mary,Brown  |34561|3000  |
+----------------+-----+------+
```

正如您注意到上面的输出，DataFrame 的输入有 5 行，因此映射的结果也有 5，但列数不同。

> As you notice the above output, the input of the DataFrame has 5 rows so the result of the map also has 5 but the column counts are different.

## Spark mapPartitions() 案例

Spark `mapPartitions()` 提供了一种工具，可以为每个分区进行一次繁重的初始化（例如数据库连接），而不是在每个 DataFrame 行上进行。 当您在较大的数据集上处理权重初始化时，这有助于提高工作的性能。

> Spark `mapPartitions()` provides a facility to do heavy initializations (for example Database connection) once for each partition instead of doing it on every DataFrame row. This helps the performance of the job when you dealing with heavy-weighted initialization on larger datasets.

**Syntax:**

```scala
1) mapPartitions[U](func : scala.Function1[scala.Iterator[T], scala.Iterator[U]])(implicit evidence$7 : org.apache.spark.sql.Encoder[U]) 
        : org.apache.spark.sql.Dataset[U]
2) mapPartitions[U](f : org.apache.spark.api.java.function.MapPartitionsFunction[T, U], encoder : org.apache.spark.sql.Encoder[U]) 
        : org.apache.spark.sql.Dataset[U]
```

`mapPartitions()`也有 2 个参数，一个采用 `scala.Function1` 和其他采用 spark `MapPartitionsFunction` 参数。

> map partitions also have 2 signatures, one take `scala.Function1` and other takes spark `MapPartitionsFunction` arguments.

mapPartitions() 将分区的结果保存在内存中，直到它完成执行分区中的所有行。

> mapPartitions() keeps the result of the partition in-memory until it finishes executing all rows in a partition.

**Usage:**

```scala
  val df4 = df2.mapPartitions(iterator => {
    // Do the heavy initialization here
    // Like database connections e.t.c
    val util = new Util()
    val res = iterator.map(row=>{
      val fullName = util.combine(row.getString(0),row.getString(1),row.getString(2))
      (fullName, row.getString(3),row.getInt(5))
    })
    res
  })
  val df4part = df4.toDF("fullName","id","salary")
  df4part.printSchema()
  df4part.show(false)
```



## 完整代码

下面是 Spark DataFrame map() & mapPartition() 示例的完整示例。

```scala
package sparkScalaExamples.SQL

import org.apache.spark.sql.types.{IntegerType, StringType, StructType}
import org.apache.spark.sql.{DataFrame, Dataset, Row, SparkSession}

object MapTransformation {
  def main(args: Array[String]): Unit = {

    val spark:SparkSession = SparkSession.builder()
      .master("local[5]")
      .appName("SparkByExamples.com")
      .getOrCreate()


    /**
      * 1 创建数据集
      * */

    val structureData = Seq(
      Row("James","","Smith","36636","NewYork",3100),
      Row("Michael","Rose","","40288","California",4300),
      Row("Robert","","Williams","42114","Florida",1400),
      Row("Maria","Anne","Jones","39192","Florida",5500),
      Row("Jen","Mary","Brown","34561","NewYork",3000)
    )

    val structureSchema = new StructType()
      .add("firstname",StringType)
      .add("middlename",StringType)
      .add("lastname",StringType)
      .add("id",StringType)
      .add("location",StringType)
      .add("salary",IntegerType)

    val df2 = spark.createDataFrame(
      spark.sparkContext.parallelize(structureData),structureSchema)
    df2.printSchema()
    df2.show(false)


    import spark.implicits._
    val util = new Util()
    /**
      * 2 map函数
      * */
    val df3 = df2.map(row => {
      val fullname = util.combine(row.getString(0), row.getString(1), row.getString(2))
      (fullname, row.getString(3), row.getInt(5))
    })
    val df3Map: DataFrame = df3.toDF("fullnaem","id","salary")
    println("-------------map转换结果-----------------")
    df3Map.printSchema()
    df3Map.show(false)

    /**
      * 3 mappartitions
      * */
      
    val df4 = df2.mapPartitions(iterator => {
      val util = new Util()
      val res = iterator.map(row=>{
        val fullName = util.combine(row.getString(0),row.getString(1),row.getString(2))
        (fullName, row.getString(3),row.getInt(5))
      })
      res
    })
    val df4part = df4.toDF("fullName","id","salary")
    println("-------------mapPartition转换结果-----------------")
    df4part.printSchema()
    df4part.show(false)

  }


}


class Util extends Serializable {
  def combine(fname:String,mname:String,lname:String):String = {
    fname+","+mname+","+lname
  }
}
```

This example is also available at [Spark Example github project](https://github.com/spark-examples/spark-scala-examples/blob/master/src/main/scala/com/sparkbyexamples/spark/dataframe/examples/MapTransformation.scala)

## Conclusion

在这篇 Spark DataFrame 文章中，您学习了 map() 和 mapPartitions() 转换对每一行执行一个函数，并返回与输入中相同数量的记录，但具有相同或不同的架构或列。 还了解到，当您进行复杂的初始化时，您应该使用 mapPratitions()，因为它能够为每个分区而不是每个 DataFrame 行进行一次初始化。

> In this Spark DataFrame article, you have learned map() and mapPartitions() transformations execute a function on each and every row and returns the same number of records as in input but with the same or different schema or columns. Also learned when you have a complex initialization you should be using mapPratitions() as it has the capability to do initializations once for each partition instead of every DataFrame row..

