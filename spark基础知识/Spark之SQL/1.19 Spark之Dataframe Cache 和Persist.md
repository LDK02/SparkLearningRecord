# Spark 之*Cache* and P*ersist* 

Spark *Cache* 和 P*ersist* 是 DataFrame / Dataset 中的优化技术，用于迭代和交互式 Spark 应用程序，以提高 Jobs 的性能。 在本文中，您将学习什么是 Spark `cache()` 和 `persist()`，如何在 DataFrame 中使用它，了解 [`Caching` 和 `Persistance` 的区别](https://sparkbyexamples.com /spark/spark-difference-between-cache-and-persist/) 以及如何将这两者与 DataFrame 和 Dataset 一起使用 Scala 示例。

> Spark *Cache* and P*ersist* are optimization techniques in DataFrame / Dataset for iterative and interactive Spark applications to improve the performance of Jobs. In this article, you will learn What is Spark `cache()` and `persist()`, how to use it in DataFrame, understanding the [difference between `Caching` and `Persistance`](https://sparkbyexamples.com/spark/spark-difference-between-cache-and-persist/) and how to use these two with DataFrame, and Dataset using Scala examples.

尽管 Spark 提供的计算速度比传统 Map Reduce 作业快 100 倍，<font color=red>但如果您没有设计作业来重用重复计算，那么在处理数十亿或数万亿数据时</font>，您会发现性能下降。 因此，我们可能需要查看各个阶段并使用优化技术作为提高性能的方法之一。

> Though Spark provides computation 100 x times faster than traditional Map Reduce jobs, If you have not designed the jobs to reuse the repeating computations you will see degrade in performance when you are dealing with billions or trillions of data. Hence, we may need to look at the stages and use optimization techniques as one of the ways to improve performance.

使用 `cache()` 和 `persist()` 方法，Spark 提供了一种优化机制来<font color=red>存储 Spark DataFrame 的中间计算，以便它们可以在后续操作中重用。</font>

> Using `cache()` and `persist()` methods, Spark provides an optimization mechanism to store the intermediate computation of a Spark DataFrame so they can be reused in subsequent actions.

持久化数据集时，<font color=red>每个节点将其分区数据存储在内存中</font>，并在对该数据集的其他操作中重用它们。 Spark 在节点上的持久化数据是容错的，这意味着如果数据集的任何分区丢失，它将使用创建它的原始转换自动重新计算。

> When you persist a dataset, each node stores its partitioned data in memory and reuses them in other actions on that dataset. And Spark’s persisted data on nodes are fault-tolerant meaning if any partition of a Dataset is lost, it will automatically be recomputed using the original transformations that created it.

## 目录



## DataFrame 缓存和持久化的优势

以下是使用 Spark Cache 和 Persist 方法的优点。

- **具有成本效益** - Spark 计算非常昂贵，因此重用计算可以节省成本。
- **省时** - 重复使用重复计算可以节省大量时间。
- **执行时间** - 节省作业的执行时间，我们可以在同一个集群上执行更多作业。

> Below are the advantages of using Spark Cache and Persist methods.
>
> - **Cost-efficient** – Spark computations are very expensive hence reusing the computations are used to save cost.
> - **Time-efficient** – Reusing repeated computations saves lots of time.
> - **Execution time** – Saves execution time of the job and we can perform more jobs on the same cluster.



## Spark Cache 语法以及案例

Spark DataFrame 或 Dataset `cache()` 方法默认将其保存到存储级别 ``MEMORY_AND_DISK`` 因为重新计算底层表的内存列表示是昂贵的。 请注意，这与“RDD.cache()”的默认缓存级别“MEMORY_ONLY”不同。

> Spark DataFrame or Dataset `cache()` method by default saves it to storage level ``MEMORY_AND_DISK`` because recomputing the in-memory columnar representation of the underlying table is expensive. Note that this is different from the default cache level of ``RDD.cache()`` which is ‘`MEMORY_ONLY`‘.

S**yntax**

```scala
 cache() : Dataset.this.type
```

Dataset 类中的 Spark `cache()` 方法内部调用 `persist()` 方法，该方法又使用`sparkSession.sharedState.cacheManager.cacheQuery` 来缓存 DataFrame 或 Dataset 的结果集。 让我们看一个例子。

> Spark `cache()` method in Dataset class internally calls `persist()` method which in turn uses `sparkSession.sharedState.cacheManager.cacheQuery` to cache the result set of DataFrame or Dataset. Let’s look at an example.

**Example**

```scala
  val spark:SparkSession = SparkSession.builder()
    .master("local[1]")
    .appName("SparkByExamples.com")
    .getOrCreate()

  //read csv with options
  val df = spark.read.options(Map("inferSchema"->"true","delimiter"->",","header"->"true"))
    .csv("src/main/resources/zipcodes.csv")
  
  val df2 = df.where(col("State") === "PR").cache()
  df2.show(false)

  println(df2.count())

  val df3 = df2.where(col("Zipcode") === 704)

  println(df2.count())
```

## DataFrame Persist 语法以及案例

Spark persist() 方法用于将 DataFrame 或 Dataset 存储到 `MEMORY_ONLY`、`MEMORY_AND_DISK`、`MEMORY_ONLY_SER`、`MEMORY_AND_DISK_SER`、`DISK_ONLY`、`MEMORY_ONLY_2`、`MEMORY_AND_DISK_2` 等存储级别之一。

> Spark persist() method is used to store the DataFrame or Dataset to one of the storage levels `MEMORY_ONLY`,`MEMORY_AND_DISK`, `MEMORY_ONLY_SER`, `MEMORY_AND_DISK_SER`, `DISK_ONLY`, `MEMORY_ONLY_2`,`MEMORY_AND_DISK_2` and more.

Spark DataFrame 或 Dataset 的缓存或持久化是一种惰性操作，这意味着 DataFrame 在您触发操作之前不会被缓存。

> Caching or persisting of Spark DataFrame or Dataset is a lazy operation, meaning a DataFrame will not be cached until you trigger an action. 

**Syntax**

```scala
1) persist() : Dataset.this.type
2) persist(newLevel : org.apache.spark.storage.StorageLevel) : Dataset.this.type
```

Spark persist 有两个参数，第一个参数不带任何参数，默认情况下将其保存到“MEMORY_AND_DISK”存储级别，第二个参数将“StorageLevel”作为参数，指定存储到不同的存储级别。

> Spark persist has two signature first signature doesn’t take any argument which by default saves it to `MEMORY_AND_DISK` storage level and the second signature which takes `StorageLevel` as an argument to store it to different storage levels.

**Example**

```scala
  val dfPersist = df.persist()
  dfPersist.show(false)
```

使用第二个签名，您可以将 DataFrame/Dataset 保存到任何存储级别。

> Using the second signature you can save DataFrame/Dataset to any storage levels.

```scala
  val dfPersist = df.persist(StorageLevel.MEMORY_ONLY)
  dfPersist.show(false)
```

这会将 DataFrame/Dataset 存储到内存中。

> This stores DataFrame/Dataset into Memory.

请注意，Dataset `cache()` 是`persist(StorageLevel.MEMORY_AND_DISK)` 的简写。

## 4 Unpersist 语法以及案例

Spark 会自动监控您进行的每个 persist() 和 cache() 调用，它会检查每个节点上的使用情况，<font color=red>如果不使用或使用最近最少使用 (LRU) 算法删除持久数据</font>。 您也可以使用 `unpersist()` 方法手动删除。 unpersist() 将数据集标记为非持久，并从内存和磁盘中删除它的所有块。

> Spark automatically monitors every persist() and cache() calls you make and it checks usage on each node and drops persisted data if not used or by using the least-recently-used (LRU) algorithm. You can also manually remove using `unpersist()` method. unpersist() marks the Dataset as non-persistent, and remove all blocks for it from memory and disk.

S**yntax**

```scala
unpersist() : Dataset.this.type
unpersist(blocking : scala.Boolean) : Dataset.this.type
```

E**xample**

```scala
  val dfPersist = dfPersist.unpersist()
```

unpersist(Boolean) 以布尔值作为参数块，直到所有块都被删除。

> unpersist(Boolean) with boolean as argument blocks until all blocks are deleted.

## Spark Persist存储级别

`org.apache.spark.storage.StorageLevel` 类中提供了 Spark 支持的所有不同存储级别。 存储级别指定持久化或缓存 Spark DataFrame 和 Dataset 的方式和位置。

> All different storage level Spark supports are available at `org.apache.spark.storage.StorageLevel` class. The storage level specifies how and where to persist or cache a Spark DataFrame and Dataset.

`MEMORY_ONLY` – This is the default behavior of the RDD `cache()` method and stores the RDD or DataFrame as deserialized objects to JVM memory. When there is not enough memory available it will not save DataFrame of some partitions and these will be re-computed as and when required. This takes more memory. but unlike RDD, this would be slower than MEMORY_AND_DISK level as it recomputes the unsaved partitions, and recomputing the in-memory columnar representation of the underlying table is expensive

`MEMORY_ONLY_SER` – This is the same as `MEMORY_ONLY` but the difference being it stores RDD as serialized objects to JVM memory. It takes lesser memory (space-efficient) than MEMORY_ONLY as it saves objects as serialized and takes an additional few more CPU cycles in order to deserialize.

`MEMORY_ONLY_2` – Same as `MEMORY_ONLY` storage level but replicate each partition to two cluster nodes.

`MEMORY_ONLY_SER_2` – Same as `MEMORY_ONLY_SER` storage level but replicate each partition to two cluster nodes.

`MEMORY_AND_DISK` – This is the default behavior of the DataFrame or Dataset. In this Storage Level, The DataFrame will be stored in JVM memory as a deserialized object. When required storage is greater than available memory, it stores some of the excess partitions into a disk and reads the data from the disk when required. It is slower as there is I/O involved.

`MEMORY_AND_DISK_SER` – This is the same as `MEMORY_AND_DISK` storage level difference being it serializes the DataFrame objects in memory and on disk when space is not available.

`MEMORY_AND_DISK_2` – Same as `MEMORY_AND_DISK` storage level but replicate each partition to two cluster nodes.

`MEMORY_AND_DISK_SER_2` – Same as `MEMORY_AND_DISK_SER` storage level but replicate each partition to two cluster nodes.

`DISK_ONLY` – In this storage level, DataFrame is stored only on disk and the CPU computation time is high as I/O is involved.

`DISK_ONLY_2` – Same as `DISK_ONLY` storage level but replicate each partition to two cluster nodes.

## 总结

在本文中，您学习了 Spark 的 `cache()` 和 `persist()` 方法作为优化技术，用于保存 DataFrame 或 Dataset 的临时计算结果并随后重用它们，并了解了 Spark Cache 和 Persist 之间的区别和 终于通过 Scala 示例看到了它们的语法和用法。

> In this article, you have learned Spark `cache()` and `persist()` methods are used as optimization techniques to save interim computation results of DataFrame or Dataset and reuse them subsequently and learned what is the difference between Spark Cache and Persist and finally saw their syntaxes and usages with Scala examples.

Happy Learning !!

