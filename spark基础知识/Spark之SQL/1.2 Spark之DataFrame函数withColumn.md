# spark withColumn函数

Spark `withColumn()` is a DataFrame function that is used to add a new column to DataFrame, change the value of an existing column, [convert the datatype of a column](https://sparkbyexamples.com/spark/spark-sql-dataframe-data-types/), derive a new column from an existing column, on this post, I will walk you through commonly used DataFrame column operations with Scala examples.

Spark `withColumn()` 是一个 DataFrame 函数，<font color=red>用于向 DataFrame 添加新列，更改现有列的值，[转换列的数据类型](https://sparkbyexamples.com/spark/spark- sql-dataframe-data-types/)，</font>从现有列派生新列，在这篇文章中，我将通过 Scala 示例向您介绍常用的 DataFrame 列操作。

## 目录

- [Spark withColumn() 语法和用法](#Spark withColumn() 语法和用法（Spark withColumn() Syntax and Usage）)
- [1、添加新列](#1. 添加新列（Add a New Column to DataFrame)
- [2、更改现有列的值](#2. 更改现有列的值（Change Value of an Existing Column）)
- [3、从现有列派生新列](#3. 从现有列派生新列（Derive New Column From an Existing Column）)
- [4、更改列数据类型](#4. 更改列数据类型（Change Column Data Type）)
- [5、添加、替换或更新多个列](#5. 添加、替换或更新多个列（Add, Replace, or Update multiple Columns）)
- [6、重命名列名](#6. 重命名列名（Rename Column Name）)
- [7、删除一列](#7.删除一列（Drop a Column）)
- [8、将列拆分为多列](#8. 将列拆分为多列（Split Column into Multiple Columns）)

## Spark withColumn() 语法和用法（Spark withColumn() Syntax and Usage）

Spark `withColumn()` is a transformation function of DataFrame that is used to manipulate the column values of all rows or selected rows on DataFrame.

Spark `withColumn()` 是 DataFrame 的一个转换函数，用于操作 DataFrame 上所有行或选定行的列值。

withColumn() function returns a new Spark DataFrame after performing operations like adding a new column, update the value of an existing column, derive a new column from an existing column, and many more.

<font color=red>withColumn() 函数在执行添加新列、更新现有列的值、从现有列派生新列等操作后返回一个新的 Spark DataFrame</font>。

Below is a syntax of `withColumn()` function.

下面是 `withColumn()` 函数的语法。

```scala
withColumn(colName : String, col : Column) : DataFrame
```

`colName:String` – specify a new column you wanted to create. use an existing column to update the value.

`col:Column` – column expression.

> `colName:String` - 指定一个你想要创建的新列。 使用现有列更新值。
>
> `col:Column` - 列表达式。

Since withColumn() is a transformation function it doesn’t execute until action is called.

注意：**由于 withColumn() 是一个转换函数，它在调用 action 之前不会执行。**

> Spark withColumn() method introduces a projection internally. Therefore, calling it multiple times, for instance, via loops in order to add multiple columns can generate big plans which can cause performance issues and even `StackOverflowException`. To avoid this, use `select` with the multiple columns at once.
>
> Spark withColumn() 方法在内部引入了投影。 因此，多次调用它，例如，通过循环以添加多个列可能会生成大计划，这可能会导致性能问题甚至 `StackOverflowException`。 为避免这种情况，请同时对多列使用“select”。
>
> Spark Documentation

First, let’screate a DataFrame to work with.

首先，让我们创建一个 DataFrame来使用。

```scala

import org.apache.spark.sql.{Row, SparkSession}
import org.apache.spark.sql.types.{StringType, StructType} 
val data = Seq(Row(Row("James;","","Smith"),"36636","M","3000"),
      Row(Row("Michael","Rose",""),"40288","M","4000"),
      Row(Row("Robert","","Williams"),"42114","M","4000"),
      Row(Row("Maria","Anne","Jones"),"39192","F","4000"),
      Row(Row("Jen","Mary","Brown"),"","F","-1")
)

val schema = new StructType()
      .add("name",new StructType()
      .add("firstname",StringType)
      .add("middlename",StringType)
      .add("lastname",StringType))
      .add("dob",StringType)
      .add("gender",StringType)
      .add("salary",StringType)

val df = spark.createDataFrame(spark.sparkContext.parallelize(data),schema)

```

## 添加新列（Add a New Column to DataFrame）

To create a new column, pass your desired column name to the first argument of withColumn() transformation function. Make sure this new column not already present on DataFrame, if it presents it updates the value of the column. On the below snippet, [lit() function](https://sparkbyexamples.com/spark/using-lit-and-typedlit-to-add-a-literal-or-constant-to-spark-dataframe/) is used to add a constant value to a DataFrame column. We can also chain in order to add multiple columns.

要创建新列，请将所需的列名传递给 withColumn() 转换函数的第一个参数。 确保这个新列尚未出现在 DataFrame 上，如果它出现，它会更新该列的值。 在下面的代码片段中，[lit() function](https://sparkbyexamples.com/spark/using-lit-and-typedlit-to-add-a-literal-or-constant-to-spark-dataframe/) 是 用于向 DataFrame 列添加常量值。 我们还可以链接以添加多个列。

```scala
/** 1 新增一列，并赋值为“USA”
      * */
import org.apache.spark.sql.functions.lit
df.withColumn("Countty",lit("USA"))
//可以一次性增加多列
df.withColumn("Country_1",lit("USA"))
.withColumn("anotherColumn",lit("anotherValue"))
println("新增几列后的数据: ")
df.show()
```

The above approach is fine if you are manipulating few columns, but when you wanted to add or update multiple columns, do not use the chaining withColumn() as it leads to performance issues, use [select() to update multiple columns](https://sparkbyexamples.com/spark/spark-dataframe-withcolumn/#add-replace-update-multiple-columns) instead.

如果您要操作几列，则上述方法很好，但是当您要添加或更新多个列时，请不要使用链接 withColumn() 因为它会导致性能问题，请使用 [select() 更新多列] 代替。

## 更改现有列的值（Change Value of an Existing Column）

Spark `withColumn()` function of DataFrame can also be used to update the value of an existing column. In order to change the value, pass an existing column name as a first argument and value to be assigned as a second column. Note that the second argument should be `Column` type .

DataFrame 的 Spark `withColumn()` 函数也可用于更新现有列的值。 要更改值，请将现有列名作为第一个参数传递，并将要分配的值作为第二列传递。 请注意，第二个参数应该是 `Column` 类型。

```scala
 /** 2 修改已有列的值
      *
      * */
import org.apache.spark.sql.functions.col
df.withColumn("salary",col("salary")*100)
println("修改后的列的值为：")
df.show()
```

This snippet multiplies the value of “salary” with 100 and updates the value back to “salary” column.

此代码段将“salary”的值乘以 100，并将该值更新回“salary”列。

## 从现有列派生新列（Derive New Column From an Existing Column）

To [create a new column](https://sparkbyexamples.com/spark/spark-add-new-column-to-dataframe/), specify the first argument with a name you want your new column to be and use the second argument to assign a value by applying an operation on an existing column.

要[创建新列](https://sparkbyexamples.com/spark/spark-add-new-column-to-dataframe/)，请使用您希望新列的名称指定第一个参数并使用第二个 通过对现有列应用操作来分配值的参数。

```scala
  /** 3 依据已有的列，创建新的列*/
    df.withColumn("CopiedColumn",col("salary")* -1)
    println("依据已有的列创建的结果：")
    df.show()
```

This snippet creates a new column “CopiedColumn” by multiplying “salary” column with value -1.

此代码段通过将“salary”列与值 -1 相乘来创建一个新列“CopiedColumn”。

## 更改列数据类型（Change Column Data Type）

By using Spark withColumn on a DataFrame and using cast function on a column, we can [change datatype of a DataFrame column](https://sparkbyexamples.com/spark/spark-change-dataframe-column-type/). The below statement changes the datatype from String to Integer for the “salary” column.

通过在 DataFrame 上使用 Spark withColumn 并在列上使用 cast 函数，我们可以 [更改 DataFrame 列的数据类型](https://sparkbyexamples.com/spark/spark-change-dataframe-column-type/)。 下面的语句将“salary”列的数据类型从 String 更改为 Integer。

```scala
/** 4 更改列的数据类型
*
* */

df.withColumn("salary",col("salary").cast("Integer"))
println("打印列名信息：")
df.printSchema()
```

## 添加、替换或更新多个列（Add, Replace, or Update multiple Columns）

When you wanted to add, replace or update multiple columns in Spark DataFrame, it is not suggestible to chain `withColumn()` function as it leads into performance issue and recommends to use select() after creating a temporary view on DataFrame.

当您想在 Spark DataFrame 中添加、替换或更新多个列时，不建议链接 `withColumn()` 函数，因为它会导致性能问题，建议在 DataFrame 上创建临时视图后使用 select()

```scala
/**
      * 5 添加、替换或更新多个列
      * */

df.createOrReplaceTempView("PERSON")
println("对多列进行添加、替换或更新")
spark.sql("SELECT salary*100 as salary,salary* -1 as CopiedColumn,'USA' as country FROM PERSON").show()
```

## 重命名列名（Rename Column Name）

Though examples in 6,7, and 8 doesn’t use `withColumn()` function, I still feel like explaining how to rename, drop, and split columns as these would be useful to you.

To rename an existing column use “[withColumnRenamed](https://sparkbyexamples.com/spark/rename-a-column-on-spark-dataframes/)” function on DataFrame.

尽管 6,7 和 8 中的示例没有使用 `withColumn()` 函数，但我仍然想解释如何重命名、删除和拆分列，因为这些对您很有用。

要重命名现有列，请使用 DataFrame 上的“[withColumnRenamed](https://sparkbyexamples.com/spark/rename-a-column-on-spark-dataframes/)”函数。

```scala
 /**
      * 6 重命名
      * */
df.withColumnRenamed("gender","sex")
println("重命名后的结果：")
df.show()
```

## 删除一列（Drop a Column）

Use `drop()` function to drop a specific column from the DataFrame.

使用 `drop()` 函数从 DataFrame 中删除特定列。

```scala
/** 7 删除列
 *
 * */
df.drop("CopiedColumn")
println("删除CopiedColumn后的结果：")
df.show()
```

## 将列拆分为多列（Split Column into Multiple Columns）

Though this example doesn’t use `withColumn()` function, I still feel like it’s good to explain on[ splitting one DataFrame column to multiple columns](https://sparkbyexamples.com/spark/spark-split-dataframe-column-into-multiple-columns/) using [Spark `map()` transformation function](https://sparkbyexamples.com/spark/spark-map-transformation/).

虽然这个例子没有使用 `withColumn()` 函数，但我仍然觉得解释一下[将一个 DataFrame 列拆分为多个列](https://sparkbyexamples.com/spark/spark-split-dataframe-column) 很好 -into-multiple-columns/) 使用 [Spark `map()` 转换函数](https://sparkbyexamples.com/spark/spark-map-transformation/)。

```scala
/** 8 划分某一列为多列
      *
      * */
import spark.implicits._
val columns = Seq("name","address")
val data1 = Seq(("Robert,Smith", "1 Main st,Newark,NJ,92537"),
                ("Maria,Garacia", "3456 Walnut st,Newark,NJ,94732"))


val dfFromData1: DataFrame = spark.createDataFrame(data1).toDF(columns:_*)
dfFromData1.printSchema()

val newDF: Dataset[(String, String, String, String, String, String)] = dfFromData1.map { f => {
    val nameSplit: Array[String] = f.getAs[String](0).split(",")
    val addSplit: Array[String] = f.getAs[String](1).split(",")
    (nameSplit(0), nameSplit(1), addSplit(0), addSplit(1), addSplit(2), addSplit(3))
}
}
val finalDF: DataFrame = newDF.toDF("First Name","Last Name","Address Line1","City","State","zipCode")
println("-----------展示生成的多列----------------")
finalDF.printSchema()
finalDF.show(false)
```



This snippet split name column into “first name”, “last name” and address column into “Address Line1”, “City”, “State” and “ZipCode”. Yields below output:

此代码段将名称列拆分为“名字”、“姓氏”，将地址列拆分为“地址 Line1”、“城市”、“州”和“邮政编码”。 产量低于输出：

```scala
root
 |-- First Name: string (nullable = true)
 |-- Last Name: string (nullable = true)
 |-- Address Line1: string (nullable = true)
 |-- City: string (nullable = true)
 |-- State: string (nullable = true)
 |-- zipCode: string (nullable = true)

+----------+---------+--------------+-------+-----+-------+
|First Name|Last Name|Address Line1 |City   |State|zipCode|
+----------+---------+--------------+-------+-----+-------+
|Robert    | Smith   |1 Main st     | Newark| NJ  | 92537 |
|Maria     | Garcia  |3456 Walnut st| Newark| NJ  | 94732 |
+----------+---------+--------------+-------+-----+-------+
```



**Note:** Note that all of these functions return the new DataFrame after applying the functions instead of updating DataFrame.

**注意：** <font color=red>请注意，所有这些函数都在应用函数后返回新的 DataFrame，而不是更新 DataFrame。</font>

## 完整代码

```scala
package sparkScalaExamples.SQL

import org.apache.spark.SparkContext
import org.apache.spark.sql.types.{StringType, StructType}
import org.apache.spark.sql.{DataFrame, Dataset, Row, SparkSession}

object WithColumnsDemo {
  def main(args: Array[String]): Unit = {
    val spark: SparkSession = SparkSession.builder()
      .appName("withColumnsDemo")
      .master("local")
      .getOrCreate()

    val data = Seq(Row(Row("James;","","Smith"),"36636","M","3000"),
      Row(Row("Michael","Rose",""),"40288","M","4000"),
      Row(Row("Robert","","Williams"),"42114","M","4000"),
      Row(Row("Maria","Anne","Jones"),"39192","F","4000"),
      Row(Row("Jen","Mary","Brown"),"","F","-1")
    )
    //定义dataframe的数据类型
    val schema = new StructType()
      .add("name",new StructType()
        .add("firstname",StringType)
        .add("middlename",StringType)
        .add("lastname",StringType))
      .add("dob",StringType)
      .add("gender",StringType)
      .add("salary",StringType)

    val sc: SparkContext = spark.sparkContext
    val df: DataFrame = spark.createDataFrame(sc.parallelize(data),schema)
    println("展示原始数据： ")
    df.show()

    /** 1 新增一列，并赋值为“USA”
      * */
    import org.apache.spark.sql.functions.lit
    df.withColumn("Countty",lit("USA"))
    //可以一次性增加多列
    df.withColumn("Country_1",lit("USA"))
      .withColumn("anotherColumn",lit("anotherValue"))
    println("新增几列后的数据: ")
    df.show()

    /** 2 修改已有列的值
      *
      * */
    import org.apache.spark.sql.functions.col
    df.withColumn("salary",col("salary")*100)
    println("修改后的列的值为：")
    df.show()

    /** 3 依据已有的列，创建新的列*/
    df.withColumn("CopiedColumn",col("salary")* -1)
    println("依据已有的列创建的结果：")
    df.show()

    /** 4 更改列的数据类型
      *
      * */

    df.withColumn("salary",col("salary").cast("Integer"))
    println("打印列名信息：")
    df.printSchema()

    /**
      * 5 添加、替换或更新多个列
      * */

    df.createOrReplaceTempView("PERSON")
    println("对多列进行添加、替换或更新")
    spark.sql("SELECT salary*100 as salary,salary* -1 as CopiedColumn,'USA' as country FROM PERSON").show()

    /**
      * 6 重命名
      * */
    df.withColumnRenamed("gender","sex")
    println("重命名后的结果：")
    df.show()

    /** 7 删除列
      *
      * */
    df.drop("CopiedColumn")
    println("删除CopiedColumn后的结果：")
    df.show()

    /** 8 划分某一列为多列
      *
      * */
    import spark.implicits._
    val columns = Seq("name","address")
    val data1 = Seq(("Robert,Smith", "1 Main st,Newark,NJ,92537"),
      ("Maria,Garacia", "3456 Walnut st,Newark,NJ,94732"))


    val dfFromData1: DataFrame = spark.createDataFrame(data1).toDF(columns:_*)
    dfFromData1.printSchema()

    val newDF: Dataset[(String, String, String, String, String, String)] = dfFromData1.map { f => {
      val nameSplit: Array[String] = f.getAs[String](0).split(",")
      val addSplit: Array[String] = f.getAs[String](1).split(",")
      (nameSplit(0), nameSplit(1), addSplit(0), addSplit(1), addSplit(2), addSplit(3))
    }
    }
    val finalDF: DataFrame = newDF.toDF("First Name","Last Name","Address Line1","City","State","zipCode")
    println("-----------展示生成的多列----------------")
    finalDF.printSchema()
    finalDF.show(false)
  }


}

```

结果：

```scala
展示原始数据： 
+--------------------+-----+------+------+
|                name|  dob|gender|salary|
+--------------------+-----+------+------+
|   [James;, , Smith]|36636|     M|  3000|
|   [Michael, Rose, ]|40288|     M|  4000|
|[Robert, , Williams]|42114|     M|  4000|
|[Maria, Anne, Jones]|39192|     F|  4000|
|  [Jen, Mary, Brown]|     |     F|    -1|
+--------------------+-----+------+------+

新增几列后的数据: 
+--------------------+-----+------+------+
|                name|  dob|gender|salary|
+--------------------+-----+------+------+
|   [James;, , Smith]|36636|     M|  3000|
|   [Michael, Rose, ]|40288|     M|  4000|
|[Robert, , Williams]|42114|     M|  4000|
|[Maria, Anne, Jones]|39192|     F|  4000|
|  [Jen, Mary, Brown]|     |     F|    -1|
+--------------------+-----+------+------+

修改后的列的值为：
+--------------------+-----+------+------+
|                name|  dob|gender|salary|
+--------------------+-----+------+------+
|   [James;, , Smith]|36636|     M|  3000|
|   [Michael, Rose, ]|40288|     M|  4000|
|[Robert, , Williams]|42114|     M|  4000|
|[Maria, Anne, Jones]|39192|     F|  4000|
|  [Jen, Mary, Brown]|     |     F|    -1|
+--------------------+-----+------+------+

依据已有的列创建的结果：
+--------------------+-----+------+------+
|                name|  dob|gender|salary|
+--------------------+-----+------+------+
|   [James;, , Smith]|36636|     M|  3000|
|   [Michael, Rose, ]|40288|     M|  4000|
|[Robert, , Williams]|42114|     M|  4000|
|[Maria, Anne, Jones]|39192|     F|  4000|
|  [Jen, Mary, Brown]|     |     F|    -1|
+--------------------+-----+------+------+

打印列名信息：
root
 |-- name: struct (nullable = true)
 |    |-- firstname: string (nullable = true)
 |    |-- middlename: string (nullable = true)
 |    |-- lastname: string (nullable = true)
 |-- dob: string (nullable = true)
 |-- gender: string (nullable = true)
 |-- salary: string (nullable = true)

对多列进行添加、替换或更新
+--------+------------+-------+
|  salary|CopiedColumn|country|
+--------+------------+-------+
|300000.0|     -3000.0|    USA|
|400000.0|     -4000.0|    USA|
|400000.0|     -4000.0|    USA|
|400000.0|     -4000.0|    USA|
|  -100.0|         1.0|    USA|
+--------+------------+-------+

重命名后的结果：
+--------------------+-----+------+------+
|                name|  dob|gender|salary|
+--------------------+-----+------+------+
|   [James;, , Smith]|36636|     M|  3000|
|   [Michael, Rose, ]|40288|     M|  4000|
|[Robert, , Williams]|42114|     M|  4000|
|[Maria, Anne, Jones]|39192|     F|  4000|
|  [Jen, Mary, Brown]|     |     F|    -1|
+--------------------+-----+------+------+

删除CopiedColumn后的结果：
+--------------------+-----+------+------+
|                name|  dob|gender|salary|
+--------------------+-----+------+------+
|   [James;, , Smith]|36636|     M|  3000|
|   [Michael, Rose, ]|40288|     M|  4000|
|[Robert, , Williams]|42114|     M|  4000|
|[Maria, Anne, Jones]|39192|     F|  4000|
|  [Jen, Mary, Brown]|     |     F|    -1|
+--------------------+-----+------+------+

root
 |-- name: string (nullable = true)
 |-- address: string (nullable = true)

-----------展示生成的多列----------------
root
 |-- First Name: string (nullable = true)
 |-- Last Name: string (nullable = true)
 |-- Address Line1: string (nullable = true)
 |-- City: string (nullable = true)
 |-- State: string (nullable = true)
 |-- zipCode: string (nullable = true)

+----------+---------+--------------+------+-----+-------+
|First Name|Last Name|Address Line1 |City  |State|zipCode|
+----------+---------+--------------+------+-----+-------+
|Robert    |Smith    |1 Main st     |Newark|NJ   |92537  |
|Maria     |Garacia  |3456 Walnut st|Newark|NJ   |94732  |
+----------+---------+--------------+------+-----+-------+

```

The complete code can be downloaded from [GitHub](https://github.com/sparkbyexamples/spark-examples/blob/master/spark-sql-examples/src/main/scala/com/sparkbyexamples/spark/dataframe/WithColumn.scala)

Happy Learning !!

