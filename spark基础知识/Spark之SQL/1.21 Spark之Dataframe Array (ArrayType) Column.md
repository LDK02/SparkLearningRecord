#  Spark 之 DataFrame ArrayType

Spark `ArrayType` (array) is a collection data type that extends `DataType` class, In this article, I will explain how to create a DataFrame ArrayType column using Spark SQL [org.apache.spark.sql.types.ArrayType](https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/types/ArrayType.scala) class and applying some SQL functions on the array column using Scala examples.



While working with Spark structured ([Avro](https://sparkbyexamples.com/spark/read-write-avro-file-spark-dataframe/), [Parquet](https://sparkbyexamples.com/spark/spark-read-write-dataframe-parquet-example/) e.t.c) or semi-structured (JSON) files, we often get data with complex structures like [MapType](https://sparkbyexamples.com/spark/spark-dataframe-map-maptype-column/), ArrayType, Array[[StructType](https://sparkbyexamples.com/spark/spark-sql-structtype-on-dataframe/)] e.t.c. and I will try my best to cover some mostly used functions on ArrayType columns.

## 目录

- What is Spark ArrayType
- Creating ArrayType map Column on Spark DataFrame
  - Using DataTypes.*createArrayType*()
  - Using ArrayType case class

## **What is Spark ArrayType**

Spark ArrayType is a collection data type that extends the [DataType ](https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/types/DataType.scala)class which is a superclass of all types in Spark. All elements of ArrayType should have the same type of elements.

## Creating Spark ArrayType Column on DataFrame

You can create the array column of type `ArrayType` on Spark DataFrame using using `DataTypes.`*`createArrayType`*`()` or using the `ArrayType`scala case class.

## Using DataTypes.createArrayType()

`DataTypes.createArrayType()` method returns a DataFrame column of `ArrayType`. This method takes one argument of type `DataType`meaning any type that extends DataType class.

```scala
val arrayCol = DataTypes.createArrayType(StringType)
```

It also has an overloaded method `DataTypes.createArrayType(DataType,Boolean) `which takes an additional boolean argument to specify if values of a column can accept null or not.

```scala
val mapCol = DataTypes.createArrayType(StringType, true)
```

## Using ArrayType case class

We can also create an instance of an ArrayType using ArraType() case class, This takes arguments valueType and one optional argument “valueContainsNull” to specify if a value can accept null.

```scala
val caseArrayCol = ArrayType(StringType,false)
```

## Example of Spark ArrayType Column on DataFrame

```scala
val arrayStructureData = Seq(
Row("James,,Smith",List("Java","Scala","C++"),List("Spark","Java"),"OH","CA"),
Row("Michael,Rose,",List("Spark","Java","C++"),List("Spark","Java"),"NY","NJ"),
Row("Robert,,Williams",List("CSharp","VB"),List("Spark","Python"),"UT","NV")
)
val arrayStructureSchema = new StructType()
    .add("name",StringType)
    .add("languagesAtSchool", ArrayType(StringType))
    .add("languagesAtWork", ArrayType(StringType))
    .add("currentState", StringType)
    .add("previousState", StringType)
val df = spark.createDataFrame(
    spark.sparkContext.parallelize(arrayStructureData),arrayStructureSchema)
df.printSchema()
df.show()
```

This snippet creates two Array columns “languagesAtSchool” and “languagesAtWork” which ideally defines languages learned at School and languages using at work. And rest of the article will learn several Spark SQL array functions using this DataFrame. printSchema() and show() from above snippet display below output.

```scala
root
 |-- name: string (nullable = true)
 |-- languagesAtSchool: array (nullable = true)
 |    |-- element: string (containsNull = true)
 |-- languagesAtWork: array (nullable = true)
 |    |-- element: string (containsNull = true)
 |-- currentState: string (nullable = true)
 |-- previousState: string (nullable = true)

--------数据集：--------------
+----------------+------------------+---------------+------------+-------------+
|            name| languagesAtSchool|languagesAtWork|currentState|previousState|
+----------------+------------------+---------------+------------+-------------+
|    James,,Smith|[Java, Scala, C++]|  [Spark, Java]|          OH|           CA|
|   Michael,Rose,|[Spark, Java, C++]|  [Spark, Java]|          NY|           NJ|
|Robert,,Williams|      [CSharp, VB]|[Spark, Python]|          UT|           NV|
+----------------+------------------+---------------+------------+-------------+
```

## Spark ArrayType (Array) Functions

[Spark SQL provides several Array functions](https://sparkbyexamples.com/spark/spark-sql-array-functions/) to work with the ArrayType column, In this section, we will see some of the most commonly used SQL functions.s

### explode()

Use explode() function to create a new row for each element in the given array column. There are various [Spark SQL explode functions](https://sparkbyexamples.com/spark/explode-spark-array-and-map-dataframe-column/) available to work with Array columns.

```scala
df.select($"name",explode($"languagesAtSchool")).show(false)

+----------------+------+
|name            |col   |
+----------------+------+
|James,,Smith    |Java  |
|James,,Smith    |Scala |
|James,,Smith    |C++   |
|Michael,Rose,   |Spark |
|Michael,Rose,   |Java  |
|Michael,Rose,   |C++   |
|Robert,,Williams|CSharp|
|Robert,,Williams|VB    |
+----------------+------+
```

### Split()

Splits the inputted column and returns an array type.

```scala
df.select(split($"name",",").as("nameAsArray") ).show(false)

+--------------------+
|nameAsArray         |
+--------------------+
|[James, , Smith]    |
|[Michael, Rose, ]   |
|[Robert, , Williams]|
+--------------------+
```

### array()

Creates a new array column. All input columns must have the same data type.

```scala
df.select($"name",array($"currentState",$"previousState").as("States") ).show(false)
+----------------+--------+
|name            |States  |
+----------------+--------+
|James,,Smith    |[OH, CA]|
|Michael,Rose,   |[NY, NJ]|
|Robert,,Williams|[UT, NV]|
+----------------+--------+
```

### array_contains()

Returns null if the array is null, true if the array contains `value`, and false otherwise.

```scala
df.select($"name",array_contains($"languagesAtSchool","Java")
    .as("array_contains")).show(false)

+----------------+--------------+
|name            |array_contains|
+----------------+--------------+
|James,,Smith    |true          |
|Michael,Rose,   |true          |
|Robert,,Williams|false         |
+----------------+--------------+
```

## 完整代码：

```scala
package sparkScalaExamples.SQL

import org.apache.spark.sql.types.{ArrayType, StringType, StructType}
import org.apache.spark.sql.{Row, SparkSession}

object ArrayTypeDemo {
  def main(args: Array[String]): Unit = {
    val spark: SparkSession =
      SparkSession
        .builder()
        .appName("ArrayTypeDemo")
        .master("local").getOrCreate()

    /**1 准备数据
    * **/
    val arrayStructureData = Seq(
      Row("James,,Smith",List("Java","Scala","C++"),List("Spark","Java"),"OH","CA"),
      Row("Michael,Rose,",List("Spark","Java","C++"),List("Spark","Java"),"NY","NJ"),
      Row("Robert,,Williams",List("CSharp","VB"),List("Spark","Python"),"UT","NV")
    )
    val arrayStructureSchema = new StructType()
      .add("name",StringType)
      //使用arrayType
      .add("languagesAtSchool", ArrayType(StringType))
      //使用arrayType
      .add("languagesAtWork", ArrayType(StringType))
      .add("currentState", StringType)
      .add("previousState", StringType)
    val df = spark.createDataFrame(
      spark.sparkContext.parallelize(arrayStructureData),arrayStructureSchema)
    df.printSchema()
    println("--------数据集：--------------")
    df.show()
    import org.apache.spark.sql.functions._
    /**
      * 2 案例：
      * 将列"languagesAtSchool"的值转为单个值的列，如：
      * +------------------+
      | languagesAtSchool|
      +------------------+
      |[Java, Scala, C++]|
      转为：
      *  列名
      * +------+
      |col   |
      +------+
      |Java  |
      |Scala |
      |C++   |
      *
      * */
    df.select(col("name"),explode(col("languagesAtSchool"))).show(false)
    /**
      * 3 案例：
      * 将原始数据中的name列拆分：
      * 如：
      * James,,Smith
      * 转为：
      * [James, , Smith]
      * */
    df.select(split(col("name"),",").as('nameAaArray)).show(false)

    /**
      * 4 案例
      * 将原始dataframe的最后两列合并为一列，并命名为States
      * 
      * 如：
      * +------------+-------------+
        |currentState|previousState|
        +------------+-------------+
        |          OH|           CA|
        |          NY|           NJ|
        |          UT|           NV|
        +------------+-------------+
      转为：
      +--------+
      |States  |
      +--------+
      |[OH, CA]|
      |[NY, NJ]|
      |[UT, NV]|
      +--------+
      * */
    df.select(col("name"),array(col("currentState"),col("previousState")).as("States")).show(false)

    /**5 案例
      * 判断dataframe中列languagesAtSchool是否包含“Java”,包含则转为true
      * 如：
      * +--------------+
        |array_contains|
        +--------------+
        |true          |
        |true          |
        |false         |
        +--------------+
      * */
    df.select(col("name"),array_contains(col("languagesAtSchool"),"Java").as("array_contains")).show(false)


  }


}

```



## Conclusion

You have learned Spark ArrayType is a collection type similar to an array in other languages that are used to store the same type of elements. ArrayType extends DataType class (superclass of all types in Spark) and also learned how to use some commonly used ArrayType functions.

Happy Learning !!