# Spark DataFrame 之MapType

In this article, I will explain how to create a Spark DataFrame MapType (map) column using org.apache.spark.sql.types.MapType class and applying some DataFrame SQL functions on the map column using the Scala examples.

While working with Spark structured (Avro, Parquet e.t.c) or semi-structured (JSON) files, we often get data with complex structures like MapType, ArrayType, Array[StructType] e.t.c. and there are not many good articles that explain these. I will try my best to cover some mostly used functions on MapType columns

## 目录

- [1. Spark MapType是什么](#1. Spark MapType是什么)
- [2. 在 Spark DataFrame 上创建 MapType 类型的列](#2. 在 Spark DataFrame 上创建 MapType 类型的列)
  - [2.1 使用 Spark DataTypes.*createMapType*()](#2.1 使用 Spark DataTypes.*createMapType*())
  - [2.2 使用 MapType 案例类创建](#2.2 使用 MapType 案例类创建)
- [3. Spark SQL 函数与MapType列一起使用](#3. Spark SQL 函数与MapType列一起使用)
  - [3.1 从 DataFrame MapType 列中获取所有的键](#3.1 从 DataFrame MapType 列中获取所有的键)
  - [3.2 从DataFrame MapType 列获取所有地图值](#3.2 从DataFrame MapType 列获取所有地图值)
  - [3.3 使用 map_concat() 合成MapType列](#3.3 使用 map_concat() 合成MapType列)
  - [3.4 将 StructType 数组转换为MapType](#3.4 将 StructType 数组转换为MapType)
  - [3.5 将StructType的map转换为StructType的数组](#3.5 将StructType的map转换为StructType的数组)
- [4、在Spark DataFrame上动态创建MapType](#4、在Spark DataFrame上动态创建MapType)

## 1. Spark MapType是什么

Spark MapType class extends [DataType ](https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/types/DataType.scala)class which is a superclass of all types in Spark and it takes two mandatory arguments “keyType” and “valueType” of type DataType and one optional boolean argument `valueContainsNull`. keyType and valueType can be any type that extends the DataType class. for e.g StringType, IntegerType, ArrayType, MapType, StructType (struct) e.t.c.

***Note:\*** *Keys in a map are not allowed to have `null` values.*

## 2. 在 Spark DataFrame 上创建 MapType 类型的列

You can create the instance of the MapType on Spark DataFrame using `DataTypes.`*`createMapType`*`()` or using the MapType scala case class.

### 2.1 使用 Spark DataTypes.*createMapType*()

We can create a map column using `createMapType()` function on the DataTypes class. This method takes two arguments keyType and valueType as mentioned above and these two arguments should be of a type that extends DataType.

```scala
val mapCol = DataTypes.createMapType(StringType, StringType)
```

This snippet creates “mapCol” object of type MapType with key and values as String type.

```scala
val mapCol = DataTypes.createMapType((StringType,
StructType(Array(StructField("col1",StringType),StructField("col2",StringType )))
```

This snippet creates “mapCol” object of type MapType with key as StringType and value as struct (StructType) with columns “col1” and “col2”.

### 2.2 使用 MapType 案例类创建

We can also create an instance of a MapType using MapType() case class, This takes 2 mandatory argument key and value and one optional argument “valueContainsNull” to specify if a value can accept null.

```scala
val caseMapCol = MapType(StringType,StringType,false)
```

```scala
val caseMapCol = MapType(StringType,StructType(Array(
StructField("col1",StringType),
StructField("col1",StringType ))))
```

This snippet creates “caseMapCol” object of type MapType with key as StringType and value as struct with columns “col1” and “col2”.

## 准备数据集

```scala
  val arrayStructureData = Seq(
    Row("James",List(Row("Newark","NY"),Row("Brooklyn","NY")),
      Map("hair"->"black","eye"->"brown"), Map("height"->"5.9")),
    Row("Michael",List(Row("SanJose","CA"),Row("Sandiago","CA")),
      Map("hair"->"brown","eye"->"black"),Map("height"->"6")),
    Row("Robert",List(Row("LasVegas","NV")),
      Map("hair"->"red","eye"->"gray"),Map("height"->"6.3")),
    Row("Maria",null,Map("hair"->"blond","eye"->"red"),
      Map("height"->"5.6")),
    Row("Jen",List(Row("LAX","CA"),Row("Orange","CA")),
      Map("white"->"black","eye"->"black"),Map("height"->"5.2"))
  )

  val mapType  = DataTypes.createMapType(StringType,StringType)

  val arrayStructureSchema = new StructType()
    .add("name",StringType)
    .add("addresses", ArrayType(new StructType()
      .add("city",StringType)
      .add("state",StringType)))
    .add("properties", mapType)
    .add("secondProp", MapType(StringType,StringType))

  val mapTypeDF = spark.createDataFrame(
    spark.sparkContext.parallelize(arrayStructureData),arrayStructureSchema)
  mapTypeDF.printSchema()
  mapTypeDF.show()
```

Outputs:

```scala
root
 |-- name: string (nullable = true)
 |-- addresses: array (nullable = true)
 |    |-- element: struct (containsNull = true)
 |    |    |-- city: string (nullable = true)
 |    |    |-- state: string (nullable = true)
 |-- properties: map (nullable = true)
 |    |-- key: string
 |    |-- value: string (valueContainsNull = true)
 |-- secondProp: map (nullable = true)
 |    |-- key: string
 |    |-- value: string (valueContainsNull = true)

+-------+-------------------------------+------------------------------+---------------+
|name   |addresses                      |properties                    |secondProp     |
+-------+-------------------------------+------------------------------+---------------+
|James  |[[Newark, NY], [Brooklyn, NY]] |[hair -> black, eye -> brown] |[height -> 5.9]|
|Michael|[[SanJose, CA], [Sandiago, CA]]|[hair -> brown, eye -> black] |[height -> 6]  |
|Robert |[[LasVegas, NV]]               |[hair -> red, eye -> gray]    |[height -> 6.3]|
|Maria  |null                           |[hair -> blond, eye -> red]   |[height -> 5.6]|
|Jen    |[[LAX, CA], [Orange, CA]]      |[white -> black, eye -> black]|[height -> 5.2]|
+-------+-------------------------------+------------------------------+---------------+
```

## 3. Spark SQL 函数与MapType列一起使用

Spark SQL provides several [map functions to work with MapType](https://sparkbyexamples.com/spark/spark-sql-map-functions/), In this section, we will see some of the most commonly used SQL functions

### 3.1 从 DataFrame MapType 列中获取所有的键

Use `map_keys()` spark function in order to retrieve all keys from a Spark DataFrame [MapType ](https://sparkbyexamples.com/spark/spark-sql-structtype-on-dataframe/#arraytype-maptype)column. Note that map_keys takes an argument of MapType while passing any other type returns an error at run time.

```scala
mapTypeDF.select(col("name"),map_keys(col("properties"))).show(false)
```

Outputs:

```scala
+-------+--------------------+
|name   |map_keys(properties)|
+-------+--------------------+
|James  |[hair, eye]         |
|Michael|[hair, eye]         |
|Robert |[hair, eye]         |
|Maria  |[hair, eye]         |
|Jen    |[white, eye]        |
+-------+--------------------+
```

### 3.2 Getting all map values from the DataFrame MapType column

Use `map_values()` spark function to retrieve all values from a Spark DataFrame  [MapType](https://sparkbyexamples.com/spark/spark-sql-structtype-on-dataframe/#arraytype-maptype) column. Note that map_values takes an argument of MapType while passing any other type returns an error at run time.

```scala
mapTypeDF.select(col("name"),map_values(col("properties"))).show(false)
```

Outputs:

```scala
+-------+----------------------+
|name   |map_values(properties)|
+-------+----------------------+
|James  |[black, brown]        |
|Michael|[brown, black]        |
|Robert |[red, gray]           |
|Maria  |[blond, red]          |
|Jen    |[black, black]        |
+-------+----------------------+
```

### 3.3 使用 map_concat() 合成MapType列

Using Spark SQL `map_concat()` function we should able to merge keys and values from more than one map to a single map. All arguments to this function should be [MapType](https://sparkbyexamples.com/spark/spark-sql-structtype-on-dataframe/#arraytype-maptype), passing any other type results a runtime error.

```scala
 mapTypeDF.select(col("name"),map_concat(col("properties"),col("secondProp"))).show(false)
```

Here, we have merged map columns “properties” and “secondProp” into a single column. This yields the below output.

```scala
+-------+---------------------------------------------+
|name   |map_concat(properties, secondProp)           |
+-------+---------------------------------------------+
|James  |[hair -> black, eye -> brown, height -> 5.9] |
|Michael|[hair -> brown, eye -> black, height -> 6]   |
|Robert |[hair -> red, eye -> gray, height -> 6.3]    |
|Maria  |[hair -> blond, eye -> red, height -> 5.6]   |
|Jen    |[white -> black, eye -> black, height -> 5.2]|
+-------+---------------------------------------------+
```

### 3.4 将 StructType 数组转换为MapType

Use [map_from_entries()](https://sparkbyexamples.com/spark/spark-sql-map-functions/#map-from-entries) SQL function to convert array of [StructType](https://sparkbyexamples.com/spark/spark-sql-structtype-on-dataframe/) entries to map ([MapType](https://sparkbyexamples.com/spark/spark-sql-structtype-on-dataframe/#arraytype-maptype)) on Spark DataFrame. This function takes DataFrame column ArrayType[StructType] as an argument, passing any other type results in an error.

### 3.5 将StructType的map转换为StructType的数组

Use Spark SQL [map_entries()](https://sparkbyexamples.com/spark/spark-sql-map-functions/#map-entries) function to convert map of [StructType](https://sparkbyexamples.com/spark/spark-sql-structtype-on-dataframe/) to array of Structype (struct) column on DataFrame.

## 4、在Spark DataFrame上动态创建MapType

`map() `SQL function is used to create a map column of [MapType](https://sparkbyexamples.com/spark/spark-sql-structtype-on-dataframe/#arraytype-maptype) on DataFrame dynamically at runtime, The input columns to the map function must be grouped as key-value pairs. e.g. (key1, value1, key2, value2, …).

**Note:** All key columns must have the same data type, and can’t be null and All value columns must have the same data type. Below snippet converts all columns from “properties” struct into map key, value pairs “propertiesMap” column.

```scala
val structureData = Seq(
  Row("36636","Finance",Row(3000,"USA")),
  Row("40288","Finance",Row(5000,"IND")),
  Row("42114","Sales",Row(3900,"USA")),
  Row("39192","Marketing",Row(2500,"CAN")),
  Row("34534","Sales",Row(6500,"USA"))
)

val structureSchema = new StructType()
  .add("id",StringType)
  .add("dept",StringType)
  .add("properties",new StructType()
    .add("salary",IntegerType)
    .add("location",StringType)
  )

var df = spark.createDataFrame(
  spark.sparkContext.parallelize(structureData),structureSchema)

val index = df.schema.fieldIndex("properties")
val propSchema = df.schema(index).dataType.asInstanceOf[StructType]
var columns = mutable.LinkedHashSet[Column]()
propSchema.fields.foreach(field =>{
  columns.add(lit(field.name))
  columns.add(col("properties." + field.name))
})

df = df.withColumn("propertiesMap",map(columns.toSeq:_*))
df = df.drop("properties")
df.printSchema()
df.show(false)
```

First, we find “properties” column on Spark DataFrame using `df.schema.fieldIndex(“properties”)` and retrieves all columns and it’s values to a LinkedHashSet. we need LinkedHashSet in order to maintain the insertion order of key and value pair. and finally use map() function with a key, value set pair.

```scala
root
 |-- id: string (nullable = true)
 |-- dept: string (nullable = true)
 |-- propertiesMap: map (nullable = false)
 |    |-- key: string
 |    |-- value: string (valueContainsNull = true)

+-----+---------+---------------------------------+
|id   |dept     |propertiesMap                    |
+-----+---------+---------------------------------+
|36636|Finance  |[salary -> 3000, location -> USA]|
|40288|Finance  |[salary -> 5000, location -> IND]|
|42114|Sales    |[salary -> 3900, location -> USA]|
|39192|Marketing|[salary -> 2500, location -> CAN]|
|34534|Sales    |[salary -> 6500, location -> USA]|
+-----+---------+---------------------------------+
```

## Conclusion

In this article, you have learned how to create a Spark MapType (map) column on DataFrame using case class and DataTypes. And also explored some of the SQL functions to work with MapType.

Happy Learning !!