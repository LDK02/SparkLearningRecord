# Spark 之 展平嵌套结构列

In Spark SQL, flatten nested struct column (convert struct to columns) of a DataFrame is simple for one level of the hierarchy and complex when you have multiple levels and hundreds of columns. When you have one level of `structure` you can simply flatten by referring structure by dot notation but when you have a multi-level struct column then things get complex and you need to write a logic to iterate all columns and comes up with a column list to use.

在 Spark SQL 中，<font color=orAGNE>扁平化 DataFrame 的嵌套结构列（将结构转换为列）对于层次结构的一层来说很简单，而当你有多个层次和数百列时，它就很复杂</font>>。 当你有一个层次的“结构”时，你可以简单地通过点符号引用结构来展平，但是当你有一个多层次的结构列时，事情会变得复杂，你需要编写一个逻辑来迭代所有列并提出一个列 要使用的列表。

In this article, I will explain how to convert/flatten the nested (single or multi-level) [struct column](https://sparkbyexamples.com/spark/spark-sql-structtype-on-dataframe/) using a Scala example.

在本文中，我将解释如何使用 Scala 转换/展平嵌套（单级或多级）[结构列]） 例子。

## 目录

[1 准备数据](#1 准备数据)

[2 展平案例](#2 展平案例)

[3 完整代码](#3 完整代码)

[4 总结](#4 总结)

## 1 准备数据

First, let’s create a DataFrame with nested structure column.

首先，创建一个含有嵌套数据结构的Dataframe，具体如下：

```scala
val structureData = Seq(
    Row(Row("James ","","Smith"),Row(Row("CA","Los Angles"),Row("CA","Sandiago"))),
    Row(Row("Michael ","Rose",""),Row(Row("NY","New York"),Row("NJ","Newark"))),
    Row(Row("Robert ","","Williams"),Row(Row("DE","Newark"),Row("CA","Las Vegas"))),
    Row(Row("Maria ","Anne","Jones"),Row(Row("PA","Harrisburg"),Row("CA","Sandiago"))),
    Row(Row("Jen","Mary","Brown"),Row(Row("CA","Los Angles"),Row("NJ","Newark")))
  )

val structureSchema = new StructType()
    .add("name",new StructType()
      .add("firstname",StringType)
      .add("middlename",StringType)
      .add("lastname",StringType))
    .add("address",new StructType()
      .add("current",new StructType()
        .add("state",StringType)
        .add("city",StringType))
      .add("previous",new StructType()
        .add("state",StringType)
        .add("city",StringType)))

val df = spark.createDataFrame(
    spark.sparkContext.parallelize(structureData),structureSchema)
df.printSchema()
```

输出：

```scala
root
 |-- name: struct (nullable = true)
 |    |-- firstname: string (nullable = true)
 |    |-- middlename: string (nullable = true)
 |    |-- lastname: string (nullable = true)
 |-- address: struct (nullable = true)
 |    |-- current: struct (nullable = true)
 |    |    |-- state: string (nullable = true)
 |    |    |-- city: string (nullable = true)
 |    |-- previous: struct (nullable = true)
 |    |    |-- state: string (nullable = true)
 |    |    |-- city: string (nullable = true)
```

From this example, column “firstname” is the first level of nested structure, and columns “state” and “city” are multi-level structures (meaning 2 or more level deep in the hierarchy).

在此示例中，**列“firstname”是嵌套结构的第一级，列“state”和“city”是多级结构（意味着层次结构中的 2 级或更多级**）。

```scala
df.show(false)
```

输出结果：

```scala
+---------------------+----------------------------------+
|name                 |address                           |
+---------------------+----------------------------------+
|[James , , Smith]    |[[CA, Los Angles], [CA, Sandiago]]|
|[Michael , Rose, ]   |[[NY, New York], [NJ, Newark]]    |
|[Robert , , Williams]|[[DE, Newark], [CA, Las Vegas]]   |
|[Maria , Anne, Jones]|[[PA, Harrisburg], [CA, Sandiago]]|
|[Jen, Mary, Brown]   |[[CA, Los Angles], [NJ, Newark]]  |
+---------------------+----------------------------------+
```

## 2 展平案例

### 2.1 使用col方法进行平展

Now, let’s convert it using a simple way. Here, we refer [nested struct columns](https://sparkbyexamples.com/spark/spark-sql-structtype-on-dataframe/#nested-spark-schema) by using dot notation (parentColumn.childColumn)

现在，让我们用一种简单的方法来转换它。 在这里，我们使用点表示法 (parentColumn.childColumn) 引用 [嵌套结构列](https://sparkbyexamples.com/spark/spark-sql-structtype-on-dataframe/#nested-spark-schema)

```scala
val df2 = df.select(col("name.*"),
    col("address.current.*"),
    col("address.previous.*"))
val df2Flatten = df2.toDF("fname","mename","lname","currAddState",
    "currAddCity","prevAddState","prevAddCity")
df2Flatten.printSchema()
df2Flatten.show(false)
```

The above snippet flattens all columns in a DataFrame. Using this approach you can also choose what columns you wanted to flatten.

上面的代码片段展平了 DataFrame 中的所有列。 使用这种方法，您还可以选择要展平的列。

```scala
root
 |-- name_firstname: string (nullable = true)
 |-- name_middlename: string (nullable = true)
 |-- name_lastname: string (nullable = true)
 |-- address_current_state: string (nullable = true)
 |-- address_current_city: string (nullable = true)
 |-- address_previous_state: string (nullable = true)
 |-- address_previous_city: string (nullable = true)

+--------+------+--------+------------+-----------+------------+-----------+
|fname   |mename|lname   |currAddState|currAddCity|prevAddState|prevAddCity|
+--------+------+--------+------------+-----------+------------+-----------+
|James   |      |Smith   |CA          |Los Angles |CA          |Sandiago   |
|Michael |Rose  |        |NY          |New York   |NJ          |Newark     |
|Robert  |      |Williams|DE          |Newark     |CA          |Las Vegas  |
|Maria   |Anne  |Jones   |PA          |Harrisburg |CA          |Sandiago   |
|Jen     |Mary  |Brown   |CA          |Los Angles |NJ          |Newark     |
+--------+------+--------+------------+-----------+------------+-----------+
```

Since we have limited columns, it seems to be easy by referring column names, but imagine how cumbersome it would be if you have 100+ columns and referring all columns in a select.

由于我们的列有限，<font color=orange>通过引用列名似乎很容易，但想象一下如果您有 100 多个列并在一个选择中引用所有列</font>，那将是多么麻烦。

### 2.2 自定义函数进行平展

Now let’s see a different way where you can easily flatten hundreds of nested level columns. Will do this by creating a nested function `flattenStructSchema()` which iterates the schema at every level and creates an Array[Column]

现在让我们看看另一种方式，您可以轻松地展平数百个嵌套级别的列。 将通过创建一个嵌套函数“flattenStructSchema()”来做到这一点，<font color=lightblue>该函数在每个级别迭代模式并创建一个 Array[Column]</font>

```scala
def flattenStructSchema(schema: StructType, prefix: String = null) : Array[Column] = {
    schema.fields.flatMap(f => {
      val columnName = if (prefix == null) f.name else (prefix + "." + f.name)

      f.dataType match {
        case st: StructType => flattenStructSchema(st, columnName)
        case _ => Array(col(columnName).as(columnName.replace(".","_")))
      }
    })
  }
```

To make it simple I still use the same DataFrame from the previous section.

为了简单起见，我仍然使用上一节中的相同 DataFrame。

```scala
val df3 = df.select(flattenStructSchema(df.schema):_*)
df3.printSchema()
df3.show(false)
```

输出：

```scala
+--------------+---------------+-------------+---------------------+--------------------+----------------------+---------------------+
|name.firstname|name.middlename|name.lastname|address.current.state|address.current.city|address.previous.state|address.previous.city|
+--------------+---------------+-------------+---------------------+--------------------+----------------------+---------------------+
|James         |               |Smith        |CA                   |Los Angles          |CA                    |Sandiago             |
|Michael       |Rose           |             |NY                   |New York            |NJ                    |Newark               |
|Robert        |               |Williams     |DE                   |Newark              |CA                    |Las Vegas            |
|Maria         |Anne           |Jones        |PA                   |Harrisburg          |CA                    |Sandiago             |
|Jen           |Mary           |Brown        |CA                   |Los Angles          |NJ                    |Newark               |
+--------------+---------------+-------------+---------------------+--------------------+----------------------+---------------------+
```

## 3 完整代码

```scala
package sparkScalaExamples.SQL

import org.apache.spark.sql.types.{StringType, StructType}
import org.apache.spark.sql.{Column, DataFrame, Row, SparkSession}

object FlattenNestStructDataFrame {
  def main(args: Array[String]): Unit = {
    val spark: SparkSession =
      SparkSession.builder()
        .appName("flattendataframe")
        .master("local")
        .getOrCreate()


    /***
      * 1 准备数据
      * */
    val structureData = Seq(
      Row(Row("James ","","Smith"),Row(Row("CA","Los Angles"),Row("CA","Sandiago"))),
      Row(Row("Michael ","Rose",""),Row(Row("NY","New York"),Row("NJ","Newark"))),
      Row(Row("Robert ","","Williams"),Row(Row("DE","Newark"),Row("CA","Las Vegas"))),
      Row(Row("Maria ","Anne","Jones"),Row(Row("PA","Harrisburg"),Row("CA","Sandiago"))),
      Row(Row("Jen","Mary","Brown"),Row(Row("CA","Los Angles"),Row("NJ","Newark")))
    )

    val structureSchema = new StructType()
      .add("name",new StructType()
        .add("firstname",StringType)
        .add("middlename",StringType)
        .add("lastname",StringType))
      .add("address",new StructType()
        .add("current",new StructType()
          .add("state",StringType)
          .add("city",StringType))
        .add("previous",new StructType()
          .add("state",StringType)
          .add("city",StringType)))

    val df = spark.createDataFrame(
      spark.sparkContext.parallelize(structureData),structureSchema)
    println("------------数据结构：-------------")
    df.printSchema()
    println("-------展示数据-------------")
    df.show(false)

    /***
      * 2 将上面案例中的dataframe中的内置数据结构，平展开来，将原有的2列数据，
      * 变为7列数据
      * */
    import org.apache.spark.sql.functions._
    val df2: DataFrame = df.select(col("name.*"),
      col("address.current.*"),
      col("address.previous.*"))
    //重新命名列名
    val df2Flatten: DataFrame = df2.toDF("fname", "mename", "lname", "currAddState",
      "currAddCity", "prevAddState", "prevAddCity")
    println("-----展平后的数据结构-------------")
    df2Flatten.printSchema()
    println("-----展平后的数据：------------")
    df2Flatten.show()

    /**
      * 3 当超过有100列时，要是按照案例的2的方法，则实现过程会复杂，
      * 故可以定义函数，以及dataframe的schema信息，进行自动平展，具体案例如下
      * */

    def flattenStructSchema(schema:StructType,prefix:String=null):Array[Column]={
      
      schema.fields.flatMap(f => {
        val columName = if (prefix == null) f.name else (prefix + "." + f.name)
        f.dataType match {
          case st:StructType => flattenStructSchema(st,columName)
          case _ => Array(col(columName).as(columName.replace(".","-")))
        }
      })

    }

    val df3: DataFrame = df.select(flattenStructSchema(df.schema):_*)
    println("---------------")
    df3.printSchema()
    df3.show()


  }

}

```

## 4 总结

In this Spark article, you have learned how to flatten nested struct column (convert struct to columns) for simple and complex struct types. Hope you like this.

在这篇 Spark 文章中，您学习了<font color=red>如何为简单和复杂的结构类型展平嵌套结构列（将结构转换为列）</font>。 希望你喜欢这个。

**相关文章**

- [How to flatten nested array column](https://sparkbyexamples.com/spark/spark-flatten-nested-array-column-to-single-column/)
- [How to explode Array & Map columns to rows](https://sparkbyexamples.com/spark/explode-spark-array-and-map-dataframe-column/)