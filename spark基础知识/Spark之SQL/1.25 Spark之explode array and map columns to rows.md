# 将内嵌数据结构转为多行

In this article, I will explain how to explode array or list and map DataFrame columns to rows using different Spark explode functions (explode, explore_outer, posexplode, posexplode_outer) with Scala example.

While working with structured files like [JSON](https://sparkbyexamples.com/spark/spark-read-and-write-json-file/), [Parquet](https://sparkbyexamples.com/spark/spark-read-write-dataframe-parquet-example/), [Avro](https://sparkbyexamples.com/spark/read-write-avro-file-spark-dataframe/), and [XML](https://sparkbyexamples.com/spark/processing-xml-files-in-spark/) we often get data in collections like arrays, lists, and maps, In such cases, these explode functions are useful to convert collection columns to rows in order to process in Spark effectively.

## 目录



Though I’ve explained here with Scala, a similar method could be used to explode array and map columns to rows with PySpark and if time permits I will cover it in the future. If you are looking for PySpark, I would still recommend reading through this article as it would give you an Idea on Spark explode functions and usage.

Before we start, let’s create a DataFrame with array and map fields, below snippet, creates a DF with columns “name” as StringType, “knownLanguage” as [ArrayType ](https://sparkbyexamples.com/spark/spark-array-arraytype-dataframe-column/)and “properties” as [MapType](https://sparkbyexamples.com/spark/spark-dataframe-map-maptype-column/).

And, from below code, “spark” is an instance of SparkSession, please refer to complete code at the end to see how to create SparkSession object.

```scala
    import spark.implicits._

    val arrayData = Seq(
      Row("James",List("Java","Scala"),Map("hair"->"black","eye"->"brown")),
    Row("Michael",List("Spark","Java",null),Map("hair"->"brown","eye"->null)),
    Row("Robert",List("CSharp",""),Map("hair"->"red","eye"->"")),
    Row("Washington",null,null),
    Row("Jefferson",List(),Map())
    )

    val arraySchema = new StructType()
      .add("name",StringType)
      .add("knownLanguages", ArrayType(StringType))
      .add("properties", MapType(StringType,StringType))

    val df = spark.createDataFrame(spark.sparkContext.parallelize(arrayData),arraySchema)
    df.printSchema()
    df.show(false)
```

## 1 explode – spark explode array or map column to rows

Spark function `explode(e: Column)` is used to explode or create array or map columns to rows. When an array is passed to this function, it creates a new default column “col1” and it contains all array elements. When a map is passed, it creates two new columns one for key and one for value and each element in map split into the row.

This will ignore elements that have null or empty. from the above example, Washington and Jefferson have null or empty values in array and map, hence the following snippet out does not contain these rows.

#### **explode – array column example**

```scala
df.select($"name",explode($"knownLanguages"))
      .show(false)
```

Outputs:

```scala
+-------+------+
|name   |col   |
+-------+------+
|James  |Java  |
|James  |Scala |
|Michael|Spark |
|Michael|Java  |
|Michael|null  |
|Robert |CSharp|
|Robert |      |
+-------+------+
```

#### **explode – map column example**

```scala
    df.select($"name",explode($"properties"))
      .show(false)
```



Outputs:

```scala
+-------+----+-----+
|name   |key |value|
+-------+----+-----+
|James  |hair|black|
|James  |eye |brown|
|Michael|hair|brown|
|Michael|eye |null |
|Robert |hair|red  |
|Robert |eye |     |
+-------+----+-----+
```

## 2 explode_outer – Create rows for each element in an array or map.

Spark SQL `explode_outer(e: Column)` function is used to create a row for each element in the array or map column. Unlike explode, if the array or map is null or empty, explode_outer returns null.

#### **explode_outer – array example**

```scala
    df.select($"name",explode_outer($"knownLanguages"))
      .show(false)
```

Outputs:

```scala
+----------+------+
|name      |col   |
+----------+------+
|James     |Java  |
|James     |Scala |
|Michael   |Spark |
|Michael   |Java  |
|Michael   |null  |
|Robert    |CSharp|
|Robert    |      |
|Washington|null  |
|Jeferson  |null  |
+----------+------+
```

#### **explode_outer – map example**

```scala
    df.select($"name",explode_outer($"properties"))
      .show(false)
```

Outputs:

```scala
+----------+----+-----+
|name      |key |value|
+----------+----+-----+
|James     |hair|black|
|James     |eye |brown|
|Michael   |hair|brown|
|Michael   |eye |null |
|Robert    |hair|red  |
|Robert    |eye |     |
|Washington|null|null |
|Jeferson  |null|null |
+----------+----+-----+
```

## 3 posexplode – explode array or map elements to rows

`posexplode(e: Column)` creates a row for each element in the array and creates two columns “pos’ to hold the position of the array element and the ‘col’ to hold the actual array value. And when the input column is a map, posexplode function creates 3 columns “pos” to hold the position of the map element, “key” and “value” columns.

This will ignore elements that have null or empty. Since the Washington and Jefferson have null or empty values in array and map, the following snippet out does not contain these.

#### **posexplode – array example**

```scala
    df.select($"name",posexplode($"knownLanguages"))
      .show(false)
```

Outputs:

```scala
+-------+---+------+
|name   |pos|col   |
+-------+---+------+
|James  |0  |Java  |
|James  |1  |Scala |
|Michael|0  |Spark |
|Michael|1  |Java  |
|Michael|2  |null  |
|Robert |0  |CSharp|
|Robert |1  |      |
+-------+---+------+
```

#### **posexplode – map example**

```scala
    df.select($"name",posexplode($"properties"))
      .show(false)
```

Outputs:

```scala
+-------+---+----+-----+
|name   |pos|key |value|
+-------+---+----+-----+
|James  |0  |hair|black|
|James  |1  |eye |brown|
|Michael|0  |hair|brown|
|Michael|1  |eye |null |
|Robert |0  |hair|red  |
|Robert |1  |eye |     |
+-------+---+----+-----+
```

## 4 posexplode_outer – explode array or map columns to rows.

Spark `posexplode_outer(e: Column)` creates a row for each element in the array and creates two columns “pos’ to hold the position of the array element and the ‘col’ to hold the actual array value. Unlike posexplode, if the array or map is null or empty, posexplode_outer function returns null, null for pos and col columns. Similarly for the map, it returns rows with nulls.

#### **posexplode_outer – array example**

```scala
    df.select($"name",posexplode_outer($"knownLanguages"))
      .show(false)
```

Outputs:

```scala
+----------+----+------+
|name      |pos |col   |
+----------+----+------+
|James     |0   |Java  |
|James     |1   |Scala |
|Michael   |0   |Spark |
|Michael   |1   |Java  |
|Michael   |2   |null  |
|Robert    |0   |CSharp|
|Robert    |1   |      |
|Washington|null|null  |
|Jeferson  |null|null  |
+----------+----+------+
```

#### **posexplode_outer – map example**

```scala
    df.select($"name",posexplode_outer($"properties"))
      .show(false)
```

Outputs:

```scala
+----------+----+----+-----+
|name      |pos |key |value|
+----------+----+----+-----+
|James     |0   |hair|black|
|James     |1   |eye |brown|
|Michael   |0   |hair|brown|
|Michael   |1   |eye |null |
|Robert    |0   |hair|red  |
|Robert    |1   |eye |     |
|Washington|null|null|null |
|Jeferson  |null|null|null |
+----------+----+----+-----+
```

## 5 The complete example of exploding array or maps to rows

```scala
package sparkScalaExamples.SQL

import org.apache.spark.sql.types.{ArrayType, MapType, StringType, StructType}
import org.apache.spark.sql.{Row, SparkSession}

object explodeDemo {
  def main(args: Array[String]): Unit = {

    val spark: SparkSession =
      SparkSession
        .builder()
        .master("local")
        .appName("explodeDemo")
        .getOrCreate()

    /**
      * 1 准备数据
      * */
    import spark.implicits._

    val arrayData = Seq(
      Row("James",List("Java","Scala"),Map("hair"->"black","eye"->"brown")),
      Row("Michael",List("Spark","Java",null),Map("hair"->"brown","eye"->null)),
      Row("Robert",List("CSharp",""),Map("hair"->"red","eye"->"")),
      Row("Washington",null,null),
      Row("Jefferson",List(),Map())
    )

    val arraySchema = new StructType()
      .add("name",StringType)
      .add("knownLanguages", ArrayType(StringType))
      .add("properties", MapType(StringType,StringType))

    val df = spark.createDataFrame(spark.sparkContext.parallelize(arrayData),arraySchema)
    df.printSchema()
    df.show(false)
    /**
      * 2 案例1
      *  将列的值为列表类型的数据，转为多列
      *  将列的值为hashmap类型的数据，转为多列
      * */
    import org.apache.spark.sql.functions._
    df.select($"name",explode($"knownLanguages")).show(false)

    df.select($"name",explode($"properties")).show(false)

    /**
      * 案例 2
      * 案例1 在处理数据时遇到空值或null时，会返回null或空
      * 当数据为null/empty时，则返回null
      * */
    df.select($"name",explode_outer($"knownLanguages")).show(false)

    df.select($"name",explode_outer($"properties")).show(false)
    
    /**案例3
      * 在展开内置的数据类型时，记录元素在的位置
      * */
    df.select($"name",posexplode($"knownLanguages")).show()
    df.select($"name",posexplode($"properties")).show(false)
    
    /** 案例4
      * 遇到到空或null时，统一返回null
    * */
    df.select($"name",posexplode_outer($"knownLanguages")).show()
    df.select($"name",posexplode_outer($"properties")).show(false)
    

  }

}

```

## Some common faq’s of explode functions

**What is explode function**

Spark SQL explode function is used to create or split an array or map DataFrame columns to rows. Spark defines several flavors of this function; explode_outer – to handle nulls and empty, posexplode – which explodes with a position of element and posexplode_outer – to handle nulls.

**Difference between explode vs explode_outer**

explode – creates a row for each element in the array or map column by ignoring null or empty values in array. whereas explode_outer returns all values in array or map including null or empty.

**Difference between explode vs posexplode**

explode – creates a row for each element in the array or map column. whereas posexplode creates a row for each element in the array and creates two columns ‘pos’ to hold the position of the array element and the ‘col’ to hold the actual array value. And, for the map, it creates 3 columns ‘pos’, ‘key’ and ‘value’

## Conclusion

In this article, you have learned how to how to explode or convert array or map DataFrame columns to rows using explode and posexplode SQL functions and their’s respective outer functions and also learned differences between these functions.

**Related:**

- [How to flatten nested Struct column](https://sparkbyexamples.com/spark/spark-flatten-nested-struct-column/)
- [How to flatten nested array column](https://sparkbyexamples.com/spark/spark-flatten-nested-array-column-to-single-column/)