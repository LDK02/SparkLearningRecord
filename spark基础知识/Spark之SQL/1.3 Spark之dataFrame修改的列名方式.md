# 在 Spark DataFrame 上重命名列的方法

In Spark `withColumnRenamed()` is used to rename one column or multiple DataFrame column names. Depends on the DataFrame schema, renaming columns might get simple to complex, especially when a column is nested with struct type it gets complicated.

在 Spark 中，`withColumnRenamed()` 用于重命名一列或多个 DataFrame 列名。 取决于 DataFrame 架构，<font color=red>重命名列可能会变得简单到复杂，尤其是当列与结构类型嵌套时，它会变得复杂</font>。

In this article, I will explain how to rename a DataFrame column with multiple use cases like rename selected multiple columns, nested struct columns, all columns with Scala examples.

在本文中，我将解释如何重命名具有多个用例的 DataFrame 列，例如重命名选定的多个列、嵌套的结构列、使用 Scala 示例的所有列。

## 目录：

1. [使用 Spark withColumnRenamed – 重命名 DataFrame 列名](#1. 使用 Spark withColumnRenamed – 重命名 DataFrame 列名)
2. [使用 withColumnRenamed – 重命名多个列](#2. 使用 withColumnRenamed – 重命名多个列)
3. [使用 Spark StructType – 重命名 Dataframe 中的嵌套列](3. 使用 Spark StructType – 重命名 Dataframe 中的嵌套列)
4. [ 使用 Select – 重命名嵌套元素](#4. 使用 Select – 重命名嵌套元素)
5. [使用 Spark DataFrame withColumn – 重命名嵌套列](#5. 使用 Spark DataFrame withColumn – 重命名嵌套列)
6. [使用 col() 函数 - 动态重命名所有或多个列](#6. 使用 col() 函数 - 动态重命名所有或多个列)
7. [使用 toDF() - 更改 Spark DataFrame 中的所有列](#7. 使用 toDF() - 更改 Spark DataFrame 中的所有列)

First, let’s create our data for our examples, we are using Row class as we [convert this data to Spark DataFrame](https://sparkbyexamples.com/spark/different-ways-to-create-a-spark-dataframe/).

首先，让我们为示例创建数据，我们使用 Row 类 将此数据转换为 Spark DataFrame。

```scala
val data = Seq(Row(Row("James ","","Smith"),"36636","M",3000),
  Row(Row("Michael ","Rose",""),"40288","M",4000),
  Row(Row("Robert ","","Williams"),"42114","M",4000),
  Row(Row("Maria ","Anne","Jones"),"39192","F",4000),
  Row(Row("Jen","Mary","Brown"),"","F",-1)
)
```

Our base [schema ](https://sparkbyexamples.com/spark/spark-schema-explained-with-examples/)with nested structure.

我们的基础 [schema](https://sparkbyexamples.com/spark/spark-schema-explained-with-examples/) 具有嵌套结构。

```scala
val schema = new StructType()
  .add("name",new StructType()
    .add("firstname",StringType)
    .add("middlename",StringType)
    .add("lastname",StringType))
  .add("dob",StringType)
  .add("gender",StringType)
  .add("salary",IntegerType)
```

Let’s create the DataFrame by using [parallelize ](https://sparkbyexamples.com/apache-spark-rdd/how-to-create-an-rdd-using-parallelize/)and provide the above schema.

让我们使用 [parallelize](https://sparkbyexamples.com/apache-spark-rdd/how-to-create-an-rdd-using-parallelize/) 创建 DataFrame 并提供上述模式。

```scala
val df = spark.createDataFrame(spark.sparkContext.parallelize(data),schema)
df.printSchema()
```

Below is our schema structure. I am not printing data here as it is not necessary for our examples. This schema has a nested structure.

下面是我们的架构结构。 我没有在这里打印数据，因为我们的示例不需要它。 此模式具有嵌套结构。

```scala
--------------打印dataframe数据结构-------------
root
 |-- name: struct (nullable = true)
 |    |-- firstname: string (nullable = true)
 |    |-- middlename: string (nullable = true)
 |    |-- lastname: string (nullable = true)
 |-- dob: string (nullable = true)
 |-- gender: string (nullable = true)
 |-- salary: integer (nullable = true)
```

### 1. 使用 Spark withColumnRenamed – 重命名 DataFrame 列名

Spark has a `withColumnRenamed()` function on DataFrame to change a column name. This is the most straight forward approach; this function takes two parameters; the first is your existing column name and the second is the new column name you wish for.

Spark 在 DataFrame 上有一个 `withColumnRenamed()` 函数来更改列名。 这是最直接的方法； 这个函数有两个参数； 第一个是您现有的列名，第二个是您想要的新列名。

**Syntax:**

```scala
def withColumnRenamed(existingName: String, newName: String): DataFrame
```

`existingName` – The existing column name you want to change

>  `existingName` - 您要更改的现有列名

`newName` – New name of the column

> `newName` - 列的新名称

Returns a new DataFrame (Dataset[Row]) with a column renamed. This is a no-op if the schema doesn’t contain `existingName`.

返回一个新的 DataFrame (Dataset[Row]) 并重命名了一个列。 如果模式不包含 `existingName`，则这是一个空操作。

**Example**:

```scala
/**
      * 1 重名命名列名
      * */
df.withColumnRenamed("dob","DataofBirth").printSchema()//并打印dataframe的数据结构
```

The above example changes the column name from “dob” to “DateOfBirth” on spark DataFrame. Note that `withColumnRenamed` function returns a new DataFrame and doesn’t modify the current DataFrame.

上面的示例将 spark DataFrame 上的列名从“dob”更改为“DateOfBirth”。 请注意，`withColumnRenamed` 函数会返回一个新的 DataFrame，并且不会修改当前的 DataFrame。

```scala
root
 |-- name: struct (nullable = true)
 |    |-- firstname: string (nullable = true)
 |    |-- middlename: string (nullable = true)
 |    |-- lastname: string (nullable = true)
 |-- DataofBirth: string (nullable = true)
 |-- gender: string (nullable = true)
 |-- salary: integer (nullable = true)
```

### 2. 使用 withColumnRenamed – 重命名多个列

To change multiple column names, we should chain `withColumnRenamed` functions as shown below.

要更改多个列名，我们应该使用 `withColumnRenamed` 函数，如下所示。

```scala
/**
      * 2 修改多个列名
      * */
val df2: DataFrame = df.withColumnRenamed("salary", "salary_amount")
.withColumnRenamed("gender", "sex")
println("------修改多个列名的信息-------")
df2.printSchema()
```

This creates a new DataFrame “df2” after renaming dob and salary columns.

这会在重命名 dob 和 Salary 列后创建一个新的 DataFrame “df2”。

### 3. 使用 Spark StructType – 重命名 Dataframe 中的嵌套列

Changing a column name on nested data is not straight forward and we can do this by creating a new schema with new [DataFrame columns using StructType](https://sparkbyexamples.com/spark/spark-sql-structtype-on-dataframe/) and use it using cast function as shown below.

更改嵌套数据的列名并不简单，我们可以通过使用新的 [DataFrame 列使用 StructType] 创建一个新模式来做到这一点)并使用 cast 函数使用它，如下所示。

```scala
/**
      * 3 使用StructType和select 方式修改嵌套列名
      * */

val schema2: StructType = new StructType()
.add("fname", StringType)
.add("middlename", StringType)
.add("lname", StringType)
```

```scala
import org.apache.spark.sql.functions.col

val df3: DataFrame = df.select(col("name").cast(schema2),//只对嵌套列名进行修改
                               col("dob"),//原始列名，不做修改
                               col("gender"),
                               col("salary"))

println("-------修改结果----------")
df3.printSchema()
```

This statement renames firstname to fname and lastname to lname within name structure.

此语句在名称结构中将 firstname 重命名为 fname 并将 lastname 重命名为 lname。

```scala
-------修改结果----------
root
 |-- name: struct (nullable = true)
 |    |-- fname: string (nullable = true)
 |    |-- middlename: string (nullable = true)
 |    |-- lname: string (nullable = true)
 |-- dob: string (nullable = true)
 |-- gender: string (nullable = true)
 |-- salary: integer (nullable = true)
```

### 4. 使用 Select – 重命名嵌套元素。

Let’s see another way to change nested columns by transposing the structure to flat.

让我们看看另一种通过将结构转置为平面来更改嵌套列的方法。

```scala
/** 4 使用select修改嵌套列名
      * */
println("-------select修改结果-----------------")
df.select(col("name.firstname").as("fname"),
          col("name.middlename").as("mname"),//对嵌套的列名进行修改
          col("name.lastname").as("lname"),
          //原始列名，不进行修改
          col("dob"),col("gender"),col("salary")).printSchema()
```

```scala
-------select修改结果-----------------
root
 |-- fname: string (nullable = true)
 |-- mname: string (nullable = true)
 |-- lname: string (nullable = true)
 |-- dob: string (nullable = true)
 |-- gender: string (nullable = true)
 |-- salary: integer (nullable = true)
```

### 5. 使用 Spark DataFrame withColumn – 重命名嵌套列

When you have nested columns on Spark DatFrame and if you want to rename it, use `withColumn` on a data frame object to create a new column from an existing and we will need to drop the existing column. Below example creates a “fname” column from “name.firstname” and drops the “name” column

当您在 Spark DatFrame 上有嵌套列并且想要重命名它时，请在数据框对象上使用“withColumn”从现有列创建新列，我们需要删除现有列。 下面的示例从“name.firstname”创建一个“fname”列并删除“name”列

```scala
/**
      * 5 使用withColname进行修改嵌套列名
      * */
val df5: DataFrame = df.withColumn("fname", col("name.firstname"))//"fname"为新的列名
.withColumn("mname", col("name.middlename"))
.withColumn("lname", col("name.lastname"))
.drop("name")//需要删除已有的列名
println("-----使用withColname进行修改嵌套列名--------")
df5.printSchema()
```



### 6. 使用 col() 函数 - 动态重命名所有或多个列

Another way to change all column names on Dataframe is to use `col()` function.

另一种更改 Dataframe 上所有列名的方法是使用 `col()` 函数。

```scala
/**
      * 6 使用col函数，进行修改列名
      * */

val old_columns = Seq("dob","gender","salary","fname","mname","lname")
val new_columns = Seq("DateOfBirth","Sex","salary","firstName","middleName","lastName")
val columnsList: Seq[Column] = old_columns.zip(new_columns).map( f => col(f._1).as(f._2))
val df6: DataFrame = df5.select(columnsList:_*)
println("-----col函数修改结果-----")
df6.printSchema()
```

### 7. 使用 toDF() - 更改 Spark DataFrame 中的所有列

When we have data in a flat structure (without nested) , use `toDF()` with a new schema to change all column names.

当我们有平面结构中的数据（没有嵌套）时，使用带有新模式的 `toDF()` 来更改所有列名。

```scala
 /**
      * 7 使用 toDF()修改列名
      *
      * */
val newColumns = Seq("newCol1","newCol2","newCol3")
val df7: DataFrame = df.toDF(newColumns:_*)
println("----使用toDF函数修改列名---------")
df7.printSchema()
```

## 完整代码

```scala
package sparkScalaExamples.SQL


import org.apache.spark.sql.types.{IntegerType, StringType, StructType}
import org.apache.spark.sql.{Column, DataFrame, Row, SparkSession}

object withColumnRenamedDemo {

  def main(args: Array[String]): Unit = {
    val spark: SparkSession =
      SparkSession
        .builder()
        .appName("withColumnRenamed")
        .master("local")
        .getOrCreate()


    val data = Seq(Row(Row("James ","","Smith"),"36636","M",3000),
      Row(Row("Michael ","Rose",""),"40288","M",4000),
      Row(Row("Robert ","","Williams"),"42114","M",4000),
      Row(Row("Maria ","Anne","Jones"),"39192","F",4000),
      Row(Row("Jen","Mary","Brown"),"","F",-1)
    )
    //定义dataframe数据结构
    val schema = new StructType()
      .add("name",new StructType().add("firstname",StringType).add("middlename",StringType).add("lastname",StringType))
      .add("dob",StringType)
      .add("gender",StringType)
      .add("salary",IntegerType)

    val df: DataFrame = spark.createDataFrame(spark.sparkContext.parallelize(data),schema)
    println("--------------打印dataframe数据结构-------------")
    df.printSchema()

    /**
      * 1 重名命名列名
      * */
    df.withColumnRenamed("dob","DataofBirth").printSchema()//并打印dataframe的数据结构
    /**
      * 2 修改多个列名
      * */
    val df2: DataFrame = df.withColumnRenamed("salary", "salary_amount")
      .withColumnRenamed("gender", "sex")
    println("------修改多个列名的信息-------")
    df2.printSchema()
    /**
      * 修改嵌套列名，即dataframe中嵌套了一个小的dataframe
      *
      *
      * */

    /**
      * 3 使用StructType和select 方式修改嵌套列名
      * */

    val schema2: StructType = new StructType()
      .add("fname", StringType)
      .add("middlename", StringType)
      .add("lname", StringType)

    import org.apache.spark.sql.functions.col

    val df3: DataFrame = df.select(col("name").cast(schema2),//只对嵌套列名进行修改
      col("dob"),//原始列名，不做修改
      col("gender"),
      col("salary"))

    println("-------修改结果----------")
    df3.printSchema()

    /** 4 使用select修改嵌套列名
      * */
    println("-------select修改结果-----------------")
    df.select(col("name.firstname").as("fname"),
      col("name.middlename").as("mname"),//对嵌套的列名进行修改
      col("name.lastname").as("lname"),
      //原始列名，不进行修改
      col("dob"),col("gender"),col("salary")).printSchema()

    /**
      * 5 使用withColname进行修改嵌套列名
      * */
    val df5: DataFrame = df.withColumn("fname", col("name.firstname"))//"fname"为新的列名
      .withColumn("mname", col("name.middlename"))
      .withColumn("lname", col("name.lastname"))
      .drop("name")//需要删除已有的列名
    println("-----使用withColname进行修改嵌套列名--------")
    df5.printSchema()

    /**
      * 6 使用col函数，进行修改列名
      * */

    val old_columns = Seq("dob","gender","salary","fname","mname","lname")
    val new_columns = Seq("DateOfBirth","Sex","salary","firstName","middleName","lastName")
    val columnsList: Seq[Column] = old_columns.zip(new_columns).map( f => col(f._1).as(f._2))
    val df6: DataFrame = df5.select(columnsList:_*)
    println("-----col函数修改结果-----")
    df6.printSchema()

    /**
      * 7 使用 toDF()修改列名
      *
      * */
    val newColumns = Seq("newCol1","newCol2","newCol3")
    val df7: DataFrame = df.toDF(newColumns:_*)
    println("----使用toDF函数修改列名---------")
    df7.printSchema()

  }

}

```

The complete code can be downloaded from [GitHub](https://github.com/sparkbyexamples/spark-examples/blob/master/spark-sql-examples/src/main/scala/com/sparkbyexamples/spark/dataframe/RenameColDataFrame.scala)

## 结论Conclusion:

This article explains `withColumnRenamed()` function and different ways to rename a single column, multiple, all, and nested columns on Spark DataFrame. Besides what explained here, we can also change column names using Spark SQL and the same concept can be used in PySpark.

<font color=red>本文介绍了“withColumnRenamed()”函数以及在 Spark DataFrame 上重命名单列、多列、所有列和嵌套列的不同方法</font>。 除了这里解释的内容外，我们还可以使用 Spark SQL 更改列名，并且在 PySpark 中也可以使用相同的概念。

