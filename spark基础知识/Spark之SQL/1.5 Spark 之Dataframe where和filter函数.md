# 使用 Spark DataFrame Where Filter

## 目录

[1. Spark DataFrame filter() 语法](#1. Spark DataFrame filter() 语法)

[2 准备数据](2 准备数据)

[3. DataFrame filter() with Column 条件](#3. DataFrame filter() with Column 条件)

[4. 带有 SQL 表达式的 DataFrame filter()](#4. 带有 SQL 表达式的 DataFrame filter())

[5. 多条件过滤](#5. 多条件过滤)

[6. 对数组列进行过滤](#6. 对数组列进行过滤)

[7. 过滤嵌套结构列](7. 过滤嵌套结构列)

[8. 完整代码](#8. 完整代码)

[9. 总结](#9. 总结)

[相关文章](相关文章)

> Spark `filter()` or `where()` function is used to filter the rows from DataFrame or Dataset based on the given one or multiple conditions or SQL expression. You can use `where()` operator instead of the filter if you are coming from SQL background. Both these functions operate exactly the same.

Spark `filter()` 或 `where()` 函数用于根据给定的一个或多个条件或 SQL 表达式从 DataFrame 或 Dataset 中过滤行。 如果您来自 SQL 背景，则可以使用 `where()` 运算符代替过滤器。 这两个函数的操作完全相同。

If you wanted to ignore rows with NULL values, please refer to [Spark filter Rows with NULL values](https://sparkbyexamples.com/spark/spark-filter-rows-with-null-values/) article.

如果您想忽略具有 NULL 值的行，请参阅 [Spark filter Rows with NULL values](https://sparkbyexamples.com/spark/spark-filter-rows-with-null-values/) 文章。

In this Spark article, you will learn how to apply where filter on primitive [data types](https://sparkbyexamples.com/spark/spark-sql-dataframe-data-types/), arrays, [struct](https://sparkbyexamples.com/spark/spark-sql-structtype-on-dataframe/) using single and multiple conditions on DataFrame with Scala examples.

在这篇 Spark 文章中，您将学习如何对原始 [数据类型](https://sparkbyexamples.com/spark/spark-sql-dataframe-data-types/)、数组、[结构](https: //sparkbyexamples.com/spark/spark-sql-structtype-on-dataframe/) 在带有 Scala 示例的 DataFrame 上使用单个和多个条件。

## 1. Spark DataFrame filter() 语法

```scala
1) filter(condition: Column): Dataset[T]
2) filter(conditionExpr: String): Dataset[T] //using SQL expression
3) filter(func: T => Boolean): Dataset[T]
4) filter(func: FilterFunction[T]): Dataset[T]
```

Using the **first signature** you can refer `Column` names using one of the following syntaxes `$colname`, `col("colname")`, `'colname` and `df("colname")` with condition expression.

The **second signature** will be used to provide [SQL expressions to filter() rows](https://sparkbyexamples.com/spark/spark-dataframe-where-filter/#sql-expression).

The **third signature** is used with [SQL functions](https://sparkbyexamples.com/spark/spark-sql-functions-understanding/) where the function applied on each row.

The **fourth signature** is used with `FilterFunction` class.

> 使用**第一个参数**，您可以使用以下语法之一引用 `Column` 名称 `$"colname"`、`col("colname")`、`'colname` 和 `df("colname")` with condition 表达。
>
> **第二个参数**将用于提供 [SQL 表达式到 filter() 行](https://sparkbyexamples.com/spark/spark-dataframe-where-filter/#sql-expression)。
>
> **第三个参数**与 [SQL 函数](https://sparkbyexamples.com/spark/spark-sql-functions-understanding/) 一起使用，其中该函数应用于每一行。
>
> **第四个参数**与 `FilterFunction` 类一起使用。

## 2 准备数据

Before we start with examples, first let’s [create a DataFrame](https://sparkbyexamples.com/spark/different-ways-to-create-a-spark-dataframe/).

```scala
/** ---准备数据----*/

    val arrayStructureData = Seq(
      Row(Row("James","","Smith"),List("Java","Scala","C++"),"OH","M"),
      Row(Row("Anna","Rose",""),List("Spark","Java","C++"),"NY","F"),
      Row(Row("Julia","","Williams"),List("CSharp","VB"),"OH","F"),
      Row(Row("Maria","Anne","Jones"),List("CSharp","VB"),"NY","M"),
      Row(Row("Jen","Mary","Brown"),List("CSharp","VB"),"NY","M"),
      Row(Row("Mike","Mary","Williams"),List("Python","VB"),"OH","M")
    )

    val arrayStructureSchema = new StructType()
      .add("name",new StructType()
        .add("firstname",StringType)
        .add("middlename",StringType)
        .add("lastname",StringType))
      .add("languages", ArrayType(StringType))
      .add("state", StringType)
      .add("gender", StringType)

    val df = spark.createDataFrame(
      spark.sparkContext.parallelize(arrayStructureData),arrayStructureSchema)
    println("-------原始数据-----------------")
    df.printSchema()
    df.show()
```

This yields below schema and DataFrame results.

```scala
-------原始数据-----------------
root
 |-- name: struct (nullable = true)
 |    |-- firstname: string (nullable = true)
 |    |-- middlename: string (nullable = true)
 |    |-- lastname: string (nullable = true)
 |-- languages: array (nullable = true)
 |    |-- element: string (containsNull = true)
 |-- state: string (nullable = true)
 |-- gender: string (nullable = true)

+--------------------+------------------+-----+------+
|                name|         languages|state|gender|
+--------------------+------------------+-----+------+
|    [James, , Smith]|[Java, Scala, C++]|   OH|     M|
|      [Anna, Rose, ]|[Spark, Java, C++]|   NY|     F|
| [Julia, , Williams]|      [CSharp, VB]|   OH|     F|
|[Maria, Anne, Jones]|      [CSharp, VB]|   NY|     M|
|  [Jen, Mary, Brown]|      [CSharp, VB]|   NY|     M|
|[Mike, Mary, Will...|      [Python, VB]|   OH|     M|
+--------------------+------------------+-----+------+
```

## 3. DataFrame filter() with Column 条件

> Use Column with the condition to filter the rows from DataFrame, using this you can express complex condition by referring column names using `col(name)`, `$"colname"` `dfObject("colname") `, this approach is mostly used while working with DataFrames. Use “`===`” for comparison.

使用带有条件的 Column 从 DataFrame 中过滤行，使用它可以通过使用 `col(name)`、`$"colname"` `dfObject("colname") ` 引用列名来表达复杂的条件，这种方法主要是 在使用 DataFrame 时使用。 使用“`===`”进行比较。

```scala
/** 1
* 使用filter筛选state 列为 OH 的数据
* */
println("使用filter筛选state 列为 OH 的数据")
df.filter(df("state") === "OH").show(false)
```

This yields below DataFrame results.

```scala
使用filter筛选state 列为 OH 的数据
+----------------------+------------------+-----+------+
|name                  |languages         |state|gender|
+----------------------+------------------+-----+------+
|[James, , Smith]      |[Java, Scala, C++]|OH   |M     |
|[Julia, , Williams]   |[CSharp, VB]      |OH   |F     |
|[Mike, Mary, Williams]|[Python, VB]      |OH   |M     |
+----------------------+------------------+-----+------+
```

Alternatively, you can also write this statement as follows. All these functions return the same result and performance.

或者，您也可以按如下方式编写此语句。 所有这些函数都返回相同的结果和性能。

```scala
import spark.implicits._
import org.apache.spark.sql.functions._
//这一部分功能和上述部分一样,只是写法不同
df.filter('state === "OH").show(false)
df.filter($"state" === "OH").show(false)
df.filter(col("state") === "OH").show(false)
df.where(df("state") === "OH").show(false)
df.where('state === "OH").show(false)
df.where($"state" === "OH").show(false)
df.where(col("state") === "OH").show(false)
```

## 4. 带有 SQL 表达式的 DataFrame filter()

If you are coming from SQL background, you can use that knowledge in Spark to filter DataFrame rows with SQL expressions.

如果你有 SQL 背景，你可以在 Spark 中使用这些知识来过滤带有 SQL 表达式的 DataFrame 行。

```scala
/**
      * 2
      * 要是熟悉SQL语句
      * 可以使用SQL语句方式进行过滤
      * */
println("使用SQL语句进行过滤")
df.filter("gender == 'M'").show(false)
df.where("gender == 'M'").show(false)
```

This yields below DataFrame results.

```scala
+----------------------+------------------+-----+------+
|name                  |languages         |state|gender|
+----------------------+------------------+-----+------+
|[James, , Smith]      |[Java, Scala, C++]|OH   |M     |
|[Maria, Anne, Jones]  |[CSharp, VB]      |NY   |M     |
|[Jen, Mary, Brown]    |[CSharp, VB]      |NY   |M     |
|[Mike, Mary, Williams]|[Python, VB]      |OH   |M     |
+----------------------+------------------+-----+------+
```

## 5. 多条件过滤

To `filter()` rows on Spark DataFrame based on multiple conditions using AND(&&), OR(||), and NOT(!), you case use either `Column` with a condition or SQL expression as explained above. Below is just a simple example, you can extend this with AND(&&), OR(||), and NOT(!) conditional expressions as needed.

要基于使用 AND(&&)、OR(||) 和 NOT(!) 的多个条件在 Spark DataFrame 上“过滤()”行，您可以使用带有条件的“Column”或 SQL 表达式，如上所述。 下面只是一个简单的示例，您可以根据需要使用 AND(&&)、OR(||) 和 NOT(!) 条件表达式对其进行扩展。

```scala
 /**3 多条件过滤数据
      * */
println("多个筛选条件过滤：----")
df.filter(df("state") === "OH" && df("gender") ==="M").show(false)
```

This yields below DataFrame results.

```scala
+----------------------+------------------+-----+------+
|name                  |languages         |state|gender|
+----------------------+------------------+-----+------+
|[James, , Smith]      |[Java, Scala, C++]|OH   |M     |
|[Mike, Mary, Williams]|[Python, VB]      |OH   |M     |
+----------------------+------------------+-----+------+
```

## 6. 对数组列进行过滤

When you want to filter rows from DataFrame based on value present in an [array collection column](https://sparkbyexamples.com/spark/spark-array-arraytype-dataframe-column/), you can use the first syntax. The below example uses `array_contains()` [Spark SQL function](https://sparkbyexamples.com/spark/spark-sql-functions-understanding/) which checks if a value contains in an array if present it returns true otherwise false.

当您想根据 [数组集合列](https://sparkbyexamples.com/spark/spark-array-arraytype-dataframe-column/) 中存在的值从 DataFrame 中过滤行时，可以使用第一种语法。 下面的示例使用 `array_contains()` [Spark SQL 函数](https://sparkbyexamples.com/spark/spark-sql-functions-understanding/) 检查值是否包含在数组中，如果存在则返回 true，否则返回 false .

```scala
/**
* 4 array_contains
* 是否包含"元素"，包含则返回数据
*
* */
println("array_contains的用法----------------：")
df.filter(array_contains(df("languages"),"Java")).show(false)
```

This yields below DataFrame results.

```scala
+----------------+------------------+-----+------+
|name            |languages         |state|gender|
+----------------+------------------+-----+------+
|[James, , Smith]|[Java, Scala, C++]|OH   |M     |
|[Anna, Rose, ]  |[Spark, Java, C++]|NY   |F     |
+----------------+------------------+-----+------+
```

## 7. 过滤嵌套结构列

If your DataFrame consists of [nested struct columns](https://sparkbyexamples.com/spark/spark-sql-structtype-on-dataframe/), you can use any of the above syntaxes to filter the rows based on the nested column.

如果您的 DataFrame 由 [嵌套结构列](https://sparkbyexamples.com/spark/spark-sql-structtype-on-dataframe/) 组成，您可以使用上述任何语法来根据嵌套列过滤行 .

```scala
/**5 过滤有嵌套的数据
      * */
df.filter(df("name.lastname") === "Williams").show(false)

```

This yields below DataFrame results

```scala
+----------------------+------------+-----+------+
|name                  |languages   |state|gender|
+----------------------+------------+-----+------+
|[Julia, , Williams]   |[CSharp, VB]|OH   |F     |
|[Mike, Mary, Williams]|[Python, VB]|OH   |M     |
+----------------------+------------+-----+------+
```

## 8. 完整代码

```scala
import org.apache.spark.sql.{Row, SparkSession}
import org.apache.spark.sql.types.{ArrayType, StringType, StructType}

object WhereFilterDemo {
  def main(args: Array[String]): Unit = {

    val spark: SparkSession = SparkSession
      .builder()
      .appName("Where Filter")
      .master("local")
      .getOrCreate()

    /** ---准备数据----*/

    val arrayStructureData = Seq(
      Row(Row("James","","Smith"),List("Java","Scala","C++"),"OH","M"),
      Row(Row("Anna","Rose",""),List("Spark","Java","C++"),"NY","F"),
      Row(Row("Julia","","Williams"),List("CSharp","VB"),"OH","F"),
      Row(Row("Maria","Anne","Jones"),List("CSharp","VB"),"NY","M"),
      Row(Row("Jen","Mary","Brown"),List("CSharp","VB"),"NY","M"),
      Row(Row("Mike","Mary","Williams"),List("Python","VB"),"OH","M")
    )

    val arrayStructureSchema = new StructType()
      .add("name",new StructType()
        .add("firstname",StringType)
        .add("middlename",StringType)
        .add("lastname",StringType))
      .add("languages", ArrayType(StringType))
      .add("state", StringType)
      .add("gender", StringType)

    val df = spark.createDataFrame(
      spark.sparkContext.parallelize(arrayStructureData),arrayStructureSchema)
    println("-------原始数据-----------------")
    df.printSchema()
    df.show()

    /** 1
      * 使用filter筛选state 列为 OH 的数据
      * */
    println("使用filter筛选state 列为 OH 的数据")
    df.filter(df("state") === "OH").show(false)

    import spark.implicits._
    import org.apache.spark.sql.functions._
    //这一部分功能和上述部分一样,只是写法不同
    df.filter('state === "OH").show(false)
    df.filter($"state" === "OH").show(false)
    df.filter(col("state") === "OH").show(false)
    df.where(df("state") === "OH").show(false)
    df.where('state === "OH").show(false)
    df.where($"state" === "OH").show(false)
    df.where(col("state") === "OH").show(false)
    //

    /**
      * 2
      * 要是熟悉SQL语句
      * 可以使用SQL语句方式进行过滤
      * */
    println("使用SQL语句进行过滤")
    df.filter("gender == 'M'").show(false)
    df.where("gender == 'M'").show(false)

    /**3 多条件过滤数据
      * */
    println("多个筛选条件过滤：----")
    df.filter(df("state") === "OH" && df("gender") ==="M").show(false)

    /**
      * 4 array_contains
      * 是否包含"元素"，包含则返回数据
      *
    * */
    println("array_contains的用法----------------：")
    df.filter(array_contains(df("languages"),"Java")).show(false)

    /**5 过滤有嵌套的数据
      * */
    df.filter(df("name.lastname") === "Williams").show(false)

  }

}

```

Examples explained here are also available at [GitHub](https://github.com/spark-examples/spark-scala-examples/blob/master/src/main/scala/com/sparkbyexamples/spark/dataframe/FilterExample.scala) project for reference.

## 9. 总结

In this tutorial, I’ve explained how to filter rows from Spark DataFrame based on single or multiple conditions and SQL expression, also learned filtering rows by providing conditions on the array and struct column with Scala examples.

Alternatively, you can also use `where()` function to filter the rows on DataFrame.

Thanks for reading. If you like it, please do share the article by following the below social links and any comments or suggestions are welcome in the comments sections! 

Happy Learning !!

在本教程中，我解释了如何根据单个或多个条件和 SQL 表达式从 Spark DataFrame 中过滤行，还通过在 Scala 示例中为数组和结构列提供条件来学习过滤行。

或者，您也可以使用 `where()` 函数来过滤 DataFrame 上的行。

谢谢阅读。 如果您喜欢它，请通过以下社交链接分享文章，欢迎在评论部分提出任何意见或建议！

快乐学习！！

## 相关文章

- [How to Add and Update DataFrame Columns in Spark](https://sparkbyexamples.com/spark/spark-dataframe-withcolumn/)
- [How to Rename a DataFrame Column](https://sparkbyexamples.com/spark/rename-a-column-on-spark-dataframes/)
- [How to Drop DataFrame Column](https://sparkbyexamples.com/spark/spark-drop-column-from-dataframe-dataset/)
- [Spark Join Types](https://sparkbyexamples.com/spark/spark-sql-dataframe-join/)
- [Spark DataFrame groupBy()](https://sparkbyexamples.com/spark/using-groupby-on-dataframe/)
- [Spark Union() & UnionAll() Examples](https://sparkbyexamples.com/spark/spark-dataframe-union-and-union-all/)
- [Spark Distinct Rows from DataFrame](https://sparkbyexamples.com/spark/spark-remove-duplicate-rows/)