# Spark之Dataframe去重

> Duplicate rows could be remove or drop from Spark SQL DataFrame using `distinct()` and `dropDuplicates()` functions, distinct() can be used to remove rows that have the same values on all columns whereas dropDuplicates() can be used to remove rows that have the same values on multiple selected columns.

可以使用 distinct() 和 dropDuplicates() 函数从 Spark SQL DataFrame 中删除或删除重复的行， <font color=red>distinct() 可用于删除所有列上具有相同值的行，而 dropDuplicates() 可用于 删除在多个选定列上具有相同值的行</font>。

## 目录

[1 准备数据](#1 准备数据)

[2. 使用 distinct() - 删除 DataFrame 上的重复行](#2. 使用 distinct() - 删除 DataFrame 上的重复行)

[3. 使用 drop Duplicate() - 删除 DataFrame 上的指定列的重复行](#3. 使用 drop Duplicate() - 删除 DataFrame 上的指定列的重复行)

[3. 源代码——删除重复行](#3. 源代码——删除重复行)

[4. 总结](#4. 总结)

## 准备数据

> Before we start, first let’s [create a DataFrame](https://sparkbyexamples.com/spark/different-ways-to-create-a-spark-dataframe/) with some duplicate rows and duplicate values on a few columns.

在开始之前，首先让我们[创建一个 DataFrame](https://sparkbyexamples.com/spark/different-ways-to-create-a-spark-dataframe/)，其中包含一些重复的行和几列上的重复值。

```scala
/**
      * 1 准备数据-----
      *
      * */
    import spark.implicits._

    val simpleData = Seq(("James", "Sales", 3000),
      ("Michael", "Sales", 4600),
      ("Robert", "Sales", 4100),
      ("Maria", "Finance", 3000),
      ("James", "Sales", 3000),
      ("Scott", "Finance", 3300),
      ("Jen", "Finance", 3900),
      ("Jeff", "Marketing", 3000),
      ("Kumar", "Marketing", 2000),
      ("Saif", "Sales", 4100)
    )
    val df = simpleData.toDF("employee_name", "department", "salary")
    println("---------原始数据集：--------")
    df.show()
```

Yields below output

```scala
---------原始数据集：--------
原始数据行数：10
+-------------+----------+------+
|employee_name|department|salary|
+-------------+----------+------+
|        James|     Sales|  3000|
|      Michael|     Sales|  4600|
|       Robert|     Sales|  4100|
|        Maria|   Finance|  3000|
|        James|     Sales|  3000|
|        Scott|   Finance|  3000|
|          Jen|   Finance|  3900|
|         Jeff| Marketing|  3000|
|        Kumar| Marketing|  3000|
|         Saif|     Sales|  4100|
+-------------+----------+------+
```

> On the above table, I’ve highlighted all duplicate rows, As you notice we have 2 rows that have duplicate values on all columns and we have 4 rows that have duplicate values on “department” and “salary” columns.

在上表中，我突出显示了所有重复的行，如您所见，我们有 2 行在所有列上都有重复值，我们有 4 行在“部门”和“工资”列上具有重复值。

## 使用 distinct() - 删除 DataFrame 上的重复行

> On the above dataset, we have a total of 10 rows and one row with all values duplicated, performing distinct on this DataFrame should get us 9 as we have one duplicate.

在上面的数据集上，我们总共有 10 行，其中一行所有值都重复，在这个 DataFrame 上执行 distinct 应该得到 9，因为我们有一个重复。

```scala
  /**
      * 2 使用distinct() 去重
      * */
    val distinctDF: Dataset[Row] = df.distinct()
    println("去重后的行数：" + distinctDF.count())
    distinctDF.show(false)

```

> `distinct()` function on DataFrame returns a new DataFrame after removing the duplicate records. This example yields the below output.

DataFrame 上的 `distinct()` 函数在删除重复记录后返回一个新的 DataFrame。 此示例产生以下输出。

```scala
去重后的行数：9
+-------------+----------+------+
|employee_name|department|salary|
+-------------+----------+------+
|Kumar        |Marketing |3000  |
|James        |Sales     |3000  |
|Michael      |Sales     |4600  |
|Maria        |Finance   |3000  |
|Scott        |Finance   |3000  |
|Robert       |Sales     |4100  |
|Saif         |Sales     |4100  |
|Jeff         |Marketing |3000  |
|Jen          |Finance   |3900  |
+-------------+----------+------+
```

> Alternatively, you can also run `dropDuplicates()` function which return a new DataFrame with duplicate rows removed.

或者，您也可以运行 `dropDuplicates()` 函数，该函数返回一个删除重复行的新 DataFrame。

```scala
 /**3 使用dropDuplication去重
      * */
    val df2: Dataset[Row] = df.dropDuplicates()
    println("Dropduplication 去重后的数据行数：" + df2.count())
    df2.show(false)
```

## 使用 drop Duplicate() - 删除 DataFrame 上的指定列的重复行

> Spark doesn’t have a distinct method that takes columns that should run distinct on however, Spark provides another signature of `dropDuplicates()` function which takes multiple columns to eliminate duplicates.

Spark 没有独特的方法来获取应该在不同的列上运行的列，但是，Spark 提供了另一个 `dropDuplicates()` 函数，<font color=red>它可以指定重复的列来消除重复</font>。

> Note that calling dropDuplicates() on DataFrame returns a new DataFrame with duplicate rows removed.

**请注意：**在 DataFrame 上调用 dropDuplicates() 会返回一个删除重复行的新 DataFrame。

```scala
 /**4
      * 指定去重的列
      * */
    val dropDisDF: Dataset[Row] = df.dropDuplicates("department","salary")
    println("去重后的department和salary的行数：" + dropDisDF.count())
    dropDisDF.show(false)

```

结果：

```scala
去重后的department和salary的行数：6
+-------------+----------+------+
|employee_name|department|salary|
+-------------+----------+------+
|Jen          |Finance   |3900  |
|Maria        |Finance   |3000  |
|Michael      |Sales     |4600  |
|Robert       |Sales     |4100  |
|James        |Sales     |3000  |
|Jeff         |Marketing |3000  |
+-------------+----------+------+
```

## 完整代码

```scala
package sparkScalaExamples.SQL

import org.apache.spark.sql.{Dataset, Row, SparkSession}

object DuplicateDemo {
  def main(args: Array[String]): Unit = {

    val spark: SparkSession =
      SparkSession.builder()
        .appName("duplicationDmeo")
        .master("local")
        .getOrCreate()

    /**
      * 1 准备数据-----
      *
      * */
    import spark.implicits._

    val simpleData = Seq(("James", "Sales", 3000),
      ("Michael", "Sales", 4600),
      ("Robert", "Sales", 4100),
      ("Maria", "Finance", 3000),
      ("James", "Sales", 3000),
      ("Scott", "Finance", 3000),
      ("Jen", "Finance", 3900),
      ("Jeff", "Marketing", 3000),
      ("Kumar", "Marketing", 3000),
      ("Saif", "Sales", 4100)
    )
    val df = simpleData.toDF("employee_name", "department", "salary")
    println("---------原始数据集：--------")
    println("原始数据行数："+df.count())
    df.show()

    /**
      * 2 使用distinct() 去重
      * */
    val distinctDF: Dataset[Row] = df.distinct()
    println("去重后的行数：" + distinctDF.count())
    distinctDF.show(false)

    /**3 使用dropDuplication去重
      * */
    val df2: Dataset[Row] = df.dropDuplicates()
    println("Dropduplication 去重后的数据行数：" + df2.count())
    df2.show(false)

    /**4
      * 指定去重的列
      * */
    val dropDisDF: Dataset[Row] = df.dropDuplicates("department","salary")
    println("去重后的department和salary的行数：" + dropDisDF.count())
    dropDisDF.show(false)






  }
}

```

The complete example is available at [GitHub](https://github.com/spark-examples/spark-scala-examples/blob/master/src/main/scala/com/sparkbyexamples/spark/dataframe/functions/aggregate/DistinctCount.scala) for reference.

## 4. 总结

In this Spark article, you have learned how to remove DataFrame rows that are exact duplicates using `distinct()` and learned how to remove duplicate rows based on multiple columns using `dropDuplicate()` function with Scala example.

在这篇 Spark 文章中，您学习了如何使用 distinct() 删除完全重复的 DataFrame 行，并学习了如何使用 Scala 示例中的 dropDuplicate() 函数删除基于多列的重复行。

